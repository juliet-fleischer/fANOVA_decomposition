The formal setup is based on \cite{chastaing2012} and \cite{rahman2014}.\par
Let $(\Omega, \mathcal{F}, \nu)$ be a measure space, where $\Omega$ is a sample space, $\mathcal{F}$ is a $\sigma-$algebra on $\Omega$ and $\nu: \mathcal{F} \rightarrow [0, 1]$ is a probability measure. $\mathcal{B}^N$ is the Borel $\sigma$-algebra on $\mathbb{R}^N, N \in \mathbb{N}$.
$\boldsymbol{X} = (X_1, \dots, X_N): (\Omega, \mathcal{F}) \rightarrow (\mathbb{R}^N, \mathcal{B}^N)$ denotes a $\mathbb{R}^N$-valued random vector.\par
We assume that the probability distribution of $\boldsymbol{X}$ is continuous and completely defined by the joint probability density function $f_{\boldsymbol{X}}: \mathbb{R}^N \rightarrow \mathbb{R}_{0}^+$. $f_{\boldsymbol{X}}$ is the probability density function with respect to (w.r.t.) the measure $\nu$. \par

Let $u$ denote a subset of indices $\{1, \dots, N\}$ and $-u := \{1, \dots, N\} \backslash{} u$ its complement. We denote strict inclusion of a subset by $\subsetneq$ and $\subseteq$ allows for equality.
$\boldsymbol{X_u} = (X_1, \dots, X_{|u|}), u \neq \emptyset, 1 \leq i_1 < \dots < i_{|u|} \leq N$ is a sub-vector of $\boldsymbol{X}$ and $\boldsymbol{X}_{-u} = \boldsymbol{X}_{\{1, \dots, N\} \backslash{} u}$ is the complement of $\boldsymbol{X}_u$.

The marginal density function is $f_u(\boldsymbol{x_u}) := \int f_{\boldsymbol{X}}(\boldsymbol{x})d\boldsymbol{x_{-u}}$ for a given set $\emptyset \neq u \subseteq \{1, \dots N\}$.
$y(\boldsymbol{X}) := y(X_1, \dots, X_N)$ is a mathematical model with random variables as inputs.
We write a vector space of square-integrable functions as:
\[
\mathcal{L}^2(\Omega, \mathcal{F}, \nu) = \left\{ y: \Omega \rightarrow \mathbb{R} \; \textit{s.t.} \; \mathbb{E}[y^2(\boldsymbol{X})] < \infty \right\}.
\]
% = \left\{ f(x) : \mathbb{R}^{n} \to \mathbb{R}, \; \textit{s.t.} \; \int f^2(x)\, d\nu(x) < \infty \right\}

$\mathcal{L}^2(\Omega, \mathcal{F}, \nu)$ is a Hilbert space with the inner product defined as:
\[
\langle y, g \rangle = \int y(\boldsymbol{x}) g(\boldsymbol{x}) \, f_{\boldsymbol{X}}d\nu(\boldsymbol{x}) = \mathbb{E}[y(\boldsymbol{X})g(\boldsymbol{X})].
\]
The norm is denoted as $\|.\| $ and defined by:
\[
\|y\| = \sqrt{\langle y, y \rangle} = \sqrt{\int y^2(x) \, d\nu(x)} = \mathbb{E}[y^2], \quad \forall y \in \mathcal{L}^2.
\]

We start by defining the fANOVA decomposition in a very general form, which is independent of distribution assumptions about the input variables or anything of the sort.

\begin{definition}
Let $y$  denote a mathematical model with input denoted by $X_1, \dots, X_N$. The functional ANOVA (fANOVA) decomposition of $y$ takes the form:
\begin{equation}
    y(\boldsymbol{X}) = \sum_{u \subseteq \{1, \dots, N\}} y_{u}(\boldsymbol{X}_u),
    \label{eq:fanova_decomposition}
\end{equation}
where $u \subseteq \{1, \dots, N\} = \{ \{1\}, \{2\}, \{1, 2\}, \dots, \{1, \dots, N\} \}$ is the set, which contains all subsets of the indices $1, \dots, N$.
\end{definition}


The decomposition consists of $2^N$ terms and its specific form is determined by the assumptions about the input variables and integration measure.

\subsection{Classical fANOVA}
For the classical case, originally proposed by \cite{sobol1993sensitivity}, we make the assumption of independent identically distributed (i.i.d.) input variables.
While Sobol originally only considered function defined on the unit hypercube, later work shows that it is no problem to work within the measure space $(\mathbb{R}^n, \mathcal{B}^N, \nu)$.
Under independence the joint probability density function is given by the product over the marginal probability density functions, i.e. \(f_{\boldsymbol{X}}(\boldsymbol{x}) \, d\nu(\boldsymbol{x}) = \prod_{i=1}^{N} f_{X_i}(x_i) \, d\nu(x_i)\), where \(f_{X_i}: \mathbb{R} \rightarrow \mathbb{R}_{0}^{+}\) is the marginal probability density function of \(X_i\) defined on $(\Omega_i, \mathcal{F}_i, \nu_i)$ (or the previously defined measure space?).

Given this setup, we formulate a condition, proposed by \cite{rahman2014}, which we would like to hold for the fANOVA terms to be well-defined and interpretable.
\begin{condition}[Strong annihilating conditions]
    For the classical fANOVA decomposition we require, that all the terms (except for the constant) integrate to zero w.r.t the individual variables contained in $u$ and weighted by the individual marginal probability density functions:
\begin{equation}
    \int y_u(\boldsymbol{x}_u) f_{X_i}(x_i) \, d\nu(x_i) = 0, \quad \text{for} \ i \in u \neq \emptyset.
    \label{eq:strong_annihilating_conditions}
\end{equation}
\label{cond:strong_annihilating_conditions}
\end{condition}

\begin{proposition}
    Given the strong annihilating conditions, the non-constant fANOVA components are centered around zero. This means for all $\emptyset \neq u \subseteq \{1, \dots, N\}$ it holds that:
\begin{equation}
    \int y_u(\boldsymbol{x}_u) f_{\boldsymbol{X}}(\boldsymbol{x}) \, d\nu (\boldsymbol{x}) := \mathbb{E}[y_u(\boldsymbol{X}_u)] = 0.
    \label{eq:zero_mean_c}
\end{equation}
\end{proposition}
\begin{proof}
\begin{align*}
    \mathbb{E}[y_u(\boldsymbol{X}_u)] &:= = \int_{\mathbb{R}^{N}} y_u(\boldsymbol{x_u}) f_{\boldsymbol{X}}(\boldsymbol{x}) \, d\nu (\boldsymbol{x}) \\
    &= \int_{\mathbb{R}^{|u|}} y_u(\boldsymbol{x_u}) f_{\boldsymbol{X}_u}(\boldsymbol{x}_u) \, d\nu (\boldsymbol{x}_u) \\
    &= \int_{\mathbb{R}^{|u|}} y_u(\boldsymbol{x_u}) \prod_{i \in u} f_{X_i}(x_i) \, d\nu (\boldsymbol{x}_u) \\
    &= \int_{\mathbb{R}^{|u|-1}} \int_{\mathbb{R}} y_u(\boldsymbol{x_u}) f_{X_i}(x_i) \, dx_u \prod_{j \in u, j \neq i} f_{X_j}(x_j) = 0
\end{align*}
\end{proof}

\begin{proposition}
    Given the strong annihilating conditions, it follows that the fANOVA terms are orthogonal to each other. If two sets of indices are not completely equivalent, i.e. $\emptyset \neq u \subseteq \{1, \dots, N\}, \emptyset \neq v \subseteq \{1, \dots, N\}, \text{ and } u \neq v$, then it holds that:
\begin{equation}
    \int y_u(\boldsymbol{x_u}) y_v(\boldsymbol{x_v}) f_{\boldsymbol{X}}(\boldsymbol{x}) d\nu (\boldsymbol{x}) = \mathbb{E}[y_u(\boldsymbol{X}_u) y_v(\boldsymbol{X}_v)] = 0.
    \label{eq:orthogonality_c}
\end{equation}
\end{proposition}

\begin{proof}
    \begin{align*}
    \mathbb{E}[y_u(\boldsymbol{X}_u) y_v(\boldsymbol{X}_v)] &= \int_{\mathbb{R}^{\mathbb{N}}} y_u(\boldsymbol{x_u}) y_v(\boldsymbol{x_v}) f_{\boldsymbol{X}}(\boldsymbol{x}) \, d\nu (\boldsymbol{x}) \\
    &= \int_{\mathbb{R}^{\mathbb{N}}} y_u(\boldsymbol{x_u}) y_v(\boldsymbol{x_v}) \prod_{i=1}^{N} f_{X_i}(x_i) \, d\nu (x_i) \\
    &= \int_{\mathbb{R}^{\mathbb{N}-1}} \int_{\mathbb{R}} y_u(\boldsymbol{x_u}) y_v(\boldsymbol{x_v}) f_{X_i}(x_i) \, dx_u \prod_{j \in \{1, \dots, N\}, j \neq i} f_{X_j}(x_j) = 0
\end{align*}
\end{proof}

As we have seen, the fANOVA terms are ``fully orthogonal'' to each other, meaning not only terms of different order are orthogonal to each other but also terms of the same order are. Zero-mean and orthogonality are desirable and important properties because they ensure that a fANOVA term can be interpretate as isolated effect of the specific variable or isolated effect of an interaction. The term $y_1$, for example, captures the isolated main effects of $X_1$; there is no other effect mixed into it, which $X_1$ might have through interactions with other variables. The term $y_{12}$ on the other hand captures the interaction effect of $X_1$ and $X_2$, while the solo effect of $X_1$ is already captured by $y_1$ and does not merge into $y_{12}$. From the lense of interpretability, this distinguishes the fANOVA decomposition from methods such as partial dependence (PD) or Shapley values.\par

% And this is how the components finally look like
\subsubsection{Construction of the fANOVA Terms}
The individual fANOVA terms for the variables with indices in $u$ are constructed by integrating the original function $y(\boldsymbol{X})$ w.r.t all variables expect for the ones in $u$, and subtracting the lower order terms. Intuitively the integral is averaging the original function over all other variables expect the ones of interest, which makes sense as we are then left with a function of the variables of interest only. Subtracting lower order terms corresponds to accounting for effects that are already explained by other variables or interactions so that we obtain the isolated effects.\par
Since $u = \emptyset$ for the constant term, we integrate w.r.t all variables:
\begin{equation}
    y_{\emptyset} = \int y(\boldsymbol{x}) \prod_{i=1}^{N} f_{X_i}(x_i) \, d\nu (x_i) = \mathbb{E}[y(\boldsymbol{X})].
    \label{eq:intercept_classical}
\end{equation}
For all other effects $\emptyset \neq u \in \{1, \dots, N\}$ we can calculate:
\begin{equation}
    y_u(\boldsymbol{X}_u) = \int y(\boldsymbol{X}_u, \boldsymbol{x}_{-u}) \prod_{i=1, i \notin u}^{N} f_{X_i}(x_i) \, d\nu (x_i)- \sum_{v \subsetneq u} y_v(\boldsymbol{X}_v).
    \label{eq:fanova_components_classical}
\end{equation}
Notice that this definition relies on a product-type measure rooted in the independence assumption. We will see what changes when we let got of this assumption in the second part of this section.\par
As suggested earlier, the fANOVA components offer a clear interpretation of the model, decomposing it into main effects, two-way interaction effects, and so on. This is why fANOVA decomposition has received increasing attention in the IML and XAI literature, holding the potential for a global model-agnostic explanation method of black box models.\par
% So this is what we did so far: we defined the decomposition, defined the terms it is made up of, and looked a bit deeper at their mathematical properties and how to satisfiy them.
% There is an alternative way to define fANOVA, or just another way of looking at it, which builds on the connection between orthogonal projections and conditional expected values.
\subsubsection{Running Example: Multivariate Normal Inputs}

Throughout this thesis, we will use the following simple function as a running example:
\[
h(x_1, x_2) = a + X_1 + 2X_2 + X_1 X_2,
\]
which contains both main effects and an interaction term. 

We assume the input vector 
\[
\boldsymbol{X} = (X_1, X_2)^T
\]
follows a bivariate standard normal distribution
\[
\boldsymbol{X} \sim \mathcal{N}\!\left(
\begin{pmatrix}0 \\ 0\end{pmatrix},
\begin{pmatrix}
1 & \rho \\ 
\rho & 1
\end{pmatrix}
\right).
\]
This general formulation includes both independent inputs ($\rho = 0$) and correlated inputs ($\rho \neq 0$).

From properties of the multivariate normal distribution, the marginal distributions are
\[
X_1 \sim \mathcal{N}(0,1), \qquad X_2 \sim \mathcal{N}(0,1),
\]
and the conditional distributions are given by
\[
X_1 \mid X_2=x_2 \sim \mathcal{N}(\rho x_2, 1-\rho^2), \quad
X_2 \mid X_1=x_1 \sim \mathcal{N}(\rho x_1, 1-\rho^2).
\]
This example will allow us to compute and compare the classical and generalized fANOVA decompositions for different correlation structures.

\subsubsection{Case 1: Independent Inputs}
The classical fANOVA decomposition we covered so far assumes independence, i.e., $\rho = 0$. 
Here, $X_1$ and $X_2$ are independent and standard normal, so the conditional means vanish, and the classical fANOVA decomposition simplifies considerably. 
Computing the constant component via expectation gives:
\begin{align*}
    y_{\emptyset} &= \mathbb{E}[h(X_1, X_2)] \\
    &= \mathbb{E}[a + X_1 + 2X_2 + X_1X_2] \\
    &= a + \mathbb{E}[X_1] + 2\mathbb{E}[X_2] + \mathbb{E}[X_1]\mathbb{E}[X_2] = a.
\end{align*}
Under zero-mean constraint and independence, the main effects and the interaction effect can be computed as follows:
\begin{align*}
y_1(x_1) &= \mathbb{E}_{X_2}[h(x_1, X_2)] - y_0 \\
&= \mathbb{E}_{X_2}[a + x_1 + 2X_2 + x_1X_2] - a \\
&= x_1 + 2\mathbb{E}[X_2] + x_1\mathbb{E}[X_2] = x_1,\\
y_2(x_2) &= \mathbb{E}_{X_1}[h(X_1, x_2)] - y_0 \\
&= \mathbb{E}_{X_1}[a + X_1 + 2x_2 + X_1x_2] - a \\
&= \mathbb{E}_{X_1}[X_1] + 2x_2 + x_2\mathbb{E}_{X_1}[X_1] = 2x_2,\\
y_{12}(x_1, x_2) &= \mathbb{E}[h(x_1, x_2)] - y_0 - y_1(x_1) - y_2(x_2) \\
&= a + x_1 + 2x_2 + x_1x_2 - a - x_1 - 2x_2 = x_1x_2.
\end{align*}

It comes as no surprise that in this simple case the fANOVA decomposition does not provide any additional insights, as the isolated effects can be directly seen from the function.
We show this simple example nevertheless to illustrate at which step which assumption is used.
This will make clearer what breaks down when we generalize to dependent variables.

\subsubsection{fANOVA as projection}
In the following we revisit the fANOVA decomposition from the view of orthogonal projections.
For this section the parallel between the (conditional) expected value and orthogonal projections formulated in \cite{Vaart_1998} is crucial.
Having this perspective on the fANOVA decomposition is useful helps in bridging different notations of the method (e.g. via expected value or via integral) and also supports in understanding the generalization of fANOVA later in this section. First we define generally what an orthogonal projection is, and then we will use the idea in the context of fANOVA.\par

\begin{definition}
    Let $\mathcal{G} \subset \mathcal{L}^2$ denote a linear subspace. The projection of $y$ onto $\mathcal{G}$ is defined by the function $\Pi_{\mathcal{G}}y$ which minimizes the distance to $y$ in $\mathcal{L}^2$:
\[
\Pi_{\mathcal{G}}y = \arg\min_{g \in \mathcal{G}} \|y - g\|^2
= \arg\min_{g \in \mathcal{G}} \mathbb{E}[(y - g)^2].
\]
\end{definition}

When we define the constant term $y_\emptyset$ our goal is to best approximate the original function $y$ by a constant function. In other words, we want to minimize the squared difference between $y$ and a constant function $g_0(x) = a$ over all possible constant functions. The solution is the orthogonal projection of $y$ onto the linear subspace of all constant functions $\mathcal{G}_0 = \{g(x) = a; a \in \mathbb{R}\}$. In a probabilistic context, we want to minimize the expected squared different between the random variables $y(\boldsymbol{X})$ and $a$, which turns out to be equivalent to the expected value of the random variable \citep{Vaart_1998}. So intuitively, in the absence of any additional information, the expected value is our best approximation of $y$. More formally we can write:
\begin{align*}
    \Pi_{\mathcal{G}_0}y
    &= \arg \min_{g_0 \in \mathcal{G}_0} \|y - g_0\|^2 \\ % here we still focus on the functions (function space view)
    &= \arg \min_{a_0 \in \mathbb{R}} \mathbb{E}[(y(\boldsymbol{X}) - a)^2] \\ % here we switch to the probabilistic view, focus on RV
    &= \mathbb{E}[y(\boldsymbol{X})] = y_{\emptyset}
\end{align*}
The main effect $y_i(x_i)$ is the projection of $y$ onto the subspace of all functions that only depend on $x_i$, i.e. $\mathcal{G}_i = \{g(x) = g_i(x_i)\}$. There is no need for additional constraints since subtracting lower order terms ensures that orthogonality and zero mean are fulfilled.
The conditional expected value of $\mathbb{E}[y(\boldsymbol{X}) \mid X_i = x_i]$ is the solution to the minimization problem \citep{Vaart_1998}, and the conditional expected value is also a way to express the fANOVA terms \citep{muehlenstaedt2012}:
\begin{align*}
    (\Pi_{\mathcal{G}_i}y)(.) - y_0
    &= \arg \min_{g_i \in \mathcal{G}_i} \|y - g_i\|^2 - y_0\\
    &= \arg \min_{g_i \in \mathcal{G}_i} \mathbb{E}[(y(\boldsymbol{X}) - g_i(X_i))^2] - y_0 \\
    &= \mathbb{E}[y(\boldsymbol{X}) \mid X_i = .] - y_0 = y_i(.)
\end{align*}

The two-way interaction effect $y_{ij}(.,.)$ is the projection of $y$ onto the subspace of all functions that depend on $x_i$ and $x_j$. i.e. $\mathcal{G}_{i,j} = \{g(x) = g_{ij}(x_i, x_j)\}$. Again, we account for lower-order effects by subtracting the constant term and all main effects:
\begin{align*}
    (\Pi_{\mathcal{G}_{ij}}y)(.;.) - (y_0 + y_i(.) + y_j(.))
    &= \arg \min_{g_{ij} \in \mathcal{G}_{ij}} \|y - g_{i, j}\|^2 - (y_0 + y_i(.) + y_j(.))\\
    &= \arg \min_{g_{ij} \in \mathcal{G}_{ij}} \mathbb{E}[(y(\boldsymbol{X}) - g(., .))^2] - (y_0 + y_i(.) + y_j(.))\\
    &= \mathbb{E}[y(\boldsymbol{X}) | X_j = x_j, X_i = x_i] - (y_0 + y_i(.) + y_j(.)) = y_{ij}(.;.)
\end{align*}

In general, we can write for a subset of indices $u \subseteq \{1, \dots, N\}$ and the subspace $\mathcal{G}_u = \{g(\boldsymbol{x}) = g_u(\boldsymbol{x}_u)\}$:
\begin{align*}
    (\Pi_{\mathcal{G}_u}y)(.) - \sum_{v \subsetneq u} y_v(.)
    &= \arg \min_{g_u \in \mathcal{G}_u} \|y - g_u\|^2 - \sum_{v \subsetneq u} y_v(.)\\
    &= \arg \min_{g_u \in \mathcal{G}_{u}} \mathbb{E}[(y(\boldsymbol{X}) - g(.))^2] - \sum_{v \subsetneq u} y_v(.)\\
    &= \mathbb{E}[y(\boldsymbol{X}) | X_{u} = x_u] - \sum_{v \subsetneq u} y_v(x) = y_u(.),
\end{align*}
which means that we project $y$ onto the subspace spanned by the own terms of the fANOVA component to be defined, while accounting for all lower-order terms.\par
On this note, we want to highlight that instead of subtracting the lower order terms from the projection, it is just as valid to first subtract lower order terms and project $y$ on what is left.
We can find both formulations in the literature.
For example, \cite{muehlenstaedt2012} subtracts from the projection and defines:
\begin{align*}
    y_u(\boldsymbol{x}_u) &:=
    \mathbb{E}[y(\boldsymbol{X}) | \boldsymbol{X}_{u} = \boldsymbol{x}_u] - \sum_{v \subsetneq u} y_v(\boldsymbol{x}) \\
    &= \int_{-u} y(\boldsymbol{x}) d \nu(\boldsymbol{x}_{-u}) - \sum_{v \subsetneq u} y_v(\boldsymbol{x}).
\end{align*}
\cite{hooker2004} takes the alternative view and defines the fANOVA components via the integral, which can be rewritten as the expected value:
\begin{align*}
    y_u(\boldsymbol{x}_u)
    &:= \int_{-u} (y(\boldsymbol{x}) - \sum_{v \subsetneq u} y_v(\boldsymbol{x})) d \nu(\boldsymbol{x}_{-u}) \\
    &= \mathbb{E}[y(\boldsymbol{X}) - \sum_{v \subsetneq u} y_v(\boldsymbol{x}) | \boldsymbol{X}_{u} = \boldsymbol{x}_u ].
\end{align*}
The first equivalence in each formulation is simply the definition in each original paper, while the second equivalence holds under the assumption of independent inputs.
% \subsubsection*{Notes \& Clarification}
% Situation: $y(\boldsymbol{X}) \in \Omega, \mathcal{G} \subseteq \Omega, g(\boldsymbol{X}) \in \mathcal{G}$.\par
% \cite{Vaart_1998} tells us that the expected value is equivalent to the projection \cite{muehlenstaedt2012} tells us that the fANOVA terms are equivalent to the conditional expected value.\par


% further analysis of the model via fANOVA decomposition --> variance decomposition
\subsubsection{Second-moment statistics}
No handbook on fANOVA is complete without at least mentioning \textit{Sobol indices}. This requires us to observe the second moment statistics of the decomposition. We already established that:
$$\mathbb{E}[y(\boldsymbol{X})] = y_{\emptyset}.$$

We can also compute the variance of $y(\boldsymbol{X})$ via the fANOVA decomposition.
We write the sum over $u$ for the sum over $\emptyset \neq u \subseteq \{1, \dots, N\}$ and the sum over $u \neq v$ for the sum over $\emptyset \neq u \subseteq \{1, \dots, N\}, \emptyset \neq v \subseteq \{1, \dots, N\}, u \neq v$.
The variance of $y$ is then given by:
\begin{align*}
    \sigma^2 := \mathbb{E}[(y(\boldsymbol{X}) - \mu])^2]
    &= \mathbb{E}[(y_{\emptyset} + \sum_{u} y_u({\boldsymbol{X}_u}) - y_{\emptyset})^2] \\
    &= \mathbb{E}[(\sum_{u} y_u({\boldsymbol{X}_u}))^2] \\
    &= \mathbb{E}[\sum_{u} y_u^2({\boldsymbol{X}_u})] + 2 \mathbb{E}[\sum_{u \neq v} y_u({\boldsymbol{X}_u})  y_v({\boldsymbol{X}_v})] \\
    & = \sum_{u} \mathbb{E}[y_u^2({\boldsymbol{X}_u})]
\end{align*}

We can verify that the variance decomposition holds for our example:
\begin{align*}
    Var(a + X_1 + 2X_2 + X_1 X_2) &= Var(X_1) + 4Var(X_2) + Var(X_1X_2) + 2Cov(X_1, X_2) \\
    &= 1 + 4 \cdot 1 + 1 \cdot 1 + 2 \cdot 0 = 6 \\
    &= \mathbb{E}[X_1^2] + 4\mathbb{E}[X_2^2] + \mathbb{E}[X_1^2]\mathbb{E}[X_2^2] + 2Cov(X_1, X_2) \\
    &= \mathbb{E}[X_1^2] + 4\mathbb{E}[X_2^2] + \mathbb{E}[X_1^2X_2^2] \\
    &= \mathbb{E}[y_1^2] + \mathbb{E}[y_2^2] + \mathbb{E}[y_{12}^2] \\
\end{align*}
Studying the variance of the decomposition was the main focus in early works on this method (see e.g. \cite{sobol1993sensitivity}).
From the variance decomposition \cite{sobol1993sensitivity} construct the \textit{Sobol indices}, which are well-known in sensitivity analysis. As it is only one application of the fANOVA decomposition, we will not go into depth here, but we should keep in mind that the presentation of fANOVA is closely linked to the Sobol indices in many works.
% Second variant of variance decomposition formulated via the expected value
% I could first show this and then show in a Lemma that if the expected value of y^2(X) exists (is smaller than \infinity) then the expected value of y_u^2(X_u) also exists, this would essentially be the translation from Sobols L^2 statement to the expected value wouldn't it?








