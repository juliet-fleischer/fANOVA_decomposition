\subsection{Motivating Example}
Some good example that illustrates how dependent input variables can lead to misleading results and break the fANOVA.
\subsection{Formal Introduction to Generalized fANOVA}
In practice assuming independent input variables is often not realistic. When this assumption is violated, it can lead to misleading results \citep{hooker2007}. The need for a generalization to dependent variables is evident. We base this chapter mainly on the generalization of \cite{rahman2014}, while there exists other work from \cite{hooker2007} or \cite{chastaing2012}.\par

Letting go of the independence assumption means that we no longer work with a product-type probability measure. $f_{\boldsymbol{X}}: \mathbb{R} \rightarrow \mathbb{R}_{0}^{+}$ denotes an arbitrary probability density function and $f_{\boldsymbol{X}_u}: \mathbb{R}^u \rightarrow \mathbb{R}_{0}^{+}$ the marginal probability density function of the subset of variables $u \subseteq d$.\par

The definition of the fANOVA decomposition, and the two main properties - zero mean and orthogonality - can be stated in the same way as for the classical case, with s slight change for the orthogonality. We build now on the weak annihilating conditions instead of the strong ones, which results in hierarchical orthogonality instead of full orthogonality.\par

\begin{definition}
    \textbf{Generalized fANOVA decomposition.}
    We denote the generalized functional fANOVA decomposition as:
    \begin{equation}
        y(\boldsymbol{X}) = \sum_{u \subseteq \{1, \dots, N\}} y_{u, G}(\boldsymbol{X}_u)
    \end{equation}
\end{definition}
The subscript $G$ indicates that we are working with the generalized fANOVA components. The rest is equivalent to the classical fANOVA. The difference comes from how the components look, which we will see in the following. 

\begin{proposition}
    \textbf{Weak annihilating conditions.}
    The weak annihilating conditions are the foundation for the generalized fANOVA decomposition. They are exactly formulated as the strong annihilating conditions, with the exception that the integral uses the joint-probability density function of the variables of interest, instead of the individual marginal probability density functions.
    This makes sense, since for the general case we cannot ensure to recover the marginal densities from the joint density function. {\color{red}{Is this really the reason? Or is the reason: When there are dependencies between variables then the individual pdfs would not assign the ``correct weight'' as they ignore the dependence between features in $u$.}}
    The weak annihilating conditions require that:
\begin{equation}
    \int_{\mathbb{R}} y_{u, G}(\boldsymbol{x}_u) f_{\boldsymbol{X}_u}(\boldsymbol{x}_u) d\nu (x_i) = 0 \quad \text{for} i \in u \neq \emptyset
\end{equation}
\end{proposition}

\begin{proposition}
    % zero mean condition
\end{proposition}

\begin{proposition}
    % hierarchical orthogonality
\end{proposition}


\subsection{Generalization by Hooker}



