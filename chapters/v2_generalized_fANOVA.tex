\subsection{Motivating Example}
Recall our example setup of standard MVN input variables and \(g = a + X_1 + 2X_2 + X_1 X_2\) from the previous section~\ref{classical_fANOVA}.
For classical fANOVA we make the assumption of independent inputs, which is often violated in practice. Let us therefore investigate what happens, when we allow for dependency between variables.

\subsubsection*{Case 2: Dependent Inputs}
Now $\rho_{12} \neq 0$, while keeping everything else the same. When we follow the exact same logic as above we obtain the following terms:
\begin{align*}
\tilde{y}_0 &= \mathbb{E}[g(X_1, X_2)] 
= a + \mathbb{E}[X_1] + 2\mathbb{E}[X_2] + \mathbb{E}[X_1 X_2] \\
&= a + \mathbb{E}[X_1 X_2] 
= a + \left( \text{Cov}(X_1, X_2) + \mathbb{E}[X_1]\mathbb{E}[X_2] \right) \\
&= a + \rho_{12} \\
\tilde{y}_1(x_1) 
&= \mathbb{E}[g(X_1, X_2) | X_1 = x_1] - \tilde{y}_0 \\
&= \mathbb{E}[a + x_1 + 2X_2 + x_1 X_2 | X_1 = x_1] - (a + \rho_{12}) \\
&= a + x_1 + 2\mathbb{E}[X_2 | X_1 = x_1] + x_1 \mathbb{E}[X_2 | X_1 = x_1] - a - \rho_{12} \\
&= x_1 + \rho_{12}(2x_1 + x_1^2 - 1) \\
\tilde{y}_2(x_2) 
&= \mathbb{E}[g(X_1, X_2) \mid X_2 = x_2] - \tilde{y}_0 \\
&= \mathbb{E}[a + X_1 + 2x_2 + X_1 x_2 \mid X_2 = x_2] - (a + \rho_{12}) \\
&= a + 2x_2 + x_2 \mathbb{E}[X_1 \mid X_2 = x_2] - a - \rho_{12} \\
&= 2x_2 + \rho_{12}(x_2 + x_2^2 - 1) \\
\tilde{y}_{12}(x_1, x_2) 
&= g(x_1, x_2) - \tilde{y}_0 - \tilde{y}_1(x_1) - \tilde{y}_2(x_2) \\
&= a + x_1 + 2x_2 + x_1 x_2 - (a + \rho_{12}) \\
&- (x_1 + \rho_{12}(2x_1 + x_1^2 - 1)) - (2x_2 + \rho_{12}(x_2 + x_2^2 - 1)) \\
% &= a + x_1 + 2x_2 + x_1 x_2 - (a + \rho_{12}) - (x_1 + 2\rho_{12} x_1 + \rho_{12} x_1^2 - \rho_{12}) - (2x_2 + \rho_{12} x_2 + \rho_{12} x_2^2 - \rho_{12}) \\
&= x_1 x_2 - 2\rho_{12} x_1 - \rho_{12} x_2  - \rho_{12} x_1^2  - \rho_{12} x_2^2 + \rho_{12}
\end{align*}

The fANOVA components are characterized by two central properties zero mean and orthogonality which follow from \autoref{eq:strong_annihilating_conditions}.
When we check if the components $\tilde{y}_0, \tilde{y}_1, \tilde{y}_2, \tilde{y}_{12}$ satisfy these properties, we find out that all components are zero-centred, but not all are orthogonal to each other. We can, for example, immediately see that checking orthogonality between $\tilde{y}_{1}, \tilde{y}_{1,2}$ will yield the expectation over the constant term $\rho_{1,2}$ exactly once, meaning even if all the other expectations cancel out, this constant will remain and the entire expression will be unequal to zero:
\begin{align*}
    \mathbb{E}(\tilde{y}_1(X_1)\tilde{y}_{1,2}(X_1, X_2)) 
    &= \mathbb{E}[(X_1 + 2\rho_{12}X_1 + \rho_{12}X_1^2 - \rho_{12}) \\
    &\quad \cdot (X_1 X_2 - 2\rho_{12} X_1 - \rho_{12} X_2 - \rho_{12} X_1^2 - \rho_{12} X_2^2 + \rho_{12})] \\
    &= \mathbb{E}[X_{1}^2X_2] \ldots - \mathbb{E}[\rho_{12}^2] \neq 0.
\end{align*}

When we no longer have independent inputs naively computing the ``fANOVA decomposition'' does not yield the fANOVA components as it turns out. What we performed in this example is not the fANOVA decomposition for dependent variables. It is Hoeffding decomposition \citep{hoeffding1948} and results in zero mean but not mutually orthogonal component functions. This shows the need for a more involved approach for generalizing fANOVA.
We basically can see from this example that correlation between features distorts the fANOVA component function, it is not pure anymore but this is the whole point of the fANOVA decomposition and central so that it adds sth. IML.

\subsection{Formal Introduction to Generalized fANOVA}
We base this chapter mainly on the generalization of \cite{rahman2014}, while there exists other work from \cite{hooker2007} or \cite{chastaing2012}. (Write this a bit more detailed: \cite{hooker2007} proofed existence of generalized fANOVA components, proposed estimation scheme, \cite{rahman2014} writes this in more general and measure theoretic fashion and proposes different estimation scheme that he argues is more feasible for high dimensions etc. read more in intro of \cite{rahman2014}; \cite{hooker2007} seems to be viewed as the first one who attempted a generalization to dependent inputs of the entire fANOVA decomposition framework, not just the Sobol indices, and he was inspired by \cite{stone1994}).\par

Letting go of the independence assumption means that we no longer work with a product-type probability measure. $f_{\boldsymbol{X}}: \mathbb{R} \rightarrow \mathbb{R}_{0}^{+}$ denotes an arbitrary probability density function and $f_{\boldsymbol{X}_u}: \mathbb{R}^u \rightarrow \mathbb{R}_{0}^{+}$ the marginal probability density function of the subset of variables $u \subseteq d$. Classical fANOVA boils down to integration w.r.t. the uniform measure and in generalized fANOVA we integrate w.r.t. the distribution of $(X_1, \dots, X_n)$.\par

% How is generalized fANOVA denoted? (bc the definition perse doesn't change, we still write the model as a sum of basis components of increasing dimensionality)
\begin{definition}
    \textbf{Generalized fANOVA decomposition.}
    We denote the generalized functional fANOVA decomposition as:
    \begin{equation}
        y(\boldsymbol{X}) = \sum_{u \subseteq \{1, \dots, N\}} y_{u, G}(\boldsymbol{X}_u)
    \end{equation}
\end{definition}
The subscript $G$ indicates that we are working with the generalized fANOVA components.
The main question is, how one can build fANOVA components that still satisfy the desired properties of zero mean and orthogonality under dependent inputs.

% But how do the components look like now? This is a difference
% So the constraint we set to the optimization problem is modified, milder.
To ensure that these statements hold for the generalized fANOVA components, we need to set the weak annihilating conditions.
They fulfill the same function as the strong annihilating conditions do in the classical case but work with the joint density of the variables of interest, instead of the individual marginal probability density functions.
{\color{red}This makes sense, because when there are dependencies between variables then the individual pdfs would not assign the correct weight to each function value as they ignore the dependence between features in $u$.}
\begin{proposition}
    \textbf{Weak annihilating conditions.}
    To ensure the two desired properties of the generalized fANOVA components (zero mean, hierarchical orthogonality), we require the weak annihilating conditions:
\begin{equation}
    \int_{\mathbb{R}} y_{u, G}(\boldsymbol{x}_u) f_{\boldsymbol{X}_u}(\boldsymbol{x}_u) d\nu (x_i) = 0 \quad \text{for} \quad i \in u \neq \emptyset
\end{equation}
\end{proposition}

% What conditions do they fulfill? Does it differ? Yes, slightly.
If components are constructed in this way, we can ensure that they have zero mean and satisfy a milder from of orthogonality - hierarchical orthogonality, which means that components of different order are orthogonal to each other while components of the same order are not.
\begin{proposition}
    % zero mean condition
    The generalized fANOVA components $y_{u, G}$, with $\emptyset \neq u \subseteq \{1, \ldots, N\}$, are centred around zero:
\begin{equation}
    \mathbb{E}[y_{u, G}(\boldsymbol{X}_u)] := \int y_{u, G}(\boldsymbol{x}_u) f_{\boldsymbol{X}}(\boldsymbol{x}) \, d\nu (\boldsymbol{x}) = 0
    \label{eq:zero_mean_g}
\end{equation}
\end{proposition}

\begin{proof}
For any subset $\emptyset \ne u \subseteq \{1, \ldots, N\}$, let $i \in u$. We assume that the weak annihilating conditions hold. Then
\begin{align*}
\mathbb{E}[y_{u,G}(\mathbf{X}_u)] 
&:= \int_{\mathbb{R}^N} y_{u,G}(\mathbf{x}_u) f_{\mathbf{X}}(\mathbf{x})\, d\mathbf{x} \\
&= \int_{\mathbb{R}^{|u|}} y_{u,G}(\mathbf{x}_u) \left( \int_{\mathbb{R}^{N - |u|}} f_{\mathbf{X}}(\mathbf{x}) \, d\mathbf{x}_{-u} \right) d\mathbf{x}_u \\
&= \int_{\mathbb{R}^{|u|}} y_{u,G}(\mathbf{x}_u) f_u(\mathbf{x}_u)\, d\mathbf{x}_u \\
&= \int_{\mathbb{R}^{|u| - 1}} \left( \int_{\mathbb{R}} y_{u,G}(\mathbf{x}_u) f_u(\mathbf{x}_u) \, dx_i \right) \prod_{j \in u,\, j \ne i} dx_j \\
&= 0,
\end{align*}
where we make use of Fubini's theorem and the last line follows from using the weak annihilating condition %~(4.2).
\end{proof}

\begin{proposition}
    % hierarchical orthogonality
    The fANOVA components are hierarchically orthogonal. This means that for two components $y_{u, G}$ and $y_{v, G}$ with $u \subsetneq v, \emptyset \neq u \subseteq \{1, \ldots, N\}, \emptyset \neq v \subseteq \{1, \ldots, N\} $ it holds that:
\begin{equation}
    \mathbb{E}[y_{u, G}(\boldsymbol{X}_u)y_{v, G}(\boldsymbol{X}_v)] := \int y_{u, G}(\boldsymbol{x}_u) y_{v, G}(\boldsymbol{X}_v) f_{\boldsymbol{X}}(\boldsymbol{x}) \, d\nu (\boldsymbol{x}) = 0
\end{equation}
\label{eq:orthogonality_g}
\end{proposition}

\begin{proof}
For any two subsets $\emptyset \ne u \subseteq \{1,\dots,N\}$ and $\emptyset \ne v \subseteq \{1,\dots,N\}$, where $v \subsetneq u$, the subset $u = v \cup (u \setminus v)$. Let $i \in (u \setminus v) \subseteq u$. Then
\begin{align*}
\mathbb{E}[y_{u,G}(\mathbf{X}_u) \, y_{v,G}(\mathbf{X}_v)]
&:= \int_{\mathbb{R}^N} y_{u,G}(\mathbf{x}_u) y_{v,G}(\mathbf{x}_v) f_{\mathbf{X}}(\mathbf{x}) \, d\mathbf{x} \\
&= \int_{\mathbb{R}^{|u|}} y_{u,G}(\mathbf{x}_u) y_{v,G}(\mathbf{x}_v) \left( \int_{\mathbb{R}^{N - |u|}} f_{\mathbf{X}}(\mathbf{x}) \, d\mathbf{x}_{-u} \right) d\mathbf{x}_u \\
&= \int_{\mathbb{R}^{|u|}} y_{u,G}(\mathbf{x}_u) y_{v,G}(\mathbf{x}_v) f_u(\mathbf{x}_u) \, d\mathbf{x}_u \\
&= \int_{\mathbb{R}^{|v|}} y_{v,G}(\mathbf{x}_v)
    \int_{\mathbb{R}^{|u \setminus v|}} y_{u,G}(\mathbf{x}_u) f_u(\mathbf{x}_u) \, d\mathbf{x}_{u \setminus v} \, d\mathbf{x}_v \\
&= \int_{\mathbb{R}^{|v|}} y_{v,G}(\mathbf{x}_v)
    \int_{\mathbb{R}^{|u \setminus v| - 1}} \left( \int_{\mathbb{R}} y_{u,G}(\mathbf{x}_u) f_u(\mathbf{x}_u) \, dx_i \right)
    \prod_{\substack{j \in (u \setminus v) \\ j \ne i}} dx_j \, d\mathbf{x}_v \\
&= 0.
\end{align*}
Repeatedly using Fubini's theorem and the weak annihilating conditions the equality to zero follows.
\end{proof}

A key contribution from \cite{hooker2007} and \cite{rahman2014} is that they construct a generalization of the fANOVA decomposition method as a whole, not only parts, such as the Sobol indices.
This means it is important that Rahman's generalized statements reduce to the classical case under product-type pdf. We can show that the weak annihilating conditions become the strong annihilating conditions under independence assumption.

\begin{proof}
Assume that the random variables $\{X_j\}_{j \in u}$ are independent. Then we can factorize the marginal density $f_u(\mathbf{x}_u)$ as
\[
f_u(\mathbf{x}_u) = \prod_{j \in u} f_{\{j\}}(x_j).
\]
Now consider the weak annihilating condition~(4.2) for some $i \in u \neq \emptyset$:
\[
\int_{\mathbb{R}} y_{u,G}(\mathbf{x}_u) f_u(\mathbf{x}_u) \, dx_i = 0.
\]
Since we assume independence, we can substitute the joint marginal density with the product of the marginal densities:
\[
\int_{\mathbb{R}} y_{u,G}(\mathbf{x}_u) \left( \prod_{j \in u} f_{\{j\}}(x_j) \right) dx_i.
\]
For fixed $x_j$ with $j \ne i$, the terms $f_{\{j\}}(x_j)$ are constant with respect to $x_i$, and can therefore be pulled out of the integral:
\[
\left( \prod_{j \in u,\, j \ne i} f_{\{j\}}(x_j) \right) \int_{\mathbb{R}} y_{u,G}(\mathbf{x}_u) f_{\{i\}}(x_i) \, dx_i = 0.
\]
As product of pdfs the prefactor is strictly positive for all $x_j$ with $j \ne i$. Therefore, the integral must be zero for the equality to hold:
\[
\int_{\mathbb{R}} y_{u,G}(\mathbf{x}_u) f_{\{i\}}(x_i) \, dx_i = 0,
\]
which are the strong annihilating conditions from the previous section.
\end{proof}


\subsubsection*{Construction of the Generalized fANOVA Terms}
Rahman basically says, that there is no need for a computationally expensive least squares problem proposed by \cite{hooker2007}. In the same fashion as for the classical fANOVA decomposition, one can integrate the desired decomposition w.r.t. to a suitable pdf and use the (weak) annihilating conditions to obtain the fANOVA decomposition. They key is to find this suitable pdf which results in an integral as we want it. \cite{rahman2014} says this suitable pdf is the $f_{-u}(\boldsymbol{x}_{-u})$. In Theorem 4.4. he shows how the generalized fANOVA components constructed by this look like. And Lemma 4.3 is a helping statement, that should cover all the integral cases that appear in Theorem 4.4. and allow us to write the expressions in Theorem 4.4. in the reduced form we see them. So I should be able to find the cases of Lemma 4.3. in the building of Theorem 4.4.

To construct the generalized fANOVA components we first need a lemma that dinstiguishes three cases of integration that we will encounter in the construction of the generalized components.

\begin{lemma}
Consider the generalized fANOVA components $y_{v,G}$, $\emptyset \ne v \subseteq \{1,\dots,N\}$, of a square-integrable function $y : \mathbb{R}^N \to \mathbb{R}$. When integrated w.r.t. the probability measure $f_{-u}(\boldsymbol{x}_{-u})\, d\boldsymbol{x}_{-u}$, $u \subseteq \{1,\dots,N\}$, one can distinguish three cases:
\[
\int_{\mathbb{R}^{N - |u|}} y_{v,G}(\mathbf{x}_v) f_{-u}(\mathbf{x}_{-u}) \, d\mathbf{x}_{-u}
=
\begin{cases}
\displaystyle \int_{\mathbb{R}^{|v \cap -u|}} y_{v,G}(\mathbf{x}_v) f_{v \cap -u}(\mathbf{x}_{v \cap -u}) \, d\mathbf{x}_{v \cap -u}, & \text{if } v \cap u \ne \emptyset \text{ and } v \not\subset u, \\
y_{v,G}(\mathbf{x}_v), & \text{if } v \cap u \ne \emptyset \text{ and } v \subseteq u, \\
0, & \text{if } v \cap u = \emptyset.
\end{cases}
\]
\end{lemma}

\begin{proof}
Let $u \subseteq \{1,\dots,N\}$ and $\emptyset \ne v \subseteq \{1,\dots,N\}$. We distinguish between three types of relationship between $v$ and $u$.

Before analyzing the first case, note that for any such $u$ and $v$, it is possible to write
\[
(v \cap -u) \subseteq -u \quad \text{and} \quad -u = (-u \setminus (v \cap -u)) \cup (v \cap -u),
\]
which will be used in the integral decomposition below.

\paragraph{Case 1: \( v \cap u \ne \emptyset \) and \( v \not\subset u \).}
We use the decomposition of $-u$ stated above to decompose the integration over $\mathbf{x}_{-u}$ as:
\[
\int_{\mathbb{R}^{N - |u|}} y_{v,G}(\mathbf{x}_v) f_{-u}(\mathbf{x}_{-u}) \, d\mathbf{x}_{-u}
= \int_{\mathbb{R}^{|v \cap -u|}} y_{v,G}(\mathbf{x}_v)
\left( \int_{\mathbb{R}^{N - |u| - |v \cap -u|}} f_{-u}(\mathbf{x}_{-u}) \, d\mathbf{x}_{-u \setminus (v \cap -u)} \right)
d\mathbf{x}_{v \cap -u}.
\]
The inner integral gives the marginal density $f_{v \cap -u}(\mathbf{x}_{v \cap -u})$, so we obtain:
\[
= \int_{\mathbb{R}^{|v \cap -u|}} y_{v,G}(\mathbf{x}_v) f_{v \cap -u}(\mathbf{x}_{v \cap -u}) \, d\mathbf{x}_{v \cap -u}.
\]

\paragraph{Case 2: $v \cap u \ne \emptyset \text{ and } v \subseteq u$.}
Since the sets $v$ and $-u$ are then completely disjoint, $y_{v,G}(\mathbf{x}_v)$ is independent of $\mathbf{x}_{-u}$ and can be pulled out of the integral:
\[
\int_{\mathbb{R}^{N - |u|}} y_{v,G}(\mathbf{x}_v) f_{-u}(\mathbf{x}_{-u}) \, d\mathbf{x}_{-u}
= y_{v,G}(\mathbf{x}_v) \int_{\mathbb{R}^{N - |u|}} f_{-u}(\mathbf{x}_{-u}) \, d\mathbf{x}_{-u}
= y_{v,G}(\mathbf{x}_v),
\]
which works because $f_{-u}$ is a pdf.

\paragraph{Case 3: \( v \cap u = \emptyset \).}
In this case, we have \( v \subseteq -u \), so \( v \cap -u = v \). Then we can write:
\[
\begin{aligned}
\int_{\mathbb{R}^{N - |u|}} y_{v,G}(\mathbf{x}_v) f_{-u}(\mathbf{x}_{-u}) \, d\mathbf{x}_{-u}
&= \int_{\mathbb{R}^{|v|}} y_{v,G}(\mathbf{x}_v)
\left( \int_{\mathbb{R}^{N - |u| - |v|}} f_{-u}(\mathbf{x}_{-u}) \, d\mathbf{x}_{-u \setminus v} \right)
d\mathbf{x}_v \\
&= \int_{\mathbb{R}^{|v|}} y_{v,G}(\mathbf{x}_v) f_v(\mathbf{x}_v) \, d\mathbf{x}_v \\
&= \int_{\mathbb{R}^{|v|-1}} \left( \int_{\mathbb{R}} y_{v,G}(\mathbf{x}_v) f_v(\mathbf{x}_v) \, dx_i \right)
\prod_{\substack{j \in v \\ j \ne i}} dx_j \\
&= 0,
\end{aligned}
\]
while we again split the interval in such a way that we recognize the marginal density $f_v$ and make use of the zero mean property from the strong annihilating conditions.

\end{proof}


\begin{theorem}
The generalized fANOVA component functions \( y_{u,G}(\mathbf{x}_u) \) can be recursively defined via the following set of equations:
\begin{align}
y_{\emptyset,G} &= \int_{\mathbb{R}^N} y(\mathbf{x}) f_{\mathbf{X}}(\mathbf{x}) \, d\mathbf{x}, \tag{4.5a} \\
y_{u,G}(\mathbf{X}_u) &= \int_{\mathbb{R}^{N - |u|}} y(\mathbf{X}_u, \mathbf{x}_{-u}) f_{-u}(\mathbf{x}_{-u}) \, d\mathbf{x}_{-u}
- \sum_{v \subset u} y_{v,G}(\mathbf{X}_v) \notag \\
&\quad - \sum_{\substack{\emptyset \ne v \subseteq \{1,\dots,N\} \\ v \cap u \ne \emptyset,\ v \not\subset u}} 
\int_{\mathbb{R}^{|v \cap -u|}} y_{v,G}(\mathbf{X}_{v \cap u}, \mathbf{x}_{v \cap -u}) f_{v \cap -u}(\mathbf{x}_{v \cap -u}) \, d\mathbf{x}_{v \cap -u}. \tag{4.5b}
\end{align}
\end{theorem}


\begin{proof}
We begin by integrating both sides of the generalized fANOVA decomposition
\[
y(\mathbf{x}) = \sum_{v \subseteq \{1,\dots,N\}} y_{v,G}(\mathbf{x}_v)
\]
w.r.t. $f_{-u}(\mathbf{x}_{-u})\, d\mathbf{x}_{-u}$, replacing $\boldsymbol{X}$ by $\boldsymbol{x}$, and changing the dummy index from $u$ to $v$. This yields:
\[
\int_{\mathbb{R}^{N - |u|}} y(\mathbf{x}) f_{-u}(\mathbf{x}_{-u}) \, d\mathbf{x}_{-u}
= \sum_{v \subseteq \{1,\dots,N\}} \int_{\mathbb{R}^{N - |u|}} y_{v,G}(\mathbf{x}_v) f_{-u}(\mathbf{x}_{-u}) \, d\mathbf{x}_{-u}.
\]

\paragraph{Case \( u = \emptyset \): computing the constant term.}
We set $u = \emptyset$, so $-u = \{1,\dots,N\}$ and $f_{-u}(\boldsymbol{x}_{-u}) = f_{\boldsymbol{X}}(\boldsymbol{x})$. The above integral can then be written as:
\[
\int_{\mathbb{R}^N} y(\mathbf{x}) f_{\mathbf{X}}(\mathbf{x}) \, d\mathbf{x}
= \sum_{v \subseteq \{1,\dots,N\}} \int_{\mathbb{R}^N} y_{v,G}(\mathbf{x}_v) f_{\mathbf{X}}(\mathbf{x}) \, d\mathbf{x}
\]
\[
= \int_{\mathbb{R}^N} y_{\emptyset,G} \, f_{\mathbf{X}}(\mathbf{x}) \, d\mathbf{x}
+ \sum_{\emptyset \ne v \subseteq \{1,\dots,N\}} \int_{\mathbb{R}^N} y_{v,G}(\mathbf{x}_v) f_{\mathbf{X}}(\mathbf{x}) \, d\mathbf{x}
\]
\[
= y_{\emptyset,G} + \sum_{\emptyset \ne v \subseteq \{1,\dots,N\}} \mathbb{E}[y_{v,G}(\mathbf{X}_v)] = y_{\emptyset,G},
\]
where the last sum vanishes under the weak annihilating condition.

\paragraph{Case \( \emptyset \ne u \subseteq \{1,\dots,N\} \): computing nonconstant terms.}
We return to the integrated decomposition
\[
\int_{\mathbb{R}^{N - |u|}} y(\mathbf{x}) f_{-u}(\mathbf{x}_{-u}) \, d\mathbf{x}_{-u}
= \sum_{v \subseteq \{1,\dots,N\}} \int_{\mathbb{R}^{N - |u|}} y_{v,G}(\mathbf{x}_v) f_{-u}(\mathbf{x}_{-u}) \, d\mathbf{x}_{-u},
\]
and apply Lemma~4.3 to evaluate each term in the sum according to the relationship between $v$ and $u$:

\begin{itemize}
  \item[\textbf{(A)}] \( v \cap u \ne \emptyset \) and \( v \not\subset u \): \\
  This is Case 1 of Lemma~4.3. The integral becomes:
  \[
  \sum_{\substack{\emptyset \ne v \subseteq \{1,\dots,N\} \\ v \cap u \ne \emptyset,\ v \not\subset u}} 
  \int_{\mathbb{R}^{|v \cap -u|}} y_{v,G}(\mathbf{x}_v) f_{v \cap -u}(\mathbf{x}_{v \cap -u}) \, d\mathbf{x}_{v \cap -u}.
  \]

  \item[\textbf{(B)}] \( v \subsetneq u \): \\
  This is Case 2 of Lemma~4.3. The integrals reduce to the component functions themselves:
  \[
  \sum_{v \subsetneq u} y_{v,G}(\mathbf{x}_v).
  \]

  \item[\textbf{(C)}] \( v = u \): \\
  Also part of Case 2 of Lemma~4.3. The integral becomes:
  \[
  y_{u,G}(\mathbf{x}_u).
  \]

  \item[\textbf{(D)}] \( v \cap u = \emptyset \): \\
  Case 3 of Lemma~4.3. These terms vanish:
  \[
  \sum_{\substack{v \subseteq \{1,\dots,N\} \\ v \cap u = \emptyset}} 0 = 0.
  \]
\end{itemize}

Putting everything together:
\[
\int_{\mathbb{R}^{N - |u|}} y(\mathbf{x}) f_{-u}(\mathbf{x}_{-u}) \, d\mathbf{x}_{-u}
= y_{u,G}(\mathbf{x}_u)
+ \sum_{v \subsetneq u} y_{v,G}(\mathbf{x}_v)
+ \sum_{\substack{\emptyset \ne v \subseteq \{1,\dots,N\} \\ v \cap u \ne \emptyset,\ v \not\subset u}} 
\int_{\mathbb{R}^{|v \cap -u|}} y_{v,G}(\mathbf{x}_v) f_{v \cap -u}(\mathbf{x}_{v \cap -u}) \, d\mathbf{x}_{v \cap -u}.
\]

Rearranging gives the almost final expression for \( y_{u,G}(\mathbf{x}_u) \):
\[
y_{u,G}(\mathbf{x}_u)
= \int_{\mathbb{R}^{N - |u|}} y(\mathbf{x}) f_{-u}(\mathbf{x}_{-u}) \, d\mathbf{x}_{-u}
- \sum_{v \subsetneq u} y_{v,G}(\mathbf{x}_v)
- \sum_{\substack{\emptyset \ne v \subseteq \{1,\dots,N\} \\ v \cap u \ne \emptyset,\ v \not\subset u}} 
\int_{\mathbb{R}^{|v \cap -u|}} y_{v,G}(\mathbf{x}_v) f_{v \cap -u}(\mathbf{x}_{v \cap -u}) \, d\mathbf{x}_{v \cap -u}.
\]
As a last step, we only have to write \( v = (v \cap u) \cup (v \cap -u) \) to obtain the expression of Theorem~5.1.

\end{proof}

% Alternative definition of components by Hooker
\cite{hooker2007} offers an alternative definition of the generalized fANOVA components\footnote{We modified the notation from the original work to match the notation of \cite{rahman2014} and the rest of this thesis.}:
\begin{equation}
\left\{ y_{u, G}(\boldsymbol{x}_u) \,\middle|\, u \subseteq d \right\}
= \arg\min_{\{g_u \in L^2(\mathbb{R}^u)\}_{u \subseteq d}} 
\int \left( \sum_{u \subseteq d} g_u(\boldsymbol{x}_u) - y(\boldsymbol{x}) \right)^2 f_{\boldsymbol{X}}(\boldsymbol{x}) \, d \nu (\boldsymbol{x})
\label{eq:generalized_fanova_components_hooker}
\end{equation}
In Hookers definition we recognize a projection problem. We are simultaneously finding the set of components functions $g_u$ that minimize the weighted squared difference to the original function $y$ (under zero mean and hierarchical orthogonality constraint), which is exactly the definition of a projection of $y$ onto a specific subspace $\mathcal{G}$, which we defined generally in section ~\ref{background}.\par

A crucial difference to the classical case is that both versions of the generalized components are defined in dependence of each other (\autoref{eq:generalized_fanova_components_rahman}, \autoref{eq:generalized_fanova_components_hooker}).

Let us come back to our example from the beginning. The goal is to write
\[
g(x_1, x_2) = y_{\emptyset, G} + y_{1, G}(x_1) + y_{2, G}(x_2) + y_{1,2, G}(x_1, x_2)
\]
under dependent inputs. We present two ways in which the problem solution can be stated.\par

\subsubsection*{Rahman method}
The system to find the generalized fANOVA components for $g$ according to \cite{rahman2014} method looks as follows:
\begin{align*}
    y_{\emptyset, G} &= \int_{\mathbb{R}^2} g(x_1, x_2)\, f(x_1, x_2)\, dx_1 dx_2 \\
    y_{1, G}(x_1) &= \int_{\mathbb{R}} g(x_1, x_2)\, f_2(x_2)\, dx_2
    - y_{\emptyset, G}
    - \int_{\mathbb{R}} y_{\{1,2\}, G}(x_1, x_2)\, f_2(x_2)\, dx_2\\
    y_{2, G}(x_2) &= \int_{\mathbb{R}} g(x_1, x_2)\, f_1(x_1)\, dx_1
    - y_{\emptyset, G}
    - \int_{\mathbb{R}} y_{\{1,2\}, G}(x_1, x_2)\, f_1(x_1)\, dx_1\\
    y_{1,2, G}(x_1, x_2) &= g(x_1, x_2) - y_{\emptyset, G} - y_{\{1\}, G}(x_1) - y_{\{2\}, G}(x_2)
\end{align*}

Since the components form a coupled system where the components are defined in interdependence of each other, finding the solution is not straight forward, even for simple examples.

\subsubsection*{Hooker method}
An alternative way to phrase the problem can be found in \cite{hooker2007}.
To find the generalized fANOVA components, we can formulate a minimization problem for each of them. 
\begin{align*}
y_{\emptyset} 
&= \arg\min_{c \in \mathbb{R}} \int_{\mathbb{R}^2} 
\left( g(x_1, x_2) 
- \left( c + y_{\{1\}}(x_1) + y_{\{2\}}(x_2) + y_{\{1,2\}}(x_1, x_2) \right) \right)^2 
f(x_1, x_2)\, dx_1 dx_2 \\[1em]
y_{1}(x_1) 
&= \arg\min_{h_1 \in L^2(\mathbb{R})} \int_{\mathbb{R}^2} 
\left( g(x_1, x_2) 
- \left( y_{\emptyset} + h_1(x_1) + y_{\{2\}}(x_2) + y_{\{1,2\}}(x_1, x_2) \right) \right)^2 
f(x_1, x_2)\, dx_1 dx_2 \\[1em]
y_{2}(x_2) 
&= \arg\min_{h_2 \in L^2(\mathbb{R})} \int_{\mathbb{R}^2} 
\left( g(x_1, x_2) 
- \left( y_{\emptyset} + y_{\{1\}}(x_1) + h_2(x_2) + y_{\{1,2\}}(x_1, x_2) \right) \right)^2 
f(x_1, x_2)\, dx_1 dx_2 \\[1em]
y_{1,2}(x_1, x_2) 
&= \arg\min_{h_{12} \in L^2(\mathbb{R}^2)} \int_{\mathbb{R}^2} 
\left( g(x_1, x_2) 
- \left( y_{\emptyset} + y_{\{1\}}(x_1) + y_{\{2\}}(x_2) + h_{12}(x_1, x_2) \right) \right)^2 
f(x_1, x_2)\, dx_1 dx_2
\end{align*}



The least-squares problems are solved subject to the following constraints, which ensure that the resulting components are zero centred and hierarchically orthogonal:
\begin{align*}
\int_{\mathbb{R}^2} y_{\{1\}}(x_1) \cdot f(x_1, x_2)\, dx_1 dx_2 &= 0 \\[1ex]
\int_{\mathbb{R}^2} y_{\{2\}}(x_2) \cdot f(x_1, x_2)\, dx_1 dx_2 &= 0 \\[1.4ex]
\int_{\mathbb{R}} y_{\{1,2\}}(x_1, x_2) \cdot f(x_1, x_2)\, dx_1 &= 0 \quad \forall x_2 \\[1ex]
\int_{\mathbb{R}} y_{\{1,2\}}(x_1, x_2) \cdot f(x_1, x_2)\, dx_2 &= 0 \quad \forall x_1
\end{align*}


Conceptually \cite{hooker2007} is doing nothing other than a projection. Earlier, we established that a projection is the same as the conditional expected value, and fANOVA can be expressed via the conditional expected value. This means from the initial idea, we do not change anything apart from the fact that we have to integrate via the joint pdf, but this is something one is ``forced'' to under dependence, not something one ``invents''. However, projections onto subspaces become more difficult under dependence; therefore, setting these constraints explicitly is necessary to ensure (hierarchical) orthogonality.\par
Obtaining an analytical solution for either of the methods is tedious even for our simple example. We leave it at the problem formulation, so that we have the comparison between which problem one has to solve the classical case versus the generalized case. In the next section, we sketch ways to estimate the fANOVA components conceptually.

% Now a correction of the example from the beginning would need to follow...
% --> If not what we thought, what are the real generalized fANOVA components then??
% but maybe i have to come to the (very unsatifying) end that i cannot correct the above components 
% because solving this coupled system is too complex...?

% estimating them is more involved, we need to introduce this in the next section
% more or less formal intro to Hookers general estimation approach

\subsubsection*{Second-moment statistics}
Maybe explain conceptually the findings of \cite{rahman2014} on the properties of his generalized fANOVA components.


