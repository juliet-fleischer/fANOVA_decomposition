% The mathematics of cooperation and attribution.

\subsection*{Questions}
\subsubsection*{Clarifying the connection between fANOVA, expected value, and projection \& bringing together different definitions}
When I start from \cite{muehlenstaedt2012} and basically go the way: \textit{fANOVA term $\rightarrow$ expected value $\rightarrow$ projection}, I arrive at the formulation where we first compute the projection and afterwards subtract lower order terms. For example following the definition by \cite{muehlenstaedt2012} and using the parallel between (conditional) expected value and the projection \citep{vanravenzwaaij2018} we can write for example for $u = \{1,2\}$:
\begin{align*}
    y_{12}
    &:= \mathbb{E}[y(\boldsymbol{X}) \mid X_1 = x_1, X_2 = x_2] - y_\emptyset - y_{\{1\}}(x_1) - y_{\{2\}}(x_2) \\
    &= \arg\min_{g_{\{1,2\}} \in \mathcal{G}_{\{1,2\}}} \mathbb{E}\left[(y(\boldsymbol{X}) - g_{\{1,2\}}(X_1, X_2))^2\right] - y_\emptyset - y_{\{1\}}(x_1) - y_{\{2\}}(x_2) \\
    &= (\Pi_{\mathcal{G}_{\{1,2\}}}y(\boldsymbol{X})) - y_\emptyset - y_{\{1\}}(x_1) - y_{\{2\}}(x_2)
\end{align*}

But when I get it correctly \cite{hooker2007} writes his generalized fANOVA components as the projection of the differences. Also, he goes the other way around, starting from the projections (and we could restate this as the conditional expected value).

Because Hooker defines the fANOVA terms as:
\begin{equation}
\left\{ f_u(x_u) \,\middle|\, u \subseteq d \right\}
= \arg\min_{\{g_u \in L^2(\mathbb{R}^u)\}_{u \subseteq d}} 
\int \left( y(\boldsymbol{x}) - \sum_{u \subseteq d} g_u(x_u) \right)^2 w(\boldsymbol{x}) \, d\boldsymbol{x}
\label{eq:fanova_decomposition_generalized}
\end{equation}

And I think we can rewrite this as the conditional expected value. For example for $u = \{1,2\}$:
\begin{align*}
    y_{12}(.;.) &:= \arg \min_{g_{\{1,2\}} \in L^2(\mathbb{R}^2)} \int \left( y(\boldsymbol{x}) - \sum_{\{1,2\}} g_{\{1,2\}}(x_1, x_2) \right)^2 w(\boldsymbol{x}) \, d\boldsymbol{x} \\
    &= \arg \min_{g_{\{1,2\}} \in L^2(\mathbb{R}^2)} \int \left( y(\boldsymbol{x}) - y_\emptyset - y_{\{1\}}(x_1) - y_{\{2\}}(x_2) - g_{\{1,2\}}(x_1, x_2) \right)^2 w(\boldsymbol{x}) \, d\boldsymbol{x} \\
    &= \arg \min_{g_{\{1,2\}} \in L^2(\mathbb{R}^2)} \mathbb{E}[(y(\boldsymbol{X}) - y_\emptyset - y_{\{1\}}(x_1) - y_{\{2\}}(x_2) - g_{\{1,2\}}(x_1, x_2))^2] \\
    &= \Pi_{\mathcal{G}_{\{1,2\}}}(y - y_\emptyset - y_{\{1\}}(x_1) - y_{\{2\}}(x_2)) \\
    &= \mathbb{E}[y(\boldsymbol{X}) - y_{\emptyset} - y_{\{1\}}(x_1) - y_{\{2\}}(x_2) | X_1 = x_1, X_2 = x_2]
\end{align*}
% noch alles im im E-Wert muss groß sein!

So \cite{hooker2007} defines the fANOVA terms via the projection \textit{fANOVA term $\rightarrow$ projection $\rightarrow$ expected value (not in Hooker)}. But he takes the projection of the differences. 

\begin{itemize}
    \item Could it happen that based on fANOVA decomposition we build a model which uses the interaction effect of $i,j$ but not the main effects $i$ and/or $j$? 
    \item Can I really compute the generalized fANOVA terms a proposed by \cite{hooker2007} by hand? \href{https://christophm.github.io/interpretable-ml-book/decomposition.html}{Molnar} writes: ``The estimation is done on a grid of points in the feature space and is stated as a minimization problem that can be solved using regression techniques. However, the components cannot be computed independently of each other, nor hierarchically, but a complex system of equations involving other components has to be solved. The computation is therefore quite complex and computationally intensive.'' \textit{If he would write he generalized fANOVA as conditional expected values it is actually not that complicated and we simply would need to solve regression problems hierarchically.}
    \item I am still confused if setting the zero-mean constraint for $X_i \overset{\text{i.i.d.}}{\sim} \mathcal{U}[0, 1]$ is essentially saying that we centre the distribution and now assume $X_i \overset{\text{i.i.d.}}{\sim} \mathcal{U}[-1, 1]$. So can we, instead of explicitly stating the zero-mean constraint just assume $X_i \overset{\text{i.i.d.}}{\sim} \mathcal{U}[-1, 1]$? And following the same principle we would shift other distributions by altering their parameters, not by explicitly stating the zero-mean constraint? Then for the standard normal distribution we wouldn't need to do anything, it is already centred around 0. For other distributions we would need to change, and for some it doesn't make inhaltlichen Sinn e.g. Poisson distribution? \textit{No generally zero-mean constraint and distribution assumption of the variables has no connection. The zero-mean constraint is sth. we set for the fANOVA terms $y_u$ while distribution assumption is about the input variables $X_i$. We don't set the zero-mean constraint for the variables"}
    \item 
    \item fANOVA decomposition via the integral, how would the zero mean constraint look here? (see ``General\_fANOVA\_handnotes'')
    \item Can you reconstruct the function from only the fANOVA terms? I think it can be reconstructed only if variables are independent, have zero-mean, are orthogonal?
    \item Is it possible to perform fANOVA for non-square-integrable functions? \textit{in general yes but the variance decomposition doesn't work then or might have problems.}
    \item fANOVA decomposition for discrete variables possible? Does it make sense even?
    % \item Denote original model with $y$ or $f$?
    \item Connection between the (conditional) expected value, (partial) integral, projections (section~\ref{general_definitions})?
    \item In the hierarchical orthogonality condition (4.2) formulated in \cite{hooker2007} for the gerneralized fAVNOA framework, shouldn't we explicitly exclude the case that $v = u$, because then, we would require that the inner product of the fANOVA component is zero wouldn't we (section~\ref{generalization})?
    % \item If the general fANOVA formulation is a true generalization then I should be able to take the general form and construct from it the simple form when I use certain constrains (such as independence, uniform distribution, etc.) right?
    \item Why is it a problem, when explainability methods also place large emphasis on regions of low probability mass when dependencies between variables exist - because in the end explainability is about explaining the model, not the data generating process; and after all it is how the model works in these regions. [But as the Hooker example illustrates, how the model works and what it estimates in these regions is wrong and then it's better to not report any model behaviour or come closer to the DGP than to give wrong estimations?]
    \item 
    \item Use of AI tools?
    \item Do we need to restrict ourselves to the unit hypercube? Or does fANOVA decomposition work in general, but maybe with some constraints? Originally it was constructed for models on the unit hypercube $[0,1]$, but other papers also use models from $R^d$ \textit{Generally no restriction, so next step could be to generalize, to $\mathbb{R}^n$, other measures, dependent variables}
    \item Still unclear: Are the terms fully orthogonal or hierarchically? See subsection on Orthogonality of the fANOVA terms (especially the example) I think in the original fANOVA decomposition the terms are orthogonal but in the generalized fANOVA \citep{hooker2007} they are hierarchically orthogonal. \textit{fully orthogonal when independence assumption, probably partially when no independence}
    \item $x_1, \dots, x_k$ are simply the standardized features, right? \textit{Yes}
    \item {\color{orange}My current understanding: we need independence of $x_1, \dots x_k$ so that fANOVA decomposition is unique (and orthogonality holds). We need zero-mean constraint for the orthogonality of the components. We need orthogonality for the variance decomposition.}\textit{zero-mean $\rightarrow$ orthogonality $\rightarrow$ uniqueness; Lemma 1 im Hooker 2007 ist verallgemeinerg ds zero-mean constraint}
    \item Next step might be to investigate the (mathematical) parallels of fANOVA decomposition and other IML methods (PDP, ALE, SHAP), e.g. there is definitely a strong relationship between Partial dependence (PD) and fANOVA terms, and PD is itself again related to other IML methods; Also look how are other IML models studied and study fANOVA in a similar way (e.g. other IML methods are defined, checked for certain properties, examined under different conditions (dependent features, independent features) etc.) (see dissertation by Christoph Molnar for this); Also I would be very interested in investigating the game theory paper further \citep{fumagalli2025} but still a bit unsure if it is too complex.
    \item Why does a fANOVA decomposition of a simple GAM not lead to the ``true'' coefficients? \href{https://christophm.github.io/interpretable-ml-book/decomposition.html}{https://christophm.github.io/interpretable-ml-book/decomposition.html} talks about this a bit in the subchapter ``Statistical regression models'' \textit{It should actually lead to the GAM; at least under all the constraint like zero-mean constraint and orthogonality}
    \item 
    \item In \cite{hooker2004} they work with $F(x)$ and $f(x)$, but in \cite{sobol2001} they only work with $f(x)$. I think this is only notation? \textit{Only notation.}
    \item Does orthogonality in fANOVA context mean that all terms are orthogonal to each other? Or that a term is orthogonal to all lower-order terms (\ldq Hierarchical orthogonality \rdq)? \textit{The terms are hierarchically orthogonal, so each term is orthogonal to all lower-order terms, but not to the same-order terms! So $f_1$ is not necessarily orthogonal to $f_2$ but it is orthogonal to $f_{12}$, $f_{0}$.} 
    \item Do the projections here serve as approximations? (linalg skript 2024 5.7.4 Projektionen als beste Annäherung) \textit{Yes, they can be interpreted as sort of approximation.}
    \item Which sub-space are we exactly projecting onto? Are the projections orthogonal by construction (orthogonal projections) or only when the zero-mean constraint is set? \textit{The subspace we project onto depends on the component. For $f_0$ we project onto the subspace of constant functions, for $f_1$ we project onto the subspace of all functions that involve $x_1$ and have an expected value of 0 (zero-mean constraint to ensure orthogonality). It depends on the formulation of the fANOVA decomposition if you need to explicitly set the zero-mean constraint for orthogonality or if it is met by construction.}
    \item How \ldq far\rdq should I go back, formally introduce $L^2$ space, etc. or assume that the reader is familiar with it? \textit{Yes, space, the inner product on this space should be formally introduced.}
    % \item zero mean condition vs. zero-sum condition: according to GPT zero mean condition is related to orthogonality and zero-sum to the additivity
\end{itemize}

% \section*{fANOVA Brainstorming Questions}

% \subsection*{What happens if...?}
% \begin{itemize}
%     \item What happens if the function \( f \) is linear in all its variables? What do the fANOVA terms look like in that case?
%     \item What happens if some variables are independent? Do any Sobol’ terms vanish automatically?
%     \item What happens if you permute the inputs? Do the Sobol’ indices change?
%     \item What happens if two variables are strongly collinear? How does that affect the interpretability of fANOVA?
% \end{itemize}

% \subsection*{Why is...?}
% \begin{itemize}
%     \item Why is orthogonality (zero mean and mutual independence of components) important in fANOVA?
%     \item Why is the constant term \( f_0 \) equal to the expected value of \( f \)?
%     \item Why is fANOVA usually associated with variance-based methods like Sobol indices?
%     \item Why is it necessary to subtract lower-order terms when computing higher-order ones?
%     \item Why is the decomposition hierarchical?
% \end{itemize}

% \subsection*{Is it possible to...?}
% \begin{itemize}
%     \item Is it possible to compute fANOVA in closed form for certain functions (e.g., polynomials)?
%     \item Is it possible to use fANOVA in models where inputs are dependent?
%     \item Is it possible to extend fANOVA to time-series models or dynamic systems?
%     \item Is it possible to use fANOVA ideas in neural networks? How would you interpret interactions then?
% \end{itemize}

% \subsection*{What does this remind you of? (Analogies and Parallels)}
% \begin{itemize}
%     \item fANOVA reminds me of \emph{Fourier decomposition}: projecting a function onto orthogonal basis functions.
%     \item It feels similar to \emph{PCA}, but in the input space instead of the output space.
%     \item fANOVA terms are like \emph{partial derivatives} in symbolic differentiation — quantifying localized influence.
%     \item It’s analogous to \emph{ANOVA in statistics}, but instead of experimental groups, you decompose function behavior.
%     \item In machine learning, it reminds me of \emph{feature importance} in random forests or \emph{Shapley values}.
%     \item fANOVA’s additive structure is similar to \emph{GAMs (Generalized Additive Models)}.
% \end{itemize}

% \subsection*{How does fANOVA compare to...?}
% \begin{itemize}
%     \item How does fANOVA compare to Shapley values? (Shapley is axiomatic, fANOVA is Hilbert space projection-based.)
%     \item How does fANOVA compare to LIME/SHAP in interpretability?
%     \item How does fANOVA compare to gradient-based sensitivity methods?
%     \item How does fANOVA compare to partial dependence plots?
%     \item How does fANOVA compare to mutual information as a dependence measure?
% \end{itemize}
