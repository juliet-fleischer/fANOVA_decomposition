% Prerequesites:
% L2 space, Hilbert space
% inner product, norm
% orthogonal projections
% expected value, conditional expected value
\subsection*{Basic Setup}
The formal setup is based on \cite{chastaing2012} and \cite{rahman2014}. Let $(\Omega, \mathcal{F}, \nu)$ be a measure space, where $\Omega$ is a sample space, $\mathcal{F}$ is a $\sigma-$algebra on $\Omega$ and $\nu: \mathcal{F} \rightarrow [0, 1]$ is a probability measure. $\mathcal{B}^N$ is the Borel $\sigma$-algebra on $\mathbb{R}^N, N \in \mathbb{N}$.
$\boldsymbol{X} = (X_1, \dots, X_N): (\Omega, \mathcal{F}) \rightarrow (\mathbb{R}^N, \mathcal{B}^N)$ denotes a $\mathbb{R}^N$-valued random vector.\par
We assume that the probability distribution of $\boldsymbol{X}$ is continuous and completely defined by the joint probability density function (pdf) $f_{\boldsymbol{X}}: \mathbb{R}^N \rightarrow \mathbb{R}_{0}^+$. $f_{\boldsymbol{X}}$ is the pdf w.r.t. measure $\nu$. \par

Let $u$ denote a subset of indices $\{1, \dots, N\}$, and $-u := \{1, \dots, N\} \backslash{} u$ its compliment. We denote strict inclusion by $\subsetneq$ and $\subset$ allows for equality.
$\boldsymbol{X_u} = (X_1, \dots, X_{|u|}), u \neq \emptyset, 1 \leq i_1 < \dots < i_{|u|} \leq N$ is a sub-vector of $\boldsymbol{X}$ and $\boldsymbol{X}_{-u} = \boldsymbol{X}_{\{1, \dots, N\} \backslash{} u}$ is the complement of $\boldsymbol{X}_u$.\par

The marginal density function is $f_u(\boldsymbol{x_u}) := \int f_{\boldsymbol{X}}(\boldsymbol{x})d\boldsymbol{x_{-u}}$ for a given set $\emptyset \neq u \subseteq \{1, \dots N\}$.
$y(\boldsymbol{X}) := y(X_1, \dots, X_N)$ is a mathematical model with random variables as inputs.
We write a vector space of square-integrable functions as
\[\mathcal{L}^2(\Omega, \mathcal{F}, \nu) = \left\{ y: \Omega \rightarrow \mathbb{R} \; \textit{s.t.} \; \mathbb{E}[y^2(\boldsymbol{X})] < \infty \right\}\].
% = \left\{ f(x) : \mathbb{R}^{n} \to \mathbb{R}, \; \textit{s.t.} \; \int f^2(x)\, d\nu(x) < \infty \right\}

$\mathcal{L}^2(\Omega, \mathcal{F}, \nu)$ is a Hilbert space with the inner product defined as:
\[
\langle y, g \rangle = \int y(\boldsymbol{x}) g(\boldsymbol{x}) \, f_{\boldsymbol{X}}d\nu(\boldsymbol{x}) = \mathbb{E}[y(\boldsymbol{X})g(\boldsymbol{X})].
\]
The norm is denoted as $\|.\| $ and defined by:
\[
\|y\| = \sqrt{\langle y, y \rangle} = \sqrt{\int y^2(x) \, d\nu(x)} = \mathbb{E}[y^2], \quad \forall y \in \mathcal{L}^2.
\]

\subsection*{Orthogonal projection}
Let $\mathcal{G} \subset \mathcal{L}^2$ denote a linear subspace. The projection of $y$ onto $\mathcal{G}$ is defined by the function $\Pi_{\mathcal{G}}y$ which minimizes the distance to $y$ in $\mathcal{L}^2$:
\[
\Pi_{\mathcal{G}}y = \arg\min_{g \in \mathcal{G}} \|y - g\|^2
= \arg\min_{g \in \mathcal{G}} \mathbb{E}[(y - g)^2].
\]

Definition of $\mathcal{L}^2$ space and projection modified from \cite{nagler2024linalg}.

% conditional expectation defined via integral conditioned on certain value/ partial integral


\subsection*{Unconditional and Conditional expectation}
$\mathbb{E}[.]$ denotes the expectation operator, $Var[.] := \mathbb{E}[(. - \mathbb{E}[.])^2]$ denotes the variance and $Cov[.,*] := \mathbb{E}[(. - \mathbb{E}[.]) (* - \mathbb{E}[*])]$ denotes the covariance operator.\par
In general, we define the conditional expectation of a vector of random variables $\boldsymbol{X} = (X_1, X_2)$ as follows:
\[
\mathbb{E}[g(X_1, X_2) \mid X_1 = x_1] = \int g(x_1, s_2) \, p_{X_2 \mid X_1}(s_2 \mid x_1) \, ds_2.
\]
Only when $X_1$ and $X_2$ are independent can we write
\begin{align*}
    \mathbb{E}[g(X_1, X_2) \mid X_1 = x_1] = \int g(x_1, s_2) \, p_{X_2 \mid X_1}(s_2 \mid x_1) \, ds_2 = \int g(x_1, s_2) \, p_{X_2}(s_2) \, ds_2 = \mathbb{E}_{X_2}[g(x_1, X_2)].
\end{align*}

Extended to $n$ random variables it looks as follows. Without loss of generality, we condition on $X_1 = x_1$:
\begin{align*}
    \mathbb{E}[g(X_1, \dots, X_n) \mid X_1 = x_1] &= \int g(x_1, s_2, \dots, s_n) \, p_{X_2, \dots, X_n \mid X_1}(s_2, \dots, s_n \mid x_1) \, ds_2 \dots ds_n \\
    &= \int g(x_1, s_2, \dots, s_n) \, p_{X_2}(s_2, \dots, s_n) \, ds_2 \dots, ds_n \\
    &= \mathbb{E}_{X_2, \dots, X_n}[g(x_1, X_2, \dots, X_n)]
\end{align*}



\subsection*{Properties of the Multivariate Normal Distribution}

Let $\boldsymbol{X} = (X_1, \dots, X_d)^\top \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ be a $d$-dimensional multivariate normal (MVN) random vector, where $\boldsymbol{\mu} \in \mathbb{R}^d$ is the mean vector and $\boldsymbol{\Sigma} \in \mathbb{R}^{d \times d}$ is the symmetric positive semi-definite covariance matrix.\par

The marginal distribution of $X_i$ is generally given by an univariate normal distribution:
\[
 X_i \sim \mathcal{N}(\mu_i, \Sigma_{ii}) \quad \text{for all } i = 1, \dots, d.
\]
If we condition on a subset of the variables, we can also make statements about the conditional distribution. For this we partition the random vector $\boldsymbol{X}$ into two parts, $\boldsymbol{X}_A$ and $\boldsymbol{X}_B$, where $\boldsymbol{X}_A$ contains the variables we condition on and $\boldsymbol{X}_B$ contains the remaining variables. The joint distribution of $\boldsymbol{X}$ can be expressed as:
    \[
    \boldsymbol{X} = 
    \begin{pmatrix}
    \boldsymbol{X}_A \\
    \boldsymbol{X}_B
    \end{pmatrix}
    \sim \mathcal{N}\left(
    \begin{pmatrix}
    \boldsymbol{\mu}_A \\
    \boldsymbol{\mu}_B
    \end{pmatrix},
    \begin{pmatrix}
    \boldsymbol{\Sigma}_{AA} & \boldsymbol{\Sigma}_{AB} \\
    \boldsymbol{\Sigma}_{BA} & \boldsymbol{\Sigma}_{BB}
    \end{pmatrix}
    \right).
    \]
The conditional distribution of $\boldsymbol{X}_B$ given $\boldsymbol{X}_A = \boldsymbol{x}_A$ is
    \[
    \boldsymbol{X}_B \mid \boldsymbol{X}_A = \boldsymbol{x}_A \sim 
    \mathcal{N} \left(
    \boldsymbol{\mu}_B + \boldsymbol{\Sigma}_{BA} \boldsymbol{\Sigma}_{AA}^{-1} (\boldsymbol{x}_A - \boldsymbol{\mu}_A),
    \boldsymbol{\Sigma}_{BB} - \boldsymbol{\Sigma}_{BA} \boldsymbol{\Sigma}_{AA}^{-1} \boldsymbol{\Sigma}_{AB}
    \right).
    \]
For normally distributed random variables, we also know that \(\text{Cov}(X_i, X_j) = 0 \text{, implies } X_i \perp X_j\).
% Lastly, for any real vector $\boldsymbol{a} \in \mathbb{R}^d$, the linear combination $\boldsymbol{a}^\top \boldsymbol{X}$ is normally distributed:
% \[
% \boldsymbol{a}^\top \boldsymbol{X} \sim \mathcal{N}(\boldsymbol{a}^\top \boldsymbol{\mu}, \boldsymbol{a}^\top \boldsymbol{\Sigma} \boldsymbol{a}).
% \]