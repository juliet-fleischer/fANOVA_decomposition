% Good historical overview of fANOVA decomposition found in: Owen (2003), Takemura (1983)

\subsection{Early Work on fANOVA}
\subsubsection*{Hoeffding decomposition 1948}
\begin{itemize}
    \item The idea of fANOVA decomposition dates back to \cite{hoeffding1948}.
    \item Introduces Hoeffding decomposition (or U-statistics ANOVA decomposition).
    \item Math-workings: involves orthogonal sums, projection functions, orthogonal kernels, and subtracting lower-order contributions.
    \item Assupmtions: unclear about all but one assumptions is (mututal?) independence of input variables, which is unrealistic in practice (different generalizations to dependent variables follow, e.g. \cite{ilidrissi2025})
    \item Relevance: shows that U-statistics or any symmetric function of the data can be broken down into simpler pieces (e.g., main effects, two-way interactions) without overlap.
    \item Pieces can be used to dissect/explain the variance.
    \item fANOVA performs a similar decomposition, not for U-statistics but for functions.
\end{itemize}
$\Rightarrow$\textbf{fANOVA and U-statistics}

\subsubsection*{Sobol Indices 1993, 2001}
\begin{itemize}
    \item In "Sensitivity Estimates for Nonlinear Mathematical Models" (1993), Sobol first introduces decomposition into summands of different dimensions of a (square) integrable function.
    \item Does not cite Hoeffding nor discuss U-statistics.
    \item "Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates" (2001) builds on his prior work \citep{sobol2001}.
    \item Math-workings: similar to Hoeffding, involving orthogonal projections, sums, and independent terms.
    \item Sobol focuses on sensitivity analysis for deterministic models, while Hoeffding is concerned with estimates of probabilistic models.
\end{itemize}

I think in his 1993 paper Sobol mainly introduces fANOVA decomposition (definition, orthogonality, L1 integrability), already speaks of L2 integrability and variance decompsoition, which leads to Sobol indices, gives some analytical examples and MC algorithm for calculations.
In the 2001 paper he focuses on illustrating three usecases of the sobol indices + the decomposition\par
\begin{itemize}
    \item ranking of variables
    \item fixing unessential variables
    \item deleting high order members
\end{itemize}
For each of the three there are some mathematical statements, sometimes an algorithm or an example.
$\Rightarrow$\\textbf{fANOVA and sensitivity analysis}

\subsubsection*{Efron and Stein (1981)}
\begin{itemize}
    \item Use idea to proof a famous lemma on jackknife variances \citep{efron1981}
\end{itemize}

\subsubsection*{Stone 1994}
\begin{itemize}
    \item \cite{stone1994}
    \item Math-workings: sum of main terms, lower-order terms, etc., with an identifiability constraint (zero-sum constraint); follows the same principle as the decomposition frameworks by \cite{hoeffding1948} and \cite{sobol2001}.
    \item All of them work independently, do not cite each other, and use the principle with different goals/build different tools on it.
    \item Stone's work is part of a broader body of fANOVA models.
\end{itemize}
$\Rightarrow$\textbf{fANOVA and smooth regression models / GAMs}\par
I think the main focus of this paper is to extend the theoretical framework of GAMs with interactions. So the baseline is logistic regression with smooth terms but only univariate components are considered.
Now the paper goes deeper into the theory where multivariate terms are also considered. For this they refer to the \ldq ANOVA decomposition\rdq of a function. The focus of the paper is on how the smooth multivariate interaction terms can be estimated, what mathematical properties they have, etc.


\subsection{Modern Work on fANOVA}
% Rabitz and Alis¸ (1999), Peccati (2004), Hooker (2007), Kuo et al. (2009), Hart and Gremaud (2018), and Chastaing, Gamboa, and
% Prieur (2012), Il Idrissi (2025)
\begin{itemize}
    \item Rabitz and Alis¸ (1999) see ANOVA decomposition as a specific high dimensional model representation (HDMR); the goal is to decompose the model iteratively from main effects, to lower order interactions and so on, but to do this in an efficient way and seletect only interaction terms that are necessary (most often lower-order interactions are sufficient). $\rightarrow$ chemistry paper
    \item Work of \cite{hooker2007} can be seen as an attempt to generalize Hoeffding decomposition (or the Hoeffding principle) to dependent variables. According to \href{https://static1.squarespace.com/static/5f704d21e5464d602d153738/t/66ec27cadf4e8d42ed9018d0/1726752718798/20240918_SADiscord_MIL.pdf}{Slides to talk on Shapley and Sobol indices}
    \item At least in his talk which is based on the paper \cite{ilidrissi2025} he puts his work in a broader context of modern attempts to generalize Hoeffding indices. So \cite{ilidrissi2025} can be seen as one attempt to generalize Hoeffding decomposition to dependent variables.
\end{itemize}


% \subsection*{Formal Setting of fANOVA}
% % Assumptions and prerequisites
% Let $f(x): I[0,1]^n \rightarrow I[0,1] \quad with \quad x = (x_1, ... x_n) $ be a function from the unit hypercube to the unit interval. $f(x)$ represents a mathematical model.\par
% \textbf{Definition:} $f(x)$ can be represented as a sum of main effects and interaction effects
% \begin{equation}
%     f = f_0 + \sum_{s=1}^{n} \sum_{i_1 <...<i_s}^{n} f_{i_{1}...i_{s}} (x_{i_{1}} , ....,x_{i_{s}})
%     \label{eq:fanova_decomposition}
% \end{equation}
% with $1 \leq i_1 < .... < i_s \leq n$.
% \autoref{eq:fanova_decomposition} is the "ANOVA-representation" \cite{sobol2001} or "functional ANOVA decomposition" \cite{hooker2004} if $f_0$ is constant and the integrals of the summands $f_{i_{1}...i_{s}}$ with respect to any of their included variables are zero.
% \begin{equation}
%     \int_{0}^{1} f_{i_{1}...i_{s}} (x_{i_{1}}, ...., x_{i_{s}}) \, dx_k = 0 for k = i_1, ...., i_s
%     \label{eq:zero_mean_condition}
% \end{equation}
% In words so far: a function is decomposed into constant term $f_0$ and a sum of main effects and interaction effects. If each term "is centred" (i.e. has zero mean) with respect to the variables it includes, then the terms are orthogonal to each other. In an applied context orthogonality means that the terms capture the isolated effect and there is no redundancy in information, i.e. no information of $x_1$ is also included in the interaction of $x_{12}$.\par
% \textbf{Example for $n=3$}:
% \begin{equation}
%     f(x_1,x_2,x_3) = f_0 + f_{1}(x_1) + f_{2}(x_2) + f_{3}(x_3) + f_{12}(x_1,x_2) + f_{13}(x_1,x_3) + f_{23}(x_2,x_3)
%     \label{eq:fanova_decomposition_example}
% \end{equation}

% Given \autoref{eq:zero_mean_condition} the following holds for the components in \autoref{eq:fanova_decomposition}:
% \begin{equation}
%     \int f(x) dx = f_0
% \end{equation}
% This means that the integral over the entire domain and all inputs gives us the constant term/ intercept == overall average
% \begin{equation}
%     \int f(x) \prod_{k \neq i} d_{x_{k}} = f_0 + f_i(x_i)
% \end{equation}
% The integral over all variables expect $x_i$ is equal to adding up the overall mean and the main effect of $x_i$.
% \begin{equation}
%     \int f(x) \prod_{k \neq i,j} d_{x_{k}} = f_0 + f_i(x_i) + f_j(x_j)
% \end{equation}
% The integral over all variables expect $x_i$ and $x_j$ is equal to adding up the overall mean, the main effect of $x_i$, and the main effect of $x_j$, and the interaction effect of $x_i, x_j$.\par
% Inhaltlich: when we are interested in "the average effect of ..." we may add the main effect and their interaction effects together.??


\subsection{Formal Introduction to fANOVA}
% Prerequesites: L2 space, inner product, orthogonal projections, integrals
\subsubsection*{Prerequisites}
Let $(X, \mathcal{F}, \nu)$ be a measure space. Then the space of all square-integrable functions is given by
\[
\mathcal{L}^2(X, \mathcal{F}, \nu) = \left\{ f(x) : \mathbb{E}[f^2(x)] < \infty \right\}
= \left\{ f(x) : \mathbb{R}^{n} \to \mathbb{R}, \; \textit{s.t.} \; \int f^2(x)\, d\nu(x) < \infty \right\}
\]

$\mathcal{L}^2$ is a Hilbert space with the inner product defined as
\[
\langle f, g \rangle = \int f(x) g(x) \, d\nu(x) \quad \forall f, g \in \mathcal{L}^2
\]
The norm is then defined as
\[
\|f\| = \sqrt{\langle f, f \rangle} = \sqrt{\int f^2(x) \, d\nu(x)} \quad \forall f \in \mathcal{L}^2
\]

{\color{blue} Which resource should I cite for these ``general'' definitions? e.g. \href{https://apachepersonal.miun.se/~andrli/Bok.pdf}{https://apachepersonal.miun.se/~andrli/Bok.pdf}?}

\subsubsection*{fANOVA decomposition}
This chapter is based on the formal introductions by \cite{sobol1993sensitivity, sobol2001, hooker2004}, Owen.
For now, we work with the measure space $(X, \mathcal{F}, \nu) = ([0, 1]^n, \mathcal{B}([0, 1])^n, \lambda_{n})$. $\mathcal{B}([0, 1])^n$ is the Borel $\sigma$-Algebra on the unit interval (wrong formulation, how is the correct definition?), $\lambda_{n}$ is the n-dimensional Lebesgue measure. 
This means we look at functions $f: [0,1]^n \rightarrow \mathbb{R}$  that represent mathematical models and for which $\int f^2(x) < \infty$ holds.
Further we assume that $X_i \overset{\text{i.i.d.}}{\sim} \mathcal{U}[0, 1]$ (we will generalize this later).\par

\textbf{Definition.} We can represent such a model $f$ as a sum of specific basis functions
\begin{equation}
    f(x) = f_0 + \sum_{s=1}^{n} \sum_{i_1 <...<i_s}^{n} f_{i_{1}...i_{s}} (x_{i_{1}} , ....,x_{i_{s}})
    \label{eq:fanova_decomposition}
\end{equation}

To ensure identifiability and interpretation, we set the zero-mean constraint. It requires that all effects, except for the constant terms, are centred around zero. Since that the constant term $f_0$ captures the overall mean of $f$, the remaining effects quantify the deviation from the overall mean. Mathematically this means
\begin{equation}
    \int_{0}^{1} f_{i_{1}...i_{s}} (x_{i_{1}}, ...., x_{i_{s}}) \, d\nu (x_k) = 0 \quad \forall k = i_1, ...., i_s
    \label{eq:zero_mean_condition}
\end{equation}
In combination with \autoref{eq:zero_mean_condition}  \cite{sobol1993sensitivity} calls \autoref{eq:fanova_decomposition} initially the \ldq Expansion into Summands of Different Dimensions\rdq. In \cite{sobol2001} he renames the decomposition to the \ldq ANOVA-representation\rdq. Now, it is mostly referred to as the \ldq functional ANOVA decomposition\rdq \citep{hooker2004}.\par

Before moving to properties of the fANOVA decomposition, let us introduce a simple function $g$ as running example. It contains a constant term $a$, isolated linear effects of two variables $x_1$ and $x_2$ and their interaction.
\[
g(x_1, x_2) = a + x_1 + 2*x_2 + x_1 * x_2 \quad \text{for} \quad a, x_1, x_2 \in \mathbb{R}
\]
The fANOVA decomposition for a \( f(x_1, x_2, x_3, x_4) \) would look like
\begin{align*}
    f(x_1, x_2, x_3, x_4) &= f_0 + f_1(x_1) + f_2(x_2) + f_3(x_3) + f_4(x_4) \\
    &+ f_{1,2}(x_1, x_2) + f_{1,3}(x_1, x_3) + f_{1,4}(x_1, x_4) + f_{2,3}(x_2, x_3) + f_{2,4}(x_2, x_4) + f_{3,4}(x_3, x_4) \\
    &+ f_{1,2,3}(x_1, x_2, x_3) + f_{1,2,4}(x_1, x_2, x_4) + f_{1,3,4}(x_1, x_3, x_4) + f_{2,3,4}(x_2, x_3, x_4) \\
    &+ f_{1,2,3,4}(x_1, x_2, x_3, x_4)
\end{align*}

    
The single terms that make up \autoref{eq:fanova_decomposition} are defined as follows.
First, we take the integral of $f$ w.r.t. all variables:
\begin{equation}
    f_{0}(\mathbf{x}) = \int_{[0, 1]^n} f(\mathbf{x}) d\nu(\mathbf{x}) = \mathbb{E}[f(x)]
    \label{eq:intercapt}
\end{equation}
Next, we take the integral of $f$ w.r.t. all variables except for $x_i$. This represents $f$ as the sum of the constant term the isolated effect of one variable $x_i$ (main effect of $x_i$).
\begin{equation}
    f_0 + f_i(x_i) = \int f(x) \prod_{k \neq i} \nu (d_{x_{k}}) = g_i(x_i)
    \label{eq:main_effect}
\end{equation}
Following the same principle, we can take the integral of $f$ w.r.t. all variables except for $x_i$ and $x_j$. With this we capture everything up to the interaction effect of $x_i$ and $x_j$:
\begin{equation}
    f_0 + f_i(x_i) + f_j(x_j) + f_{ij}(x_i,x_j) = \int f(x) \prod_{k \neq i,j} \nu(d_{x_{k}}) = g_{ij}(x_i, x_j)
    \label{eq:interaction_effects}
\end{equation}
And so on, up to the n-way interaction of all $x_1, \dots, x_n$.\par

To actually compute the fANOVA decomposition for $f$, it is clearer to rearrange terms. When we rearrange \autoref{eq:main_effect} we get that the main effect of $x_i$ is calculated by taking the marginal effect while explicitly accounting for what was already explained by lower terms, in this case the intercept. 
\begin{equation}
    f_i(x_i) = \int f(x) \prod_{k \neq i} \nu(d_{x_{k}}) - f_0
    \label{eq:main_effect_rearranged}
\end{equation}
The two-way interactions can then be seen as the marginal effects of the involved variables, while accounting for all main effects and the constant term. 
\begin{equation}
    f_{ij}(x_i,x_j) = \int f(x) \prod_{k \neq i,j} \nu(d_{x_{k}}) - f_0 - f_i(x_i) - f_j(x_j) 
    \label{eq:interaction_effects}
\end{equation}

Therefore, it is also common to formulate the fANOVA decomposition in the following way \citep{hooker2007,hooker2004}:
\begin{equation}
    f_u(\mathbf{x}) = \int_{[0,1]^{d - |u|}} \left( f(\mathbf{x}) - \sum_{v \subsetneq u} f_v(\mathbf{x}) \right) \, d\nu(\mathbf{x}_{-u}).
\end{equation}
Which simplifies to:
\begin{equation}
    f_u(\mathbf{x}) = \int_{[0,1]^{d - |u|}} f(\mathbf{x}) \, d\nu(\mathbf{x}_{-u}) - \sum_{v \subsetneq u} f_v(\mathbf{x}).
\end{equation}

The basis components offer a clear interpretation of the model, decomposing it into main effects, two-way interaction effects, and so on. This is why fANOVA decomposition has received increasing attention in the IML and XAI literature, holding the potential for a global explanation method of black box models.\par

A technical remark: The fANOVA terms can be understood as projections.\par

$f_0$ is the projections of $f$ onto the space of all constant functions $G = \{g(x) = a; a \in \mathbb{R}\}$. It is an unconditional expected value and the best approximation of $f$ given my a constant function.\par

The main effect $f_i(x_i)$ is the projection of $f$ onto the subspace of all functions that only depend on $x_i$ and have an expected value of zero, $G = \{g(x) = g_i(x_i); \int g(x) d\nu (x_i) = 0\}$. It is a mean conditioned on $x_i$ and the best approximation of $f$ given by a function that depends on a single variable $x_i$.\par

The two-way interaction effect $f_{ij}(x_i,x_j)$ is the projection of $f$ onto the subspace of all functions that depend on $x_i$ and $x_j$ and have an expected value of zero in each of it's single components, $G = \{g(x) = g_{ij}(x_i, x_j); \int g(x) d\nu (x_i) = 0 \land \int g(x) d\nu (x_j) = 0\}$. It is the expected value conditioned on $x_i, x_j$ and the best approximation of $f$ given by a function that depends on only two variables.\par
In general, \ldq each term is calculated as the projection of $f$ onto a particular subset of the predictors, taking out the lower-order effects which have already been accounted for.\rdq \cite{hooker2004}.\par

\subsubsection*{Orthogonality of the fANOVA terms}
Orthogonality of the fANOVA terms follows using the zero-mean constraint (\autoref{eq:zero_mean_condition}). If two sets of indices are not completely equivalent $(i_1, \dots, i_s) \neq (j_1, \dots, j_l)$ then
\begin{equation}
    \int f_{i_{1}, \dots, i_{s}} f_{j_{1}, \dots, j_{l}} d\nu(x) = 0
    \label{eq:orthogonality}
\end{equation}
This means that fANOVA terms are ``fully orthogonal'' to each other.\par
Consider an example for $(i_1, i_2) = (1, 2)$ and $(j_1, j_2) = (1, 3)$. We take the inner product between these fANOVA components
\[
\int_0^1 \int_0^1 \int_0^1 f_{1,2}(x_1, x_2) \cdot f_{1,3}(x_1, x_3) \, dx_1 \, dx_2 \, dx_3
\]

We begin by integrating with respect to \( x_1 \) and define
\[
\int_0^1 f_{1,2}(x_1, x_2) \cdot f_{1,3}(x_1, x_3) \, dx_1 := h(x_2, x_3)
\]

Then the full integral becomes
\[
\int_0^1 \int_0^1 \left( \int_0^1 f_{1,2}(x_1, x_2) \cdot f_{1,3}(x_1, x_3) \, dx_1 \right) dx_2 dx_3 = \int_0^1 \int_0^1 h(x_2, x_3) \, dx_2 \, dx_3
\]

But can I now simply say that the integral wrt to $x_2$ is zero because of the zero-mean constraint \autoref{eq:zero_mean_condition}?
\[
\int_0^1 h(x_2, x_3) \, dx_2 = 0 \quad \text{for all } x_3
\]


\subsubsection*{Variance decomposition}
If $f \in \mathcal{L}^2$, then $f_{i_{1}, \dots, i_{n}} \in \mathcal{L}^2$ {\color{blue}proof? reference?; Sobol 1993 says it is easy to show using Schwarz inequality and the definition of the single fANOVA terms.}
Therefore, we define the variance of $f$ as follows:
\begin{align*}
    D = \int_{K^n} f^2(x)d\nu (x) - f^2_{0} &= \int_{K^n} f^2(x)d\nu (x) - (\int_{K^n} f(x)d\nu (x))^2 &= \mathbb{E}[f^2(x)] - \mathbb{E}[f(x)]^2
    \label{variance_whole}
\end{align*}
The variance of the fANOVA components is then defined as
\begin{align*}
    D_{i_{1}, \dots, i_{n}} 
    &= \int \cdots \int f^2_{i_{1}, \dots, i_{n}} \, d\nu(x_1) \cdots d\nu(x_n) - \left( \int \cdots \int f_{i_{1}, \dots, i_{n}} \, d\nu(x_1) \cdots d\nu(x_n) \right)^2
\end{align*}
Because of the zero-mean constraint (\autoref{eq:zero_mean_condition}) the second term vanishes and we get
\begin{align*}
    D_{i_{1}, \dots, i_{n}} = \int \cdots \int f^2_{i_{1}, \dots, i_{n}} \, d\nu(x_1) \cdots d\nu(x_n)
\end{align*}

With the definition of the total variance $D$ and the component-wise variance $D_{i_{1}, \dots, i_{n}}$ we can now see that the total variance can be decomposed into the sum of the component-wise variances.

We illustrate this for a fANOVA decomposition function \( f(x_1, x_2) \in L^2 \):

\[
f(x_1, x_2) = f_0 + f_1(x_1) + f_2(x_2) + f_{1,2}(x_1, x_2)
\]

\vspace{1em}

First, we square the decomposition

\begin{align*}
f^2(x_1, x_2) &= \left(f_0 + f_1(x_1) + f_2(x_2) + f_{1,2}(x_1, x_2)\right)^2 \\
&= f_0^2 + f_1(x_1)^2 + f_2(x_2)^2 + f_{1,2}(x_1, x_2)^2 \\
&\quad + 2f_0 f_1(x_1) + 2f_0 f_2(x_2) + 2f_0 f_{1,2}(x_1, x_2) \\
&\quad + 2f_1(x_1) f_2(x_2) + 2f_1(x_1) f_{1,2}(x_1, x_2) + 2f_2(x_2) f_{1,2}(x_1, x_2)
\end{align*}

\vspace{1em}

Next, we integrate over the domain \( [0,1]^2 \)

\begin{align*}
\int f^2(x_1, x_2) \, dx_1 dx_2 &= \int f_0^2 \, dx_1 dx_2 + \int f_1(x_1)^2 \, dx_1 dx_2 + \int f_2(x_2)^2 \, dx_1 dx_2 \\
&\quad + \int f_{1,2}(x_1, x_2)^2 \, dx_1 dx_2 \\
&\quad + \text{(all cross-terms vanish due to orthogonality)} \\
&= f_0^2 + \int f_1(x_1)^2 \, dx_1 + \int f_2(x_2)^2 \, dx_2 + \int f_{1,2}(x_1, x_2)^2 \, dx_1 dx_2
\end{align*}

\vspace{1em}

After rearranging terms, we find that

\[
\int f(x_1, x_2)^2 \, dx_1 dx_2 - f_0^2 = \int f_1(x_1)^2 \, dx_1 + \int f_2(x_2)^2 \, dx_2 + \int f_{1,2}(x_1, x_2)^2 \, dx_1 dx_2
\]

which is equivalent to 

\[
D = D_1 + D_2 + D_{1,2}
\]

The variance decomposition only holds when the $x_i$ are independent and therefore all fANOVA terms are orthogonal to each other (Fumagalli, 2025).

% where

% \[
% D_1 = \int f_1(x_1)^2 \, dx_1, \quad
% D_2 = \int f_2(x_2)^2 \, dx_2, \quad
% D_{1,2} = \int f_{1,2}(x_1, x_2)^2 \, dx_1 dx_2
% \]

% \subsection*{Alternative Formulation of fANOVA}
% Based on \cite{hooker2004} and Owen Lecture notes (find paper to cite).
% We again work with a square integrable function $f(x): [0, 1]^n \rightarrow \mathbb{R}$ with $x = (x_1, \dots, x_n)$ and $x_1, \dots, x_n$ are independent.
% The set of indices $1, \dots, n$ is denoted as $1\colon d$. $u \subset 1\colon d$ and $-u$ is the complement of $u$, i.e. $1\colon d \backslash u$.
% Given a set of indices $u = {i_1, i_2, \dots, i_{|u|}}$ we can write $x_u$ for $(x_{i_1}, x_{i_2}, \dots, x_{i_{|u|}}) = (x_i)_{i \in u}$.\par

% This notation allows us to sum over a set of indices, which avoids lengthy notation. The fANOVA can then be formulated as follows:
% \begin{equation}
%     f_{\emptyset}(\mathbf{x}) = \int_{[0, 1]^n} f(\mathbf{x}) d\nu(\mathbf{x}) = \mu
% \end{equation}

% Generally, the fANOVA decomposition can be written as
% \begin{equation}
%     f_u(\mathbf{x}) = \int_{[0,1]^{d - |u|}} \left( f(\mathbf{x}) - \sum_{v \subsetneq u} f_v(\mathbf{x}) \right) \, d\nu(\mathbf{x}_{-u}).
% \end{equation}
% We can rewrite this as follows, which simplifies the calculation of the integral:
% \begin{equation}
%     f_u(\mathbf{x}) = \int_{[0,1]^{d - |u|}} f(\mathbf{x}) \, d\nu(\mathbf{x}_{-u}) - \sum_{v \subsetneq u} f_v(\mathbf{x}).
% \end{equation}

% In Lemma A.3. Owen states a general formulation of orthogonality of two functions, which is applicable for the fANOVA components and ensures their orthogonality.
% Given to square-integrable, real-valued functions on the unit hypercube $f(x)$ and $g(x)$ with $u,v \subseteq {1, \dots, d}$, it is true that, if $u \neq v$, then
% \[
% \int_{[0,1]^n} f_u(\mathbf{x}) g_v(\mathbf{x}) \, d\nu(\mathbf{x}) = 0
% \]


\subsubsection*{Example fANOVA decomposition}
Example for a specific function $f$?


% \subsection*{An attempt to explain the motivation behind fANOVA}
% Disclaimer: Intuitive explanation.\par
% To motivate the fANOVA decomposition, we tell the story "backwards"\footnote{In the opposite way, it is usually introduced formally.}.
% We start with a statistical model $f(x)$ that takes in multiple covariables $x_1,\dots, x_n$ that are potentially interacting with each other in a complex way and having a complex effect on the target variable. We therefore think, it would be very nice to disentangle the effects of the covariables and clearly tell which influence comes from a single variable, which comes from interactions and so on.
% Mathematically this would correspond to writing $f(x)$ as a sum of isolated terms and all n-way interaction terms, i.e. \autoref{eq:fanova_decomposition}.\par
% The challenge becomes to find this specific representation of $f(x)$. In this sense we are facing an approximation problem, which we solve by using projections (we need the best approximation of $f(x)$ in a certain subspace, so the orthogonal projection onto the subspace is the best solution). A projection is defined via the inner product, which is defined via integrals in case of functions. The projections we use to define the components of the decomposition therefore look like this:
% \begin{equation}
%     \int f(x) dx = f_0
% \end{equation}
% This means that the integral over the entire domain and all inputs gives us the constant term/ intercept == overall average
% \begin{equation}
%     \int f(x) \prod_{k \neq i} d_{x_{k}} = f_0 + f_i(x_i)
% \end{equation}
% The integral over all variables expect $x_i$ is equal to adding up the overall mean and the main effect of $x_i$.
% \begin{equation}
%     \int f(x) \prod_{k \neq i,j} d_{x_{k}} = f_0 + f_i(x_i) + f_j(x_j)
% \end{equation}
% With the tool of projections we solved the approximation problem. Next we face the problem of ensuring this approximation exists. We know that a representation with orthogonal projections always exists for function in $L^2$. Thus, we choose to restrict the functions we look at to be in $L^2$. This brings us to define the fANOVA decomposition as follows.\par



\subsection*{Questions}


\begin{itemize}
    \item 
    \item In the hierarchical orthogonality condition (4.2) formulated in \cite{hooker2007} for the gerneralized fAVNOA framework, shouldn't we explicitly exclude the case that $v = u$, because then, we would require that the inner product of the fANOVA component is zero wouldn't we?
    \item Why is it a problem, when explainability methods also place large emphasis on regions of low probability mass when dependencies between variables exist - because in the end explainability is about explaining the model, not the data generating process; and after all it is how the model works in these regions (it might inhibit artefacts and weird behaviour because there was to little data for good model fit in the regions but still that's how the specific model works) 
    \item \cite{hooker2007} what should the conditional expectation (dotted line) in Figure 2 show? How should it look like for the true data generating process?
    \item If the general fANOVA formulation is a true generalization then I should be able to take the general form and construct from it the simple form when I use certain constrains (such as independence, uniform distribution, etc.) right?
    \item 
    \item Use of AI tools?
    \item Do we need to restrict ourselves to the unit hypercube? Or does fANOVA decomposition work in general, but maybe with some constraints? Originally it was constructed for models on the unit hypercube $[0,1]$, but other papers also use models from $R^d$ \textit{Generally no restriction, so next step could be to generalize, to $\mathbb{R}^n$, other measures, dependent variables}
    \item Still unclear: Are the terms fully orthogonal or hierarchically? See subsection on Orthogonality of the fANOVA terms (especially the example) I think in the original fANOVA decomposition the terms are orthogonal but in the generalized fANOVA \citep{hooker2007} they are hierarchically orthogonal. \textit{fully orthogonal when independence assumption, probably partially when no independence}
    \item $x_1, \dots, x_k$ are simply the standardized features, right? \textit{Yes}
    \item {\color{orange}My current understanding: we need independence of $x_1, \dots x_k$ so that fANOVA decomposition is unique (and orthogonality holds). We need zero-mean constraint for the orthogonality of the components. We need orthogonality for the variance decomposition.}\textit{zero-mean $\rightarrow$ orthogonality $\rightarrow$ uniqueness; Lemma 1 im Hooker 2007 ist verallgemeinerg ds zero-mean constraint}
    \item Next step might be to investigate the (mathematical) parallels of fANOVA decomposition and other IML methods (PDP, ALE, SHAP), e.g. there is definitely a strong relationship between Partial dependence (PD) and fANOVA terms, and PD is itself again related to other IML methods; Also look how are other IML models studied and study fANOVA in a similar way (e.g. other IML methods are defined, checked for certain properties, examined under different conditions (dependent features, independent features) etc.) (see dissertation by Christoph Molnar for this); Also I would be very interested in investigating the game theory paper further \citep{fumagalli2025} but still a bit unsure if it is too complex.
    \item Why does a fANOVA decomposition of a simple GAM not lead to the ``true'' coefficients? \href{https://christophm.github.io/interpretable-ml-book/decomposition.html}{https://christophm.github.io/interpretable-ml-book/decomposition.html} talks about this a bit in the subchapter ``Statistical regression models'' \textit{It should actually lead to the GAM; at least under all the constraint like zero-mean constraint and orthogonality}
    \item 
    \item In \cite{hooker2004} they work with $F(x)$ and $f(x)$, but in \cite{sobol2001} they only work with $f(x)$. I think this is only notation? \textit{Only notation.}
    \item Does orthogonality in fANOVA context mean that all terms are orthogonal to each other? Or that a term is orthogonal to all lower-order terms (\ldq Hierarchical orthogonality \rdq)? \textit{The terms are hierarchically orthogonal, so each term is orthogonal to all lower-order terms, but not to the same-order terms! So $f_1$ is not necessarily orthogonal to $f_2$ but it is orthogonal to $f_{12}$, $f_{0}$.} 
    \item Do the projections here serve as approximations? (linalg skript 2024 5.7.4 Projektionen als beste Annäherung) \textit{Yes, they can be interpreted as sort of approximation.}
    \item Which sub-space are we exactly projecting onto? Are the projections orthogonal by construction (orthogonal projections) or only when the zero-mean constraint is set? \textit{The subspace we project onto depends on the component. For $f_0$ we project onto the subspace of constant functions, for $f_1$ we project onto the subspace of all functions that involve $x_1$ and have an expected value of 0 (zero-mean constraint to ensure orthogonality). It depends on the formulation of the fANOVA decomposition if you need to explicitly set the zero-mean constraint for orthogonality or if it is met by construction.}
    \item How \ldq far\rdq should I go back, formally introduce $L^2$ space, etc. or assume that the reader is familiar with it? \textit{Yes, space, the inner product on this space should be formally introduced.}
    % \item zero mean condition vs. zero-sum condition: according to GPT zero mean condition is related to orthogonality and zero-sum to the additivity
\end{itemize}


% % inner math workings of fANOVA
% \begin{itemize}
%     \item orthogonal projections
%     \item function spaces, finite-dimensional spaces
% \end{itemize}








