% Good historical overview of fANOVA decomposition found in: Owen (2003), Takemura (1983)

\subsection{Early Work on fANOVA}
\subsubsection*{Hoeffding decomposition 1948}
\begin{itemize}
    \item The idea of fANOVA decomposition dates back to \cite{hoeffding1948}.
    \item Introduces Hoeffding decomposition (or U-statistics ANOVA decomposition).
    \item Math-workings: involves orthogonal sums, projection functions, orthogonal kernels, and subtracting lower-order contributions.
    \item Assupmtions: unclear about all but one assumptions is (mututal?) independence of input variables, which is unrealistic in practice (different generalizations to dependent variables follow, e.g. \cite{ilidrissi2025})
    \item Relevance: shows that U-statistics or any symmetric function of the data can be broken down into simpler pieces (e.g., main effects, two-way interactions) without overlap.
    \item Pieces can be used to dissect/explain the variance.
    \item fANOVA performs a similar decomposition, not for U-statistics but for functions.
\end{itemize}
$\Rightarrow$\textbf{fANOVA and U-statistics}

\subsubsection*{Sobol Indices 1993, 2001}
\begin{itemize}
    \item In "Sensitivity Estimates for Nonlinear Mathematical Models" (1993), Sobol first introduces decomposition into summands of different dimensions of a (square) integrable function.
    \item Does not cite Hoeffding nor discuss U-statistics.
    \item "Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates" (2001) builds on his prior work \citep{sobol2001}.
    \item Math-workings: similar to Hoeffding, involving orthogonal projections, sums, and independent terms.
    \item Sobol focuses on sensitivity analysis for deterministic models, while Hoeffding is concerned with estimates of probabilistic models.
\end{itemize}

I think in his 1993 paper Sobol mainly introduces fANOVA decomposition (definition, orthogonality, L1 integrability), already speaks of L2 integrability and variance decompsoition, which leads to Sobol indices, gives some analytical examples and MC algorithm for calculations.
In the 2001 paper he focuses on illustrating three usecases of the sobol indices + the decomposition\par
\begin{itemize}
    \item ranking of variables
    \item fixing unessential variables
    \item deleting high order members
\end{itemize}
For each of the three there are some mathematical statements, sometimes an algorithm or an example.
$\Rightarrow$\\textbf{fANOVA and sensitivity analysis}

\subsubsection*{Efron and Stein (1981)}
\begin{itemize}
    \item Use idea to proof a famous lemma on jackknife variances \citep{efron1981}
\end{itemize}

\subsubsection*{Stone 1994}
\begin{itemize}
    \item \cite{stone1994}
    \item Math-workings: sum of main terms, lower-order terms, etc., with an identifiability constraint (zero-sum constraint); follows the same principle as the decomposition frameworks by \cite{hoeffding1948} and \cite{sobol2001}.
    \item All of them work independently, do not cite each other, and use the principle with different goals/build different tools on it.
    \item Stone's work is part of a broader body of fANOVA models.
\end{itemize}
$\Rightarrow$\textbf{fANOVA and smooth regression models / GAMs}\par
I think the main focus of this paper is to extend the theoretical framework of GAMs with interactions. So the baseline is logistic regression with smooth terms but only univariate components are considered.
Now the paper goes deeper into the theory where multivariate terms are also considered. For this they refer to the \ldq ANOVA decomposition\rdq of a function. The focus of the paper is on how the smooth multivariate interaction terms can be estimated, what mathematical properties they have, etc.


\subsection{Modern Work on fANOVA}
% Rabitz and Alis¸ (1999), Peccati (2004), Hooker (2007), Kuo et al. (2009), Hart and Gremaud (2018), and Chastaing, Gamboa, and
% Prieur (2012), Il Idrissi (2025)
\begin{itemize}
    \item Rabitz and Alis¸ (1999) see ANOVA decomposition as a specific high dimensional model representation (HDMR); the goal is to decompose the model iteratively from main effects, to lower order interactions and so on, but to do this in an efficient way and seletect only interaction terms that are necessary (most often lower-order interactions are sufficient). $\rightarrow$ chemistry paper
    \item Work of \cite{hooker2007} can be seen as an attempt to generalize Hoeffding decomposition (or the Hoeffding principle) to dependent variables. According to \href{https://static1.squarespace.com/static/5f704d21e5464d602d153738/t/66ec27cadf4e8d42ed9018d0/1726752718798/20240918_SADiscord_MIL.pdf}{Slides to talk on Shapley and Sobol indices}
    \item At least in his talk which is based on the paper \cite{ilidrissi2025} he puts his work in a broader context of modern attempts to generalize Hoeffding indices. So \cite{ilidrissi2025} can be seen as one attempt to generalize Hoeffding decomposition to dependent variables.
\end{itemize}


% \subsection*{Formal Setting of fANOVA}
% % Assumptions and prerequisites
% Let $f(x): I[0,1]^n \rightarrow I[0,1] \quad with \quad x = (x_1, ... x_n) $ be a function from the unit hypercube to the unit interval. $f(x)$ represents a mathematical model.\par
% \textbf{Definition:} $f(x)$ can be represented as a sum of main effects and interaction effects
% \begin{equation}
%     f = f_0 + \sum_{s=1}^{n} \sum_{i_1 <...<i_s}^{n} f_{i_{1}...i_{s}} (x_{i_{1}} , ....,x_{i_{s}})
%     \label{eq:fanova_decomposition}
% \end{equation}
% with $1 \leq i_1 < .... < i_s \leq n$.
% \autoref{eq:fanova_decomposition} is the "ANOVA-representation" \cite{sobol2001} or "functional ANOVA decomposition" \cite{hooker2004} if $f_0$ is constant and the integrals of the summands $f_{i_{1}...i_{s}}$ with respect to any of their included variables are zero.
% \begin{equation}
%     \int_{0}^{1} f_{i_{1}...i_{s}} (x_{i_{1}}, ...., x_{i_{s}}) \, dx_k = 0 for k = i_1, ...., i_s
%     \label{eq:zero_mean_condition}
% \end{equation}
% In words so far: a function is decomposed into constant term $f_0$ and a sum of main effects and interaction effects. If each term "is centred" (i.e. has zero mean) with respect to the variables it includes, then the terms are orthogonal to each other. In an applied context orthogonality means that the terms capture the isolated effect and there is no redundancy in information, i.e. no information of $x_1$ is also included in the interaction of $x_{12}$.\par
% \textbf{Example for $n=3$}:
% \begin{equation}
%     f(x_1,x_2,x_3) = f_0 + f_{1}(x_1) + f_{2}(x_2) + f_{3}(x_3) + f_{12}(x_1,x_2) + f_{13}(x_1,x_3) + f_{23}(x_2,x_3)
%     \label{eq:fanova_decomposition_example}
% \end{equation}

% Given \autoref{eq:zero_mean_condition} the following holds for the components in \autoref{eq:fanova_decomposition}:
% \begin{equation}
%     \int f(x) dx = f_0
% \end{equation}
% This means that the integral over the entire domain and all inputs gives us the constant term/ intercept == overall average
% \begin{equation}
%     \int f(x) \prod_{k \neq i} d_{x_{k}} = f_0 + f_i(x_i)
% \end{equation}
% The integral over all variables expect $x_i$ is equal to adding up the overall mean and the main effect of $x_i$.
% \begin{equation}
%     \int f(x) \prod_{k \neq i,j} d_{x_{k}} = f_0 + f_i(x_i) + f_j(x_j)
% \end{equation}
% The integral over all variables expect $x_i$ and $x_j$ is equal to adding up the overall mean, the main effect of $x_i$, and the main effect of $x_j$, and the interaction effect of $x_i, x_j$.\par
% Inhaltlich: when we are interested in "the average effect of ..." we may add the main effect and their interaction effects together.??


\subsection{Formal Introduction to fANOVA}

\subsubsection*{fANOVA decomposition}
This chapter is based on the formal introductions by \cite{sobol1993sensitivity, sobol2001, hooker2004}, Owen.
For now, we assume that $X_i \overset{\text{i.i.d.}}{\sim} \mathcal{U}[0, 1]$ and work in the measure space $(X, \mathcal{F}, \nu) = ([0, 1]^n, \mathcal{B}([0, 1])^n, \lambda_{n})$. $\mathcal{B}([0, 1])^n$ is the Borel $\sigma$-Algebra on the unit interval (wrong formulation, how is the correct definition?) and $\lambda_{n}$ is the n-dimensional Lebesgue measure. 
The general inner product and norm we defined earlier simplify under these assumptions.\par
The inner product under uniform distribution assumption:
\[
\langle f, g \rangle = \int f(x) g(x) \, d(x) \quad \forall f, g \in \mathcal{L}^2
\]
The norm under uniform distribution assumption:
\[
\|f\| = \sqrt{\langle f, f \rangle} = \sqrt{\int f^2(x) \, d(x)} \quad \forall f \in \mathcal{L}^2
\]
Some more notation: $i_1, \dots , i_s$ is a set of indices.\par
\textbf{Definition.} Let $y(x)$  be a mathematical model with input $X_i$ as described above. We can represent such a model $y$ as a sum of specific basis functions
\begin{equation}
    y(x) = f_0 + \sum_{s=1}^{n} \sum_{i_1 <...<i_s}^{n} f_{i_{1}...i_{s}} (x_{i_{1}} , ....,x_{i_{s}})
    \label{eq:fanova_decomposition}
\end{equation}

To ensure identifiability and interpretation, we set the zero-mean constraint. It requires that all effects, except for the constant terms, are centred around zero. Since that the constant term $f_0$ captures the overall mean of $f$, the remaining effects quantify the deviation from the overall mean. Mathematically this means
\begin{equation}
    \int_{0}^{1} f_{i_{1}...i_{s}} (x_{i_{1}}, ...., x_{i_{s}}) \, d\nu (x_k) = 0 \quad \forall k = i_1, ...., i_s
    \label{eq:zero_mean_condition}
\end{equation}
In combination with \autoref{eq:zero_mean_condition}  \cite{sobol1993sensitivity} calls \autoref{eq:fanova_decomposition} initially the \ldq Expansion into Summands of Different Dimensions\rdq. In \cite{sobol2001} he renames the decomposition to the \ldq ANOVA-representation\rdq. Now, it is mostly referred to as the \ldq functional ANOVA decomposition\rdq \citep{hooker2004}.\par

% The fANOVA decomposition for a \( f(x_1, x_2, x_3, x_4) \) would look like
% \begin{align*}
%     f(x_1, x_2, x_3, x_4) &= f_0 + f_1(x_1) + f_2(x_2) + f_3(x_3) + f_4(x_4) \\
%     &+ f_{1,2}(x_1, x_2) + f_{1,3}(x_1, x_3) + f_{1,4}(x_1, x_4) + f_{2,3}(x_2, x_3) + f_{2,4}(x_2, x_4) + f_{3,4}(x_3, x_4) \\
%     &+ f_{1,2,3}(x_1, x_2, x_3) + f_{1,2,4}(x_1, x_2, x_4) + f_{1,3,4}(x_1, x_3, x_4) + f_{2,3,4}(x_2, x_3, x_4) \\
%     &+ f_{1,2,3,4}(x_1, x_2, x_3, x_4)
% \end{align*}

The single terms that make up \autoref{eq:fanova_decomposition} are defined as follows.
To get the constant term, we take the integral of $f$ w.r.t. all variables:
\begin{equation}
    f_{0}(x) = \int_{[0, 1]^n} f(x) d\nu(x) = \mathbb{E}[y(x)]
    \label{eq:intercapt}
\end{equation}
We see that $f_0$ is the overall mean of the function and serves as a baseline. Next, we take the integral of $y$ w.r.t. all variables except for $x_i$. This represents $f$ as the sum of the constant term the isolated effect of one variable $x_i$ (main effect of $x_i$).
\begin{equation}
    f_0 + f_i(x_i) = \int f(x) \prod_{k \neq i} \nu (d_{x_{k}}) = g_i(x_i)
    \label{eq:main_effect}
\end{equation}
Following the same principle, we can take the integral of $f$ w.r.t. all variables except for $x_i$ and $x_j$. With this we capture everything up to the interaction effect of $x_i$ and $x_j$:
\begin{equation}
    f_0 + f_i(x_i) + f_j(x_j) + f_{ij}(x_i,x_j) = \int f(x) \prod_{k \neq i,j} \nu(d_{x_{k}}) = g_{ij}(x_i, x_j)
    \label{eq:interaction_effects}
\end{equation}
And so on, up to the n-way interaction of all $x_1, \dots, x_n$.\par

To actually compute the fANOVA decomposition for $f$, it is clearer to rearrange terms. When we rearrange \autoref{eq:main_effect} we get that the main effect of $x_i$ is calculated by taking the marginal effect while explicitly accounting for what was already explained by lower terms, in this case the intercept. 
\begin{equation}
    f_i(x_i) = \int f(x) \prod_{k \neq i} \nu(d_{x_{k}}) - f_0
    \label{eq:main_effect_rearranged}
\end{equation}
The two-way interactions can then be seen as the marginal effects of the involved variables, while accounting for all main effects and the constant term. 
\begin{equation}
    f_{ij}(x_i,x_j) = \int f(x) \prod_{k \neq i,j} \nu(d_{x_{k}}) - f_0 - f_i(x_i) - f_j(x_j) 
    \label{eq:interaction_effects}
\end{equation}

Therefore, it is also common to formulate the fANOVA decomposition in the following way \citep{hooker2007,hooker2004}:
\begin{equation}
    f_u(x) = \int_{[0,1]^{d - |u|}} \left( f(x) - \sum_{v \subsetneq u} f_v(x) \right) \, d\nu(x_{-u}).
\end{equation}
Which simplifies to:
\begin{equation}
    f_u(x) = \int_{[0,1]^{d - |u|}} f(x) \, d\nu(x_{-u}) - \sum_{v \subsetneq u} f_v(x).
\end{equation}

The basis components offer a clear interpretation of the model, decomposing it into main effects, two-way interaction effects, and so on. This is why fANOVA decomposition has received increasing attention in the IML and XAI literature, holding the potential for a global explanation method of black box models.\par

\subsubsection*{Example}
Before moving to properties of the fANOVA decomposition, let us introduce a simple function $g$ as running example. It contains a constant term $a$, isolated linear effects of two variables $x_1$ and $x_2$ and their interaction.
\[
g(x_1, x_2) = a + x_1 + 2x_2 + x_1x_2 \quad \text{for} \quad a, x_1, x_2 \in \mathbb{R}
\]
Computing the fANOVA decomposition of $g(x_1, x_2)$ by hand, we start with the constant term and make use of formulation via the expected value instead of the integral for notational simplicity:
\[
f_0 = \mathbb{E}[g(x_1, x_2)] = \mathbb{E}[a + x_1 + 2x_2 + x_1x_2] = \mathbb{E}[a] + \mathbb{E}[x_1] + 2\mathbb{E}[x_2] + \mathbb{E}[x_1x_2]
\]
Making use of the independence assumption of $x_1$ and $x_2$, the last term can be written as the product of the expected values. Additionally, given the zero-mean constraint, all terms, except for the constant, vanish and we obtain:
\[
f_0 = \mathbb{E}[a] + \mathbb{E}[x_1] + 2\mathbb{E}[x_2] + \mathbb{E}[x_1]\mathbb{E}[x_2] = a
\]
Under zero-mean constraint and independence, the main effects and the interaction effect can be computed as follows:
\begin{align*}
f_1(x_1) &= \mathbb{E}_{X_2}[g(x_1, X_2)] - f_0 = \mathbb{E}_{X_2}[a + x_1 + 2x_2 + x_1x_2] - a = x_1 + 2\mathbb{E}[x_2] + x_1\mathbb{E}[x_2] = x_1\\
f_2(x_2) &= \mathbb{E}_{X_1}[g(X_1, x_2)] - f_0 = \mathbb{E}_{X_1}[a + x_1 + 2x_2 + x_1x_2] - a = \mathbb{E}_{X_1}[x_1] + 2x_2 + x_2\mathbb{E}_{X_1}[x_1] = 2x_2\\
f_{12}(x_1, x_2) &= \mathbb{E}[g(x_1, x_2)] - f_0 - f_1(x_1) - f_2(x_2) = a + x_1 + 2x_2 + x_1x_2 - a - x_1 - 2x_2 = x_1x_2
\end{align*}

It comes as no surprise that in this trivial case the fANOVA decomposition does not provide any additional insights. This is because the model consists of only linear terms, constant terms, or a simple (linear?) interaction. We show this simple example nevertheless to illustrate at which step we use which assumption. Understanding this will be relevant for the generalization of the method to dependent inputs later on.\par

\subsubsection*{Orthogonality of the fANOVA terms}
Orthogonality of the fANOVA terms follows using the zero-mean constraint (\autoref{eq:zero_mean_condition}). If two sets of indices are not completely equivalent $(i_1, \dots, i_s) \neq (j_1, \dots, j_l)$ then
\begin{equation}
    \int f_{i_{1}, \dots, i_{s}} f_{j_{1}, \dots, j_{l}} d(x) = 0
    \label{eq:orthogonality}
\end{equation}
This means that fANOVA terms are ``fully orthogonal'' to each other, meaning not only terms of different order are orthogonal to each other but also terms of the same order are.
In our example from before we can test this for $i = 1$ and $j = 2$:
\begin{align*}
    \int f_{1}(x_1) f_{2}(x_2) d(x) &= \int x_1 \cdot 2x_2 \, dx_1 \, dx_2 = \mathbb{E}[x_12x_2] = \mathbb{E}[x_1] \cdot 2\mathbb{E}[x_2] = 0
\end{align*}
% To see this, consider a simple example for $(i_1, i_2) = (1, 2)$ and $(j_1, j_2) = (1, 3)$. We take the inner product between these fANOVA components:
% \[
% \int_0^1 \int_0^1 \int_0^1 f_{1,2}(x_1, x_2) \cdot f_{1,3}(x_1, x_3) \, dx_1 \, dx_2 \, dx_3
% \]

% It is allowed to switch the order of integration because of the independence assumption and \href{https://en.wikipedia.org/wiki/Fubini%27s_theorem}{Fubini theorem}, so we begin by integrating with respect to \( x_2 \). Because $x_2$ integrates to zero w.r.t any of its own terms, we immediately get that the whole integral becomes zero:
% \[
% \int_0^1 \int_0^1 \left( \int_0^1 f_{1,2}(x_1, x_2) \cdot f_{1,3}(x_1, x_3) \, dx_2 \right) dx_1 dx_3 = 0
% \]
This is of course also true for terms of different order, e.g. \( f_{1,2}(x_1, x_2) \) and \( f_{1}(x_1) \). It ensures that the effects do not overlap and each term represents the isolated contribution.

\subsubsection*{Variance decomposition}
The variance decomposition is Sobol's major use of fANOVA. He built the Sobol indices for sensitivity analysis on it. We sketch the variance decomposition here and note that it is only possible under independence.\par
If $f \in \mathcal{L}^2$, then $f_{i_{1}, \dots, i_{n}} \in \mathcal{L}^2$ {\color{blue}proof? reference?; Sobol 1993 says it is easy to show using Schwarz inequality and the definition of the single fANOVA terms.}
Therefore, we define the variance of $f$ as follows:
\begin{align*}
    D = \int_{K^n} f^2(x)d\nu (x) - f^2_{0} &= \int_{K^n} f^2(x)d\nu (x) - (\int_{K^n} f(x)d\nu (x))^2 &= \mathbb{E}[f^2(x)] - \mathbb{E}[f(x)]^2
    \label{variance_whole}
\end{align*}
The variance of the fANOVA components is then defined as
\begin{align*}
    D_{i_{1}, \dots, i_{n}} 
    &= \int \cdots \int f^2_{i_{1}, \dots, i_{n}} \, d\nu(x_1) \cdots d\nu(x_n) - \left( \int \cdots \int f_{i_{1}, \dots, i_{n}} \, d\nu(x_1) \cdots d\nu(x_n) \right)^2
\end{align*}
Because of the zero-mean constraint (\autoref{eq:zero_mean_condition}) the second term vanishes and we get
\begin{align*}
    D_{i_{1}, \dots, i_{n}} = \int \cdots \int f^2_{i_{1}, \dots, i_{n}} \, d\nu(x_1) \cdots d\nu(x_n)
\end{align*}

With the definition of the total variance $D$ and the component-wise variance $D_{i_{1}, \dots, i_{n}}$ we can now see that the total variance can be decomposed into the sum of the component-wise variances.

We illustrate this for a fANOVA decomposition function \( f(x_1, x_2) \in L^2 \):

\[
f(x_1, x_2) = f_0 + f_1(x_1) + f_2(x_2) + f_{1,2}(x_1, x_2)
\]

\vspace{1em}

First, we square the decomposition

\begin{align*}
f^2(x_1, x_2) &= \left(f_0 + f_1(x_1) + f_2(x_2) + f_{1,2}(x_1, x_2)\right)^2 \\
&= f_0^2 + f_1(x_1)^2 + f_2(x_2)^2 + f_{1,2}(x_1, x_2)^2 \\
&\quad + 2f_0 f_1(x_1) + 2f_0 f_2(x_2) + 2f_0 f_{1,2}(x_1, x_2) \\
&\quad + 2f_1(x_1) f_2(x_2) + 2f_1(x_1) f_{1,2}(x_1, x_2) + 2f_2(x_2) f_{1,2}(x_1, x_2)
\end{align*}

\vspace{1em}

Next, we integrate over the domain \( [0,1]^2 \)

\begin{align*}
\int f^2(x_1, x_2) \, dx_1 dx_2 &= \int f_0^2 \, dx_1 dx_2 + \int f_1(x_1)^2 \, dx_1 dx_2 + \int f_2(x_2)^2 \, dx_1 dx_2 \\
&\quad + \int f_{1,2}(x_1, x_2)^2 \, dx_1 dx_2 \\
&\quad + \text{(all cross-terms vanish due to orthogonality)} \\
&= f_0^2 + \int f_1(x_1)^2 \, dx_1 + \int f_2(x_2)^2 \, dx_2 + \int f_{1,2}(x_1, x_2)^2 \, dx_1 dx_2
\end{align*}

\vspace{1em}

After rearranging terms, we find that

\[
\int f(x_1, x_2)^2 \, dx_1 dx_2 - f_0^2 = \int f_1(x_1)^2 \, dx_1 + \int f_2(x_2)^2 \, dx_2 + \int f_{1,2}(x_1, x_2)^2 \, dx_1 dx_2
\]

which is equivalent to 

\[
D = D_1 + D_2 + D_{1,2}
\]

The variance decomposition only holds when the $x_i$ are independent and therefore all fANOVA terms are orthogonal to each other (Fumagalli, 2025).

\subsubsection*{fANOVA as projection}
Referring to the general connection between the expected value and orthogonal projections presented in section ~\ref{general_definitions}, the fANOVA terms can also be understood from this view, which will help to understand the generalization of fANOVA in section ~\ref{generalization}.\par

$f_0$ is the projections of the original function $f$ onto the space of all constant functions $\mathcal{G}_0 = \{g(x) = a; a \in \mathbb{R}\}$. It is an unconditional expected value and the best approximation of $f$ given my a constant function:
\begin{align*}
    \Pi_{\mathcal{G}_0}(f) 
    &= \arg \min_{g \in \mathcal{G}_0} \|f(x) - g\|^2 \\
    &= \mathbb{E}[\|f(x) - g\|^2] \\
    &= \mathbb{E}[f(x)]
\end{align*}
The main effect $f_i(x_i)$ is the projection of $f$ onto the subspace of all functions that only depend on $x_i$ and have an expected value of zero while accounting for the lower-order effects. The subspace we project onto is $\mathcal{G}_i = \{g(x) = g_i(x_i); \int g(x) d\nu (x_i) = 0\}$.
\begin{align*}
    \Pi_{\mathcal{G}_i}(f) - f_0
    &= \arg \min_{g \in \mathcal{G}_i} \|f(x) - g(x_i)\|^2 - f_0\\
    &= \mathbb{E}_{-x_i}[\|f(x) - g(x_i)\|^2] - \mathbb{E}[f(x)] \\
    &= \mathbb{E}_{-x_i}[f(X_1, \dots, x_i, \dots, X_n)] - \mathbb{E}[f(x)]
\end{align*}

The two-way interaction effect $f_{ij}(x_i,x_j)$ is the projection of $f$ onto the subspace of all functions that depend on $x_i$ and $x_j$ and have an expected value of zero in each of it's single components, i.e. $\mathcal{G}_{i,j} = \{g(x) = g_{ij}(x_i, x_j); \int g(x) d\nu (x_i) = 0 \land \int g(x) d\nu (x_j) = 0\}$. Again, we account for lower-order effects by subtracting the constant term and all main effects:
\begin{align*}
    \Pi_{\mathcal{G}_{ij}}(f) - f_0 - f_1(x_i) - \dots
    &= \arg \min_{g \in \mathcal{G}_{ij}} \|f(x) - g(x_i, x_j)\|^2 - f_0 - f_1(x_i) - \dots \\
    &= \mathbb{E}_{-x_i, -x_j}[\|f(x) - g(x_i, x_j)\|^2] - \mathbb{E}[f(x)] - \mathbb{E}_{-x_i}[f(x)]\\
    &= \mathbb{E}_{-x_i, -x_j}[f(X_1, \dots, x_i, x_j, \dots, X_n)] - \mathbb{E}[f(x)] - \mathbb{E}_{-x_i}[f(x)]
\end{align*}
{\color{blue}I think the last inequality in each definition is not directly visible, would need to be shown?}

In general, general we can write:
\begin{equation}
    f_u(x) = \Pi_{\mathcal{G}_u}(f) - \sum_{v \subsetneq u} f_v(x)
\end{equation}
We project $f$ onto the subspace spanned by the own terms of the fANOVA component to be defined, while accounting for all lower-order terms.

% where

% \[
% D_1 = \int f_1(x_1)^2 \, dx_1, \quad
% D_2 = \int f_2(x_2)^2 \, dx_2, \quad
% D_{1,2} = \int f_{1,2}(x_1, x_2)^2 \, dx_1 dx_2
% \]

% \subsection*{Alternative Formulation of fANOVA}
% Based on \cite{hooker2004} and Owen Lecture notes (find paper to cite).
% We again work with a square integrable function $f(x): [0, 1]^n \rightarrow \mathbb{R}$ with $x = (x_1, \dots, x_n)$ and $x_1, \dots, x_n$ are independent.
% The set of indices $1, \dots, n$ is denoted as $1\colon d$. $u \subset 1\colon d$ and $-u$ is the complement of $u$, i.e. $1\colon d \backslash u$.
% Given a set of indices $u = {i_1, i_2, \dots, i_{|u|}}$ we can write $x_u$ for $(x_{i_1}, x_{i_2}, \dots, x_{i_{|u|}}) = (x_i)_{i \in u}$.\par

% This notation allows us to sum over a set of indices, which avoids lengthy notation. The fANOVA can then be formulated as follows:
% \begin{equation}
%     f_{\emptyset}(\mathbf{x}) = \int_{[0, 1]^n} f(\mathbf{x}) d\nu(\mathbf{x}) = \mu
% \end{equation}

% Generally, the fANOVA decomposition can be written as
% \begin{equation}
%     f_u(\mathbf{x}) = \int_{[0,1]^{d - |u|}} \left( f(\mathbf{x}) - \sum_{v \subsetneq u} f_v(\mathbf{x}) \right) \, d\nu(\mathbf{x}_{-u}).
% \end{equation}
% We can rewrite this as follows, which simplifies the calculation of the integral:
% \begin{equation}
%     f_u(\mathbf{x}) = \int_{[0,1]^{d - |u|}} f(\mathbf{x}) \, d\nu(\mathbf{x}_{-u}) - \sum_{v \subsetneq u} f_v(\mathbf{x}).
% \end{equation}

% In Lemma A.3. Owen states a general formulation of orthogonality of two functions, which is applicable for the fANOVA components and ensures their orthogonality.
% Given to square-integrable, real-valued functions on the unit hypercube $f(x)$ and $g(x)$ with $u,v \subseteq {1, \dots, d}$, it is true that, if $u \neq v$, then
% \[
% \int_{[0,1]^n} f_u(\mathbf{x}) g_v(\mathbf{x}) \, d\nu(\mathbf{x}) = 0
% \]







