% Good historical overview of fANOVA decomposition found in: Owen (2003), Takemura (1983)

\subsection*{Early work on fANOVA}
\subsubsection*{Hoeffding decomposition 1948}
\begin{itemize}
    \item The idea of fANOVA decomposition dates back to \cite{hoeffding_class_1948}.
    \item Introduces Hoeffding decomposition (or U-statistics ANOVA decomposition).
    \item Math-workings: involves orthogonal sums, projection functions, orthogonal kernels, and subtracting lower-order contributions.
    \item Assupmtions: unclear about all but one assumptions is (mututal?) independence of input variables, which is unrealistic in practice (different generalizations to dependent variables follow, e.g. \cite{il_idrissi_hoeffding_2025})
    \item Relevance: shows that U-statistics or any symmetric function of the data can be broken down into simpler pieces (e.g., main effects, two-way interactions) without overlap.
    \item Pieces can be used to dissect/explain the variance.
    \item fANOVA performs a similar decomposition, not for U-statistics but for functions.
\end{itemize}
$\Rightarrow$\textbf{fANOVA and U-statistics}

\subsubsection*{Sobol Indices 1993, 2001}
\begin{itemize}
    \item In "Sensitivity Estimates for Nonlinear Mathematical Models" (1993), Sobol first introduces decomposition into summands of different dimensions of a (square) integrable function.
    \item Does not cite Hoeffding nor discuss U-statistics.
    \item "Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates" (2001) builds on his prior work \citep{sobol_global_2001}.
    \item Math-workings: similar to Hoeffding, involving orthogonal projections, sums, and independent terms.
    \item Sobol focuses on sensitivity analysis for deterministic models, while Hoeffding is concerned with estimates of probabilistic models.
\end{itemize}

I think in his 1993 paper Sobol mainly introduces fANOVA decomposition (definition, orthogonality, L1 integrability), already speaks of L2 integrability and variance decompsoition, which leads to Sobol indices, gives some analytical examples and MC algorithm for calculations.
In the 2001 paper he focuses on illustrating three usecases of the sobol indices + the decomposition\par
\begin{itemize}
    \item ranking of variables
    \item fixing unessential variables
    \item deleting high order members
\end{itemize}
For each of the three there are some mathematical statements, sometimes an algorithm or an example.
$\Rightarrow$\\textbf{fANOVA and sensitivity analysis}

\subsubsection*{Efron and Stein (1981)}
\begin{itemize}
    \item Use idea to proof a famous lemma on jackknife variances \citep{efron_jackknife_1981}
\end{itemize}

\subsubsection*{Stone 1994}
\begin{itemize}
    \item \cite{stone_use_1994}
    \item Math-workings: sum of main terms, lower-order terms, etc., with an identifiability constraint (zero-sum constraint); follows the same principle as the decomposition frameworks by \cite{hoeffding_class_1948} and \cite{sobol_global_2001}.
    \item All of them work independently, do not cite each other, and use the principle with different goals/build different tools on it.
    \item Stone's work is part of a broader body of fANOVA models.
\end{itemize}
$\Rightarrow$\textbf{fANOVA and smooth regression models / GAMs}\par
I think the main focus of this paper is to extend the theoretical framework of GAMs with interactions. So the baseline is logistic regression with smooth terms but only univariate components are considered.
Now the paper goes deeper into the theory where multivariate terms are also considered. For this they refer to the \ldq ANOVA decomposition\rdq of a function. The focus of the paper is on how the smooth multivariate interaction terms can be estimated, what mathematical properties they have, etc.


\subsection*{Modern Interpretations of fANOVA}
% Rabitz and Alis¸ (1999), Peccati (2004), Hooker (2007), Kuo et al. (2009), Hart and Gremaud (2018), and Chastaing, Gamboa, and
% Prieur (2012), Il Idrissi (2025)
\begin{itemize}
    \item Rabitz and Alis¸ (1999) see ANOVA decomposition as a specific high dimensional model representation (HDMR); the goal is to decompose the model iteratively from main effects, to lower order interactions and so on, but to do this in an efficient way and seletect only interaction terms that are necessary (most often lower-order interactions are sufficient). $\rightarrow$ chemistry paper
    \item Work of \cite{hooker_generalized_2007} can be seen as an attempt to generalize Hoeffding decomposition (or the Hoeffding principle) to dependent variables. According to \href{https://static1.squarespace.com/static/5f704d21e5464d602d153738/t/66ec27cadf4e8d42ed9018d0/1726752718798/20240918_SADiscord_MIL.pdf}{Slides to talk on Shapley and Sobol indices}
    \item At least in his talk which is based on the paper \cite{il_idrissi_hoeffding_2025} he puts his work in a broader context of modern attempts to generalize Hoeffding indices. So \cite{il_idrissi_hoeffding_2025} can be seen as one attempt to generalize Hoeffding decomposition to dependent variables.
\end{itemize}


% \subsection*{Formal Setting of fANOVA}
% % Assumptions and prerequisites
% Let $f(x): I[0,1]^n \rightarrow I[0,1] \quad with \quad x = (x_1, ... x_n) $ be a function from the unit hypercube to the unit interval. $f(x)$ represents a mathematical model.\par
% \textbf{Definition:} $f(x)$ can be represented as a sum of main effects and interaction effects
% \begin{equation}
%     f = f_0 + \sum_{s=1}^{n} \sum_{i_1 <...<i_s}^{n} f_{i_{1}...i_{s}} (x_{i_{1}} , ....,x_{i_{s}})
%     \label{eq:fanova_decomposition}
% \end{equation}
% with $1 \leq i_1 < .... < i_s \leq n$.
% \autoref{eq:fanova_decomposition} is the "ANOVA-representation" \cite{sobol_global_2001} or "functional ANOVA decomposition" \cite{hooker_discovering_2004} if $f_0$ is constant and the integrals of the summands $f_{i_{1}...i_{s}}$ with respect to any of their included variables are zero.
% \begin{equation}
%     \int_{0}^{1} f_{i_{1}...i_{s}} (x_{i_{1}}, ...., x_{i_{s}}) \, dx_k = 0 for k = i_1, ...., i_s
%     \label{eq:zero_mean_condition}
% \end{equation}
% In words so far: a function is decomposed into constant term $f_0$ and a sum of main effects and interaction effects. If each term "is centred" (i.e. has zero mean) with respect to the variables it includes, then the terms are orthogonal to each other. In an applied context orthogonality means that the terms capture the isolated effect and there is no redundancy in information, i.e. no information of $x_1$ is also included in the interaction of $x_{12}$.\par
% \textbf{Example for $n=3$}:
% \begin{equation}
%     f(x_1,x_2,x_3) = f_0 + f_{1}(x_1) + f_{2}(x_2) + f_{3}(x_3) + f_{12}(x_1,x_2) + f_{13}(x_1,x_3) + f_{23}(x_2,x_3)
%     \label{eq:fanova_decomposition_example}
% \end{equation}

% Given \autoref{eq:zero_mean_condition} the following holds for the components in \autoref{eq:fanova_decomposition}:
% \begin{equation}
%     \int f(x) dx = f_0
% \end{equation}
% This means that the integral over the entire domain and all inputs gives us the constant term/ intercept == overall average
% \begin{equation}
%     \int f(x) \prod_{k \neq i} d_{x_{k}} = f_0 + f_i(x_i)
% \end{equation}
% The integral over all variables expect $x_i$ is equal to adding up the overall mean and the main effect of $x_i$.
% \begin{equation}
%     \int f(x) \prod_{k \neq i,j} d_{x_{k}} = f_0 + f_i(x_i) + f_j(x_j)
% \end{equation}
% The integral over all variables expect $x_i$ and $x_j$ is equal to adding up the overall mean, the main effect of $x_i$, and the main effect of $x_j$, and the interaction effect of $x_i, x_j$.\par
% Inhaltlich: when we are interested in "the average effect of ..." we may add the main effect and their interaction effects together.??


\subsection*{Formal Introduction to fANOVA}
% Prerequesites: L2 space, inner product, orthogonal projections, integrals
\subsubsection{Prerequisites}
We are operating the space of square integrable functions. The space of square integrable functions is denoted by $\mathcal{L}^2$ and is defined as follows:
\[
\mathcal{L}^2 = \left\{ f(x) : \mathbb{E}[f^2(x)] < \infty \right\}
= \left\{ f(x) : \mathbb{R}^{n} \to \mathbb{R}, \; \textit{s.t.} \; \int f^2(x)\, d\nu(x) < \infty \right\}
\]
\begin{itemize}
    \item $\mathbb{E}$ denotes the expectation operator
    \item $\mathbb{R}^{n}$ is the n-dimensional Euclidean space
    \item $\nu$ is the measure of the Lebesgue integral
\end{itemize}
For the following we will restrict ourselves to functions defined on the unit hypercube $[0,1]^n$, i.e. all functions $f(x): [0,1]^n \rightarrow \mathbb{R}$, where $[0,1]^n \subset \mathcal{L}^2$ \par
$\mathcal{L}^2$ is a Hilbert space with an inner product defined as
\[
\langle f, g \rangle = \int f(x) g(x) \, d\mu(x)
\]
where $f,g \in \mathcal{L}^2$ are two functions.\par
The norm is then defined as
\[
\|f\| = \sqrt{\langle f, f \rangle} = \sqrt{\int f^2(x) \, d\mu(x)}
\]
\begin{itemize}
    \item inputs $x_1, \dots, x_n$ are independent
    \item we assume that each input is uniformly distributed over the unit interval, i.e. $x_i \sim U[0,1]$ for $i = 1, \dots, n$ (but this is not necessary, independence is though (ANOVA lecture notes by Owen))
\end{itemize}

{\color{blue} could cite this resource for general definition of Normed vectors paces, Hilbert space, inner product, etc. \href{https://apachepersonal.miun.se/~andrli/Bok.pdf}{https://apachepersonal.miun.se/~andrli/Bok.pdf}?}

\subsubsection{fANOVA decomposition}
This chapter is based on the formal introductions by \cite{hoeffding_class_1948, sobol_global_2001, hooker_discovering_2004}.
Let $f(x)$ be a mathematical model. $f(x) \in \mathcal{L}^2$, which means $\int|f(x)|^2 < \infty$. $f(x)$ is a multivariate function with input $x = (x_1,\dots, x_n)$ and output $f(x) \in \mathbb{R}$.
In order for fANOVA decomposition to be unique, the function has to be in the unit hypercube, i.e. $f: [0,1]^n \rightarrow \mathbb{R}$ (reference?).\par
\textbf{Definition.} We can represent such a function $f(x)$ as a sum of specific basis functions
\begin{equation}
    f = f_0 + \sum_{s=1}^{n} \sum_{i_1 <...<i_s}^{n} f_{i_{1}...i_{s}} (x_{i_{1}} , ....,x_{i_{s}})
    \label{eq:fanova_decomposition}
\end{equation}

To ensure identifiability and interpretation, we set the zero-mean constraint. It requires that all effects are centred around zero\footnote{Except for the constant term}. Since that the constant term $f_0$ captures the overall mean of $f$, the remaining effects quantify the deviation from the overall mean.
\begin{equation}
    \int_{0}^{1} f_{i_{1}...i_{s}} (x_{i_{1}}, ...., x_{i_{s}}) \, dx_k = 0 \forall k = i_1, ...., i_s
    \label{eq:zero_mean_condition}
\end{equation}
In combination with \autoref{eq:zero_mean_condition} I.M. Sobol (1993) calls \autoref{eq:fanova_decomposition} initially the \ldq Expansion into Summands of Different Dimensions\rdq. In \cite{sobol_global_2001} he renames the decomposition to the \ldq ANOVA-representation\rdq. Now, it is mostly referred to as the \ldq functional ANOVA decomposition\rdq \citep{hooker_discovering_2004}.\par
The basis components as they are defined offer a clear interpretation of the model. They are nothing other than the main effects $f_i(x_i)$, two-way interaction effects $f_{ij}(x_i,x_j)$, three-way interaction effects $f_{ijk}(x_i,x_j,x_k)$, and so on. This is why the idea of fANOVA decomposition has received increasing attention in the IML and XAI literature.\par

The single terms that make up \autoref{eq:fanova_decomposition} are defined as follows.
First, we take the integral of $f$ w.r.t. all variables:
\begin{equation}
    f_{\emptyset}(\mathbf{x}) = \int_{[0, 1]^n} f(\mathbf{x}) d\nu(\mathbf{x}) = \mathbb{E}[f(x)]
    \label{eq:intercapt}
\end{equation}
Next, we take the integral of $f$ w.r.t. all variables except for $x_i$. This gives us the sum of the constant term and the isolated effect of $x_i$, called main effect of $x_i$.
\begin{equation}
    f_0 + f_i(x_i) = \int f(x) \prod_{k \neq i} d_{x_{k}} = g_i(x_i)
    \label{eq:main_effect}
\end{equation}
Following the same principle, we can take the integral of $f$ w.r.t. all variables except for $x_i$ and $x_j$. With this we capture everything up to the interaction effect of $x_i$ and $x_j$:
\begin{equation}
    f_0 + f_i(x_i) + f_j(x_j) + f_{ij}(x_i,x_j) = \int f(x) \prod_{k \neq i,j} d_{x_{k}} = g_{ij}(x_i, x_j)
    \label{eq:interaction_effects}
\end{equation}
And so on, up to the n-way interaction of all $x_1, \dots, x_n$.\par

To actually compute the fANOVA decomposition for $f$, it is clearer to rearrange terms. We write \autoref{eq:main_effect} as
\begin{equation}
    f_i(x_i) = \int f(x) \prod_{k \neq i} d_{x_{k}} - f_0
    \label{eq:main_effect_rearranged}
\end{equation}
Then \autoref{eq:interaction_effects} becomes
\begin{equation}
    f_{ij}(x_i,x_j) = \int f(x) \prod_{k \neq i,j} d_{x_{k}} - f_0 - f_i(x_i) - f_j(x_j) 
    \label{eq:interaction_effects}
\end{equation}
Intuitively we adjust for the effects already explained by lower order terms by substracting them from {\color{blue}the overall effects (not true, not the overall but a partial effect, a conditional effect)}.\par

Therefore, it is also common to formulate the fANOVA decomposition in the following way \citep{hooker_generalized_2007,hooker_discovering_2004}:

\begin{equation}
    f_u(\mathbf{x}) = \int_{[0,1]^{d - |u|}} \left( f(\mathbf{x}) - \sum_{v \subsetneq u} f_v(\mathbf{x}) \right) \, d\nu(\mathbf{x}_{-u}).
\end{equation}
Which simplifies to:
\begin{equation}
    f_u(\mathbf{x}) = \int_{[0,1]^{d - |u|}} f(\mathbf{x}) \, d\nu(\mathbf{x}_{-u}) - \sum_{v \subsetneq u} f_v(\mathbf{x}).
\end{equation}

A technical remark: The fANOVA terms can be understood as projections.\par

$f_0$ is the projections of $f$ onto the space of all constant functions $G = \{g(x) = a; a \in \mathbb{R}\}$. It is an unconditional expected value and the best approximation of $f$ given my a constant function.\par

The main effect $f_i(x_i)$ is the projection of $f$ onto the subspace of all functions that only depend on $x_i$ and have an expected value of zero, $G = \{g(x) = g_i(x_i); \int g(x) d\nu (x_i) = 0\}$. It is a mean conditioned on $x_i$ and the best approximation of $f$ given by a function that depends on a single variable $x_i$.\par

The two-way interaction effect $f_{ij}(x_i,x_j)$ is the projection of $f$ onto the subspace of all functions that depend on $x_i$ and $x_j$ and have an expected value of zero in each of it's single components, $G = \{g(x) = g_{ij}(x_i, x_j); \int g(x) d\nu (x_i) = 0 \land \int g(x) d\nu (x_j) = 0\}$. It is the expected value conditioned on $x_i, x_j$ and the best approximation of $f$ given by a function that depends on only two variables.\par
In general, \ldq each term is calculated as the projection of $f$ onto a particular subset of the predictors, taking out the lower-order effects which have already been accounted for.\rdq \cite{hooker_discovering_2004}.\par

\subsubsection*{Orthogonality of the fANOVA terms}
Orthogonality of the fANOVA terms follows using \autoref{eq:zero_mean_condition}. If two sets of indices are not completely equivalent $(i_1, \dots, i_s) \neq (j_1, \dots, j_l)$ then
\begin{equation}
    \int f_{i_{1}, \dots, i_{s}} f_{j_{1}, \dots, j_{l}} dx = 0
    \label{eq:orthogonality}
\end{equation}

\subsubsection*{Variance decomposition}
If $f \in \mathcal{L}^2$, it follows that $f_{i_{1}, \dots, i_{n}} \in \mathcal{L}^2$ {\color{blue}proof? reference?; Sobol 1993 says it is easy to show using Schwarz inequality and the definition of the single fANOVA terms.}
Therefore, we define the variance of f as follows:
\begin{equation}
    D = \int_{K^n} f^2(x)d\nu (x) - f^2_{0} = \int_{K^n} f^2(x)d\nu (x) - (\int_{K^n} f(x)d\nu (x))^2
    \label{variance_whole}
\end{equation}
The variance of the fANOVA components is then defined as
\begin{align*}
    D_{i_{1}, \dots, i_{n}} 
    &= \int \cdots \int f^2_{i_{1}, \dots, i_{n}} \, d\nu(x_1) \cdots d\nu(x_n) \\
    &\quad - \left( \int \cdots \int f_{i_{1}, \dots, i_{n}} \, d\nu(x_1) \cdots d\nu(x_n) \right)^2 \\
    &= \int \cdots \int f^2_{i_{1}, \dots, i_{n}} \, d\nu(x_1) \cdots d\nu(x_n)
\end{align*}
This means that the variance of $f$ can be decomposed into the variance of the fANOVA components. Sobol 1993 uses this result to construct the Sobol Indices used in sensitivity analysis.

% \subsection*{Alternative Formulation of fANOVA}
% Based on \cite{hooker_discovering_2004} and Owen Lecture notes (find paper to cite).
% We again work with a square integrable function $f(x): [0, 1]^n \rightarrow \mathbb{R}$ with $x = (x_1, \dots, x_n)$ and $x_1, \dots, x_n$ are independent.
% The set of indices $1, \dots, n$ is denoted as $1\colon d$. $u \subset 1\colon d$ and $-u$ is the complement of $u$, i.e. $1\colon d \backslash u$.
% Given a set of indices $u = {i_1, i_2, \dots, i_{|u|}}$ we can write $x_u$ for $(x_{i_1}, x_{i_2}, \dots, x_{i_{|u|}}) = (x_i)_{i \in u}$.\par

% This notation allows us to sum over a set of indices, which avoids lengthy notation. The fANOVA can then be formulated as follows:
% \begin{equation}
%     f_{\emptyset}(\mathbf{x}) = \int_{[0, 1]^n} f(\mathbf{x}) d\nu(\mathbf{x}) = \mu
% \end{equation}

% Generally, the fANOVA decomposition can be written as
% \begin{equation}
%     f_u(\mathbf{x}) = \int_{[0,1]^{d - |u|}} \left( f(\mathbf{x}) - \sum_{v \subsetneq u} f_v(\mathbf{x}) \right) \, d\nu(\mathbf{x}_{-u}).
% \end{equation}
% We can rewrite this as follows, which simplifies the calculation of the integral:
% \begin{equation}
%     f_u(\mathbf{x}) = \int_{[0,1]^{d - |u|}} f(\mathbf{x}) \, d\nu(\mathbf{x}_{-u}) - \sum_{v \subsetneq u} f_v(\mathbf{x}).
% \end{equation}

% In Lemma A.3. Owen states a general formulation of orthogonality of two functions, which is applicable for the fANOVA components and ensures their orthogonality.
% Given to square-integrable, real-valued functions on the unit hypercube $f(x)$ and $g(x)$ with $u,v \subseteq {1, \dots, d}$, it is true that, if $u \neq v$, then
% \[
% \int_{[0,1]^n} f_u(\mathbf{x}) g_v(\mathbf{x}) \, d\nu(\mathbf{x}) = 0
% \]


\subsubsection{Example fANOVA decomposition}
We will now make the fANOVA decomposition of a function $f \in \mathcal{L}^2$ explicit for n = 4. We continue to follow the definition by \cite{sobol_global_2001}.


% \subsection*{An attempt to explain the motivation behind fANOVA}
% Disclaimer: Intuitive explanation.\par
% To motivate the fANOVA decomposition, we tell the story "backwards"\footnote{In the opposite way, it is usually introduced formally.}.
% We start with a statistical model $f(x)$ that takes in multiple covariables $x_1,\dots, x_n$ that are potentially interacting with each other in a complex way and having a complex effect on the target variable. We therefore think, it would be very nice to disentangle the effects of the covariables and clearly tell which influence comes from a single variable, which comes from interactions and so on.
% Mathematically this would correspond to writing $f(x)$ as a sum of isolated terms and all n-way interaction terms, i.e. \autoref{eq:fanova_decomposition}.\par
% The challenge becomes to find this specific representation of $f(x)$. In this sense we are facing an approximation problem, which we solve by using projections (we need the best approximation of $f(x)$ in a certain subspace, so the orthogonal projection onto the subspace is the best solution). A projection is defined via the inner product, which is defined via integrals in case of functions. The projections we use to define the components of the decomposition therefore look like this:
% \begin{equation}
%     \int f(x) dx = f_0
% \end{equation}
% This means that the integral over the entire domain and all inputs gives us the constant term/ intercept == overall average
% \begin{equation}
%     \int f(x) \prod_{k \neq i} d_{x_{k}} = f_0 + f_i(x_i)
% \end{equation}
% The integral over all variables expect $x_i$ is equal to adding up the overall mean and the main effect of $x_i$.
% \begin{equation}
%     \int f(x) \prod_{k \neq i,j} d_{x_{k}} = f_0 + f_i(x_i) + f_j(x_j)
% \end{equation}
% With the tool of projections we solved the approximation problem. Next we face the problem of ensuring this approximation exists. We know that a representation with orthogonal projections always exists for function in $L^2$. Thus, we choose to restrict the functions we look at to be in $L^2$. This brings us to define the fANOVA decomposition as follows.\par



\subsection*{Questions}



\begin{itemize}
    \item Use of AI tools?
    \item -----------------------
    \item In \cite{hooker_discovering_2004} they work with $F(x)$ and $f(x)$, but in \cite{sobol_global_2001} they only work with $f(x)$. I think this is only notation? \textit{Only notation.}
    \item Does orthogonality in fANOVA context mean that all terms are orthogonal to each other? Or that a term is orthogonal to all lower-order terms (\ldq Hierarchical orthogonality \rdq)? \textit{The terms are hierarchically orthogonal, so each term is orthogonal to all lower-order terms, but not to the same-order terms! So $f_1$ is not necessarily orthogonal to $f_2$ but it is orthogonal to $f_{12}$, $f_{0}$.} 
    \item Do the projections here serve as approximations? (linalg skript 2024 5.7.4 Projektionen als beste Annäherung) \textit{Yes, they can be interpreted as sort of approximation.}
    \item Which sub-space are we exactly projecting onto? Are the projections orthogonal by construction (orthogonal projections) or only when the zero-mean constraint is set? \textit{The subspace we project onto depends on the component. For $f_0$ we project onto the subspace of constant functions, for $f_1$ we project onto the subspace of all functions that involve $x_1$ and have an expected value of 0 (zero-mean constraint to ensure orthogonality). It depends on the formulation of the fANOVA decomposition if you need to explicitly set the zero-mean constraint for orthogonality or if it is met by construction.}
    \item How \ldq far\rdq should I go back, formally introduce $L^2$ space, etc. or assume that the reader is familiar with it? \textit{Yes, space, the inner product on this space should be formally introduced.}
    % \item zero mean condition vs. zero-sum condition: according to GPT zero mean condition is related to orthogonality and zero-sum to the additivity
\end{itemize}


\subsection*{Notes}
% (Desirable ) properties of fANOVA
\begin{itemize}
    \item decomposition always exists
    \item zero-mean-condition $\rightarrow$ orthogonality of the terms
    \item $K^n$-integrable functions $\rightarrow$ uniqueness of the decomposition
    \item when does the variance decomposition exist? I think this is related to square integrability, i.e. $L^2$ integrable\footnote{$L^1$ integrable does not imply $L^2$ integrable, and vice versa}, which means that the integral of the square of the function is finite
    \item keep in mind: projections, hierarchical orthogonality constraints
\end{itemize}



% % inner math workings of fANOVA
% \begin{itemize}
%     \item orthogonal projections
%     \item function spaces, finite-dimensional spaces
% \end{itemize}








