% Good historical overview of fANOVA decomposition found in: Owen (2003), Takemura (1983)

\subsection*{Early work on fANOVA}
\subsubsection*{Hoeffding decomposition 1948}
\begin{itemize}
    \item The idea of fANOVA decomposition dates back to \cite{hoeffding_class_1948}.
    \item Introduces Hoeffding decomposition (or U-statistics ANOVA decomposition).
    \item Math-workings: involves orthogonal sums, projection functions, orthogonal kernels, and subtracting lower-order contributions.
    \item Assupmtions: unclear about all but one assumptions is (mututal?) independence of input variables, which is unrealistic in practice (different generalizations to dependent variables follow, e.g. \cite{il_idrissi_hoeffding_2025})
    \item Relevance: shows that U-statistics or any symmetric function of the data can be broken down into simpler pieces (e.g., main effects, two-way interactions) without overlap.
    \item Pieces can be used to dissect/explain the variance.
    \item fANOVA performs a similar decomposition, not for U-statistics but for functions.
\end{itemize}
$\Rightarrow$\textbf{fANOVA and U-statistics}

\subsubsection*{Sobol Indices 1993, 2001}
\begin{itemize}
    \item In "Sensitivity Estimates for Nonlinear Mathematical Models" (1993), Sobol first introduces decomposition into summands of different dimensions of a (square) integrable function.
    \item Does not cite Hoeffding nor discuss U-statistics.
    \item "Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates" (2001) builds on his prior work \citep{sobol_global_2001}.
    \item Math-workings: similar to Hoeffding, involving orthogonal projections, sums, and independent terms.
    \item Sobol focuses on sensitivity analysis for deterministic models, while Hoeffding is concerned with estimates of probabilistic models.
\end{itemize}

I think in his 1993 paper Sobol mainly introduces fANOVA decomposition (definition, orthogonality, L1 integrability), already speaks of L2 integrability and variance decompsoition, which leads to Sobol indices, gives some analytical examples and MC algorithm for calculations.
In the 2001 paper he focuses on illustrating three usecases of the sobol indices + the decomposition\par
\begin{itemize}
    \item ranking of variables
    \item fixing unessential variables
    \item deleting high order members
\end{itemize}
For each of the three there are some mathematical statements, sometimes an algorithm or an example.
$\Rightarrow$\\textbf{fANOVA and sensitivity analysis}

\subsubsection*{Efron and Stein (1981)}
\begin{itemize}
    \item Use idea to proof a famous lemma on jackknife variances \citep{efron_jackknife_1981}
\end{itemize}

\subsubsection*{Stone 1994}
\begin{itemize}
    \item \cite{stone_use_1994}
    \item Math-workings: sum of main terms, lower-order terms, etc., with an identifiability constraint (zero-sum constraint); follows the same principle as the decomposition frameworks by \cite{hoeffding_class_1948} and \cite{sobol_global_2001}.
    \item All of them work independently, do not cite each other, and use the principle with different goals/build different tools on it.
    \item Stone's work is part of a broader body of fANOVA models.
\end{itemize}
$\Rightarrow$\textbf{fANOVA and smooth regression models / GAMs}\par
I think the main focus of this paper is to extend the theoretical framework of GAMs with interactions. So the baseline is logistic regression with smooth terms but only univariate components are considered.
Now the paper goes deeper into the theory where multivariate terms are also considered. For this they refer to the \ldq ANOVA decomposition\rdq of a function. The focus of the paper is on how the smooth multivariate interaction terms can be estimated, what mathematical properties they have, etc.


\subsection*{Modern Interpretations of fANOVA}
% Rabitz and Alis¸ (1999), Peccati (2004), Hooker (2007), Kuo et al. (2009), Hart and Gremaud (2018), and Chastaing, Gamboa, and
% Prieur (2012), Il Idrissi (2025)
\begin{itemize}
    \item Rabitz and Alis¸ (1999) see ANOVA decomposition as a specific high dimensional model representation (HDMR); the goal is to decompose the model iteratively from main effects, to lower order interactions and so on, but to do this in an efficient way and seletect only interaction terms that are necessary (most often lower-order interactions are sufficient). $\rightarrow$ chemistry paper
    \item Work of \cite{hooker_generalized_2007} can be seen as an attempt to generalize Hoeffding decomposition (or the Hoeffding principle) to dependent variables. According to \href{https://static1.squarespace.com/static/5f704d21e5464d602d153738/t/66ec27cadf4e8d42ed9018d0/1726752718798/20240918_SADiscord_MIL.pdf}{Slides to talk on Shapley and Sobol indices}
    \item At least in his talk which is based on the paper \cite{il_idrissi_hoeffding_2025} he puts his work in a broader context of modern attempts to generalize Hoeffding indices. So \cite{il_idrissi_hoeffding_2025} can be seen as one attempt to generalize Hoeffding decomposition to dependent variables.
\end{itemize}


% \subsection*{Formal Setting of fANOVA}
% % Assumptions and prerequisites
% Let $f(x): I[0,1]^n \rightarrow I[0,1] \quad with \quad x = (x_1, ... x_n) $ be a function from the unit hypercube to the unit interval. $f(x)$ represents a mathematical model.\par
% \textbf{Definition:} $f(x)$ can be represented as a sum of main effects and interaction effects
% \begin{equation}
%     f = f_0 + \sum_{s=1}^{n} \sum_{i_1 <...<i_s}^{n} f_{i_{1}...i_{s}} (x_{i_{1}} , ....,x_{i_{s}})
%     \label{eq:fanova_decomposition}
% \end{equation}
% with $1 \leq i_1 < .... < i_s \leq n$.
% \autoref{eq:fanova_decomposition} is the "ANOVA-representation" \cite{sobol_global_2001} or "functional ANOVA decomposition" \cite{hooker_discovering_2004} if $f_0$ is constant and the integrals of the summands $f_{i_{1}...i_{s}}$ with respect to any of their included variables are zero.
% \begin{equation}
%     \int_{0}^{1} f_{i_{1}...i_{s}} (x_{i_{1}}, ...., x_{i_{s}}) \, dx_k = 0 for k = i_1, ...., i_s
%     \label{eq:zero_mean_condition}
% \end{equation}
% In words so far: a function is decomposed into constant term $f_0$ and a sum of main effects and interaction effects. If each term "is centred" (i.e. has zero mean) with respect to the variables it includes, then the terms are orthogonal to each other. In an applied context orthogonality means that the terms capture the isolated effect and there is no redundancy in information, i.e. no information of $x_1$ is also included in the interaction of $x_{12}$.\par
% \textbf{Example for $n=3$}:
% \begin{equation}
%     f(x_1,x_2,x_3) = f_0 + f_{1}(x_1) + f_{2}(x_2) + f_{3}(x_3) + f_{12}(x_1,x_2) + f_{13}(x_1,x_3) + f_{23}(x_2,x_3)
%     \label{eq:fanova_decomposition_example}
% \end{equation}

% Given \autoref{eq:zero_mean_condition} the following holds for the components in \autoref{eq:fanova_decomposition}:
% \begin{equation}
%     \int f(x) dx = f_0
% \end{equation}
% This means that the integral over the entire domain and all inputs gives us the constant term/ intercept == overall average
% \begin{equation}
%     \int f(x) \prod_{k \neq i} d_{x_{k}} = f_0 + f_i(x_i)
% \end{equation}
% The integral over all variables expect $x_i$ is equal to adding up the overall mean and the main effect of $x_i$.
% \begin{equation}
%     \int f(x) \prod_{k \neq i,j} d_{x_{k}} = f_0 + f_i(x_i) + f_j(x_j)
% \end{equation}
% The integral over all variables expect $x_i$ and $x_j$ is equal to adding up the overall mean, the main effect of $x_i$, and the main effect of $x_j$, and the interaction effect of $x_i, x_j$.\par
% Inhaltlich: when we are interested in "the average effect of ..." we may add the main effect and their interaction effects together.??


\subsection*{Formal Introudction to fANOVA}
Let $f(x)$ be a mathematical model. $f(x) \in L^2$, which means $\int|f(x)|^2 < \infty$. $f(x)$ is a multivariate function with input $x = (x_1,\dots, x_n)$ and output $f(x) \in R$.\par
\textbf{Definition.} We can represent $f(x)$ as a sum of specific basis functions
\begin{equation}
    f = f_0 + \sum_{s=1}^{n} \sum_{i_1 <...<i_s}^{n} f_{i_{1}...i_{s}} (x_{i_{1}} , ....,x_{i_{s}})
    \label{eq:fanova_decomposition}
\end{equation}
The basis components are constructed via projections:
\begin{equation}
    \int f(x) dx = f_0
\end{equation}
The constant term $f_0$ (intercept) is the projection of $f(x)$ onto the constant function $1$.
\begin{equation}
    \int f(x) \prod_{k \neq i} d_{x_{k}} = f_0 + f_i(x_i)
\end{equation}
The main effect $f_i(x_i)$ is the projection of $f(x)$ onto the subspace spanned by the constant term $f_0$ and all main-effect functions $f_i(x_j)$, where $j \neq i$. 
\begin{equation}
    \int f(x) \prod_{k \neq i,j} d_{x_{k}} = f_0 + f_i(x_i) + f_j(x_j) + f_{ij}(x_i,x_j)
\end{equation}
The two-way interaction effect $f_{ij}(x_i,x_j)$ is the projection of $f(x)$ onto the subspace spanned by the constant term $f_0$ and all main effects $f_k(x_k)$ that are not involved in the interaction (so $k \neq i,j$) (?).\par
In general, \ldq each term is calculated as the projection of $f$ onto a particular subset of the predictors, taking out the lower-order effects which have already been accounted for.\rdq \cite{hooker_discovering_2004}.\par

The zero-mean constraint is set to ensure that the basis components are orthogonal, i.e.
\begin{equation}
    \int_{0}^{1} f_{i_{1}...i_{s}} (x_{i_{1}}, ...., x_{i_{s}}) \, dx_k = 0 for k = i_1, ...., i_s
    \label{eq:zero_mean_condition}
\end{equation}
In combination with \autoref{eq:zero_mean_condition} I.M. Sobol (1993) calls \autoref{eq:fanova_decomposition} initially the \ldq Expansion into Summands of Different Dimensions\rdq. In \cite{sobol_global_2001} he renames the decomposition to the \ldq ANOVA-representation\rdq. Now, it is mostly referred to as the \ldq functional ANOVA decomposition\rdq \citep{hooker_discovering_2004}.\par
The basis components as they are defined offer a clear interpretation of the model. They are nothing other than the main effects $f_i(x_i)$, two-way interaction effects $f_{ij}(x_i,x_j)$, three-way interaction effects $f_{ijk}(x_i,x_j,x_k)$, and so on. This is why the idea of fANOVA decomposition has received increasing attention in the IML and XAI literature.\par

Next paragraph probably on the functional variance $\sigma^2(f)$ and its' decomposition.

% \subsection*{An attempt to explain the motivation behind fANOVA}
% Disclaimer: Intuitive explanation.\par
% To motivate the fANOVA decomposition, we tell the story "backwards"\footnote{In the opposite way, it is usually introduced formally.}.
% We start with a statistical model $f(x)$ that takes in multiple covariables $x_1,\dots, x_n$ that are potentially interacting with each other in a complex way and having a complex effect on the target variable. We therefore think, it would be very nice to disentangle the effects of the covariables and clearly tell which influence comes from a single variable, which comes from interactions and so on.
% Mathematically this would correspond to writing $f(x)$ as a sum of isolated terms and all n-way interaction terms, i.e. \autoref{eq:fanova_decomposition}.\par
% The challenge becomes to find this specific representation of $f(x)$. In this sense we are facing an approximation problem, which we solve by using projections (we need the best approximation of $f(x)$ in a certain subspace, so the orthogonal projection onto the subspace is the best solution). A projection is defined via the inner product, which is defined via integrals in case of functions. The projections we use to define the components of the decomposition therefore look like this:
% \begin{equation}
%     \int f(x) dx = f_0
% \end{equation}
% This means that the integral over the entire domain and all inputs gives us the constant term/ intercept == overall average
% \begin{equation}
%     \int f(x) \prod_{k \neq i} d_{x_{k}} = f_0 + f_i(x_i)
% \end{equation}
% The integral over all variables expect $x_i$ is equal to adding up the overall mean and the main effect of $x_i$.
% \begin{equation}
%     \int f(x) \prod_{k \neq i,j} d_{x_{k}} = f_0 + f_i(x_i) + f_j(x_j)
% \end{equation}
% With the tool of projections we solved the approximation problem. Next we face the problem of ensuring this approximation exists. We know that a representation with orthogonal projections always exists for function in $L^2$. Thus, we choose to restrict the functions we look at to be in $L^2$. This brings us to define the fANOVA decomposition as follows.\par




\subsection*{Questions}
\begin{itemize}
    \item In \cite{hooker_discovering_2004} they work with $F(x)$ and $f(x)$, but in \cite{sobol_global_2001} they only work with $f(x)$. I think this is only notation?
    \item Does orthogonality in fANOVA context mean that all terms are orthogonal to each other? Or that a term is orthogonal to all lower-order terms (\ldq Hierarchical orthogonality \rdq)?
    \item Do the projections here serve as approximations? (linalg skript 2024 5.7.4 Projektionen als beste Annäherung)
    % \item zero mean condition vs. zero-sum condition: according to GPT zero mean condition is related to orthogonality and zero-sum to the additivity
\end{itemize}


\subsection*{Notes}
% (Desirable ) properties of fANOVA
\begin{itemize}
    \item decomposition always exists
    \item zero-mean-condition $\rightarrow$ orthogonality of the terms
    \item $K^n$-integrable functions $\rightarrow$ uniqueness of the decomposition
    \item when does the variance decomposition exist? I think this is related to square integrability, i.e. $L^2$ integrable\footnote{$L^1$ integrable does not imply $L^2$ integrable, and vice versa}, which means that the integral of the square of the function is finite
    \item keep in mind: projections, hierarchical orthogonality constraints
\end{itemize}



% % inner math workings of fANOVA
% \begin{itemize}
%     \item orthogonal projections
%     \item function spaces, finite-dimensional spaces
% \end{itemize}








