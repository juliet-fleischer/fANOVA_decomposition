% Good historical overview of fANOVA decomposition found in: Owen (2003), Takemura (1983)

\subsection{Early Work on fANOVA}
The main idea of the fANOVA decomposition is to decompose a statistical model into the sum of the main effects and interaction effects of its input variables. The underlying principle of fANOVA decomposition dates back to \cite{hoeffding1948}. In his famous paper he introduced U-statistics, along with the ``Hoeffding decomposition'', which allows to write a symmetric function of the data as a sum of orthogonal components. \cite{sobol1993sensitivity} used the same principle and applied it to deterministic mathematical models.
% Both methods involve the sum of orthogonal components and independent input variables.
He built on the originally called ``Decomposition into Summands of Different Dimension'' in \cite{sobol2001}, where he introduces Sobol indices and renames the method to the ``ANOVA-representation''. Sobol indices are now commonly used in sensitivity analysis. \cite{efron1981} use the idea of the decomposition to proof their famous lemma on jackknife variances. \cite{stone1994} mainly uses fANOVA decomposition to base smooth regression models with interactions on it and his paper is the building block for a broader body of work of fANOVA-based models {\color{blue}example citations needed}.


\subsection{Modern Work on fANOVA}
The fANOVA decomposition has a long history with roots in mathematical statistics and non-parametric estimation theory. In more recent years, the method has been rediscovered by the machine-learning community, especially in the context of interpretable machine learning (IML) and explainable AI (XAI). \cite{hooker2004} introduces the fANOVA decomposition with the goal of providing a global explanation method for black-box models. Since the assumptions of independent variables in classical fANOVA is often too restrictive in practice, \cite{hooker2007} generalizes the method to dependent variables. A recent paper by \cite{ilidrissi2025} can be seen as another approach to generalize the principle of fANOVA decomposition to dependent inputs.\par
There are specific domains of statistics, such as geostatistics, that explcitly build models on fANOVA framework (see \cite{muehlenstaedt2012} for fANOVA Kriging models). And recent work discovered interesting mathematical parallels between fANOVA and other IML methods, such as PDP \cite{friedman2001}, or Shapley values (\cite{fumagalli2025}, Herren, Owen preprint).

\cite{liu2006} use of fANOVA and sensitivity analysis for functions arising in computational finance.
\cite{owen2013} formal intro to fANOVA decomposition and generalization of Sobol indices.

\textbf{fANOVA and U-statistics, fANOVA and sensitivity analysis, fANOVA and GAMs (with interactions)}

\subsection{Formal Introduction to fANOVA}

\subsubsection*{fANOVA decomposition}
This chapter is based on the formal introductions by \cite{sobol1993sensitivity, sobol2001, hooker2004}, Owen, \cite{muehlenstaedt2012}. Where suitable we show both formulations of the fANOVA, via the integral and via the expected value.
Let $i_1, \dots , i_s$ denote a set of indices. For now, we assume that $X_i \overset{\text{i.i.d.}}{\sim} \mathcal{U}[0, 1]$ and work in the measure space $(X, \mathcal{F}, \nu) = ([0, 1]^n, \mathcal{B}([0, 1]^n), \lambda_{n})$. $\mathcal{B}([0, 1]^n)$ is the Borel $\sigma$-algebra on the n-dimensional unit interval and $\lambda_{n}$ is the n-dimensional Lebesgue measure. 
The general inner product and norm we defined earlier simplify under these assumptions.\par
The inner product under uniform distribution assumption:
\[
\langle f, g \rangle = \int f(x) g(x) \, d(x) \quad \forall f, g \in \mathcal{L}^2
\]
The norm under uniform distribution assumption:
\[
\|f\| = \sqrt{\langle f, f \rangle} = \sqrt{\int f^2(x) \, d(x)} \quad \forall f \in \mathcal{L}^2
\]

\textbf{Definition.} Let $f(x)$  be a mathematical model with input $X_i$ as described above. We can represent such a model $f$ as a sum of specific basis functions
\begin{equation}
    f(x) = f_0 + \sum_{s=1}^{n} \sum_{i_1 <...<i_s}^{n} f_{i_{1}...i_{s}} (x_{i_{1}} , ....,x_{i_{s}})
    \label{eq:fanova_decomposition}
\end{equation}

To ensure identifiability and interpretation, we set the zero-mean constraint. It requires that all effects, except for the constant terms, are centred around zero.
Mathematically this means that the effects integrate to zero w.r.t. their own variables:
\begin{equation}
    \int f_{i_{1}...i_{s}} (x_{i_{1}}, ...., x_{i_{s}}) \, d\nu (x_k) = 0 \quad \forall k = i_1, ...., i_s
    \label{eq:zero_mean_condition}
\end{equation}
% In combination with the zero-mean constraint (\autoref{eq:zero_mean_condition}),  \cite{sobol1993sensitivity} calls \autoref{eq:fanova_decomposition} initially the \ldq Expansion into Summands of Different Dimensions\rdq. In \cite{sobol2001} he renames the decomposition to the \ldq ANOVA-representation\rdq. Now, it is mostly referred to as the \ldq functional ANOVA decomposition\rdq \citep{hooker2004}.\par

% The fANOVA decomposition for a \( f(x_1, x_2, x_3, x_4) \) would look like
% \begin{align*}
%     f(x_1, x_2, x_3, x_4) &= f_0 + f_1(x_1) + f_2(x_2) + f_3(x_3) + f_4(x_4) \\
%     &+ f_{1,2}(x_1, x_2) + f_{1,3}(x_1, x_3) + f_{1,4}(x_1, x_4) + f_{2,3}(x_2, x_3) + f_{2,4}(x_2, x_4) + f_{3,4}(x_3, x_4) \\
%     &+ f_{1,2,3}(x_1, x_2, x_3) + f_{1,2,4}(x_1, x_2, x_4) + f_{1,3,4}(x_1, x_3, x_4) + f_{2,3,4}(x_2, x_3, x_4) \\
%     &+ f_{1,2,3,4}(x_1, x_2, x_3, x_4)
% \end{align*}

The individual terms that make up \autoref{eq:fanova_decomposition} are defined in the following.
To get the constant term, we take the integral of $f$ w.r.t. all variables:
\begin{equation}
    f_{0}(x) = \int f(x) d\nu(x) = \mathbb{E}[f(X)]
    \label{eq:intercapt}
\end{equation}
The constant term $f_0$ captures the overall mean of $f$ and serves as a baseline. Since the the remaining effects are centred around zero, they quantify the deviation from the overall mean.  Next, we take the integral of $y$ w.r.t. all variables except for $x_i$. This represents $f$ as the sum of the constant term and the isolated effect of one variable $x_i$ (main effect of $x_i$). This partial integral is equivalent to the expected value conditioned on the variable of interest $x_i$.
\begin{equation}
    f_0 + f_i(x_i) = \int f(x) \prod_{k \neq i} \nu (d_{x_{k}}) = \mathbb{E}[f(X) | X_i = x_i]
    \label{eq:main_effect}
\end{equation}
Following the same principle, we can take the integral of $f$ w.r.t. all variables except for $x_i$ and $x_j$. With this we capture everything up to the interaction effect of $x_i$ and $x_j$. This is equivalent to the expected value conditioned on both variables $x_i$ and $x_j$:
\begin{equation}
    f_0 + f_i(x_i) + f_j(x_j) + f_{ij}(x_i,x_j) = \int f(x) \prod_{k \neq i,j} \nu(d_{x_{k}}) = \mathbb{E}[f(X) | X_i = x_i, X_j = x_j]
    \label{eq:interaction_effects}
\end{equation}
For a successive construction of the fANOVA decomposition, we can generally write:
\begin{equation}
    \int f(x) \prod_{k \notin u} \nu(d_{x_{k}}) = \mathbb{E}[f(X) | X_u = x_u]
    \label{eq:fanova_component}
\end{equation}
With these partial integrations (or conditional expected values) we build up the fANOVA decomposition in a cumulative way. To actually see the fANOVA terms defined in isolation, it is clearer to rearrange terms. When we rearrange \autoref{eq:main_effect} we see that the main effect of $x_i$ is calculated by taking the marginal effect while explicitly accounting for what was already explained by lower-order terms, in this case the intercept:
\begin{equation}
    f_i(x_i) = \int f(x) \prod_{k \neq i} \nu(d_{x_{k}}) - f_0
    \label{eq:main_effect_rearranged}
\end{equation}
The two-way interactions can then be seen as the marginal effects of the involved variables, while accounting for all main effects and the constant term:
\begin{equation}
    f_{ij}(x_i,x_j) = \int f(x) \prod_{k \neq i,j} \nu(d_{x_{k}}) - f_0 - f_i(x_i) - f_j(x_j) 
    \label{eq:interaction_effects}
\end{equation}

Therefore, it is also common to formulate the fANOVA decomposition in the following way \citep{hooker2007,hooker2004}:
\begin{equation}
    f_u(x) = \int_{[0,1]^{d - |u|}} \left( f(x) - \sum_{v \subsetneq u} f_v(x) \right) \, d\nu(x_{-u}).
\end{equation}
This means we subtract all lower-order terms from the original function $f$ and then integrate over all the variables not in $u$ to get the effect of $x_u$. Using the linearity of the integral, we can also first take the partial integral of the original function w.r.t. all variables not in $u$ and then subtract all the lower-order terms, as we did above for the main effects and two-way interaction effects. So generally we write:
\begin{equation}
    f_u(x) = \int_{[0,1]^{d - |u|}} f(x) \, d\nu(x_{-u}) - \sum_{v \subsetneq u} f_v(x).
\end{equation}

The basis components offer a clear interpretation of the model, decomposing it into main effects, two-way interaction effects, and so on. This is why fANOVA decomposition has received increasing attention in the IML and XAI literature, holding the potential for a global explanation method of black box models.\par

\subsubsection*{Example 1}
Before moving to properties of the fANOVA decomposition, let us introduce a simple function $g$ as running example. It contains a constant term $a$, isolated linear effects of two variables $x_1$ and $x_2$ and their interaction.
\[
g_{1}(x_1, x_2) = a + x_1 + 2x_2 + x_1x_2 \quad \text{for} \quad a, x_1, x_2 \in \mathbb{R}
\]
Computing the fANOVA decomposition of $g(x_1, x_2)$ by hand, we start with the constant term and make use of formulation via the expected value instead of the integral for notational simplicity:
\[
f_0 = \mathbb{E}[g_{1}(x_1, x_2)] = \mathbb{E}[a + x_1 + 2x_2 + x_1x_2] = \mathbb{E}[a] + \mathbb{E}[x_1] + 2\mathbb{E}[x_2] + \mathbb{E}[x_1x_2]
\]
Making use of the independence assumption of $x_1$ and $x_2$, the last term can be written as the product of the expected values. Additionally, given the zero-mean constraint, all terms, except for the constant, vanish and we obtain:
\[
f_0 = \mathbb{E}[a] + \mathbb{E}[x_1] + 2\mathbb{E}[x_2] + \mathbb{E}[x_1]\mathbb{E}[x_2] = a
\]
Under zero-mean constraint and independence, the main effects and the interaction effect can be computed as follows:
\begin{align*}
f_1(x_1) &= \mathbb{E}_{X_2}[g_{1}(x_1, X_2)] - f_0 \\
&= \mathbb{E}_{X_2}[a + x_1 + 2x_2 + x_1x_2] - a \\
&= x_1 + 2\mathbb{E}[x_2] + x_1\mathbb{E}[x_2] = x_1\\
f_2(x_2) &= \mathbb{E}_{X_1}[g_{1}(X_1, x_2)] - f_0 \\
&= \mathbb{E}_{X_1}[a + x_1 + 2x_2 + x_1x_2] - a \\
&= \mathbb{E}_{X_1}[x_1] + 2x_2 + x_2\mathbb{E}_{X_1}[x_1] = 2x_2\\
f_{12}(x_1, x_2) &= \mathbb{E}[g_{1}(x_1, x_2)] - f_0 - f_1(x_1) - f_2(x_2) \\
&= a + x_1 + 2x_2 + x_1x_2 - a - x_1 - 2x_2 = x_1x_2
\end{align*}

It comes as no surprise that in this simple case the fANOVA decomposition does not provide any additional insights. This is because the model consists of only linear terms, constant terms, and an interaction. We show this simple example nevertheless to illustrate at which step we use which assumption. Understanding this will be relevant for the generalization of the method to dependent inputs later on. Also, it is interesting to compare this example with only linear effects (and an interaction) to the following, which will include a non-linear effect.\par

\subsubsection*{Example 2}
We now look at the function $g_{2} = a + x_1 + x_{2}^2$ which includes a quadratic effects. The constant fANOVA term is given by:
\begin{align*}
    &f_0 = \mathbb{E}[g_{2}(x_1, x_2)] = \mathbb{E}[a + x_1 + x_{2}^2] = a + \mathbb{E}[X_1] + \mathbb{E}[X_{2}^2] = a + \frac{1}{12} \\
    &\text{This works because we are still in the setting, in which we assume } X_i \overset{\text{i.i.d.}}{\sim} \mathcal{U}[0, 1], \\
    &\text{in combination with the zero-mean constraint this allows us to state:} \\
    &\mathbb{E}[X_2^2] = \mathbb{V}[X_2] = \frac{1}{12}(1 - 0)^2 = \frac{1}{12} \\
    &\text{Next, we write the main effects:} \\
    &f_1(x_1) = \mathbb{E}_{X_2}[g_{2}(x_1, X_2)] - f_0 = \mathbb{E}_{X_2}[a + x_1 + X_{2}^2] - f_0 \\
    &= a + x_1 + \mathbb{E}[X_{2}^2] - f_0 = a + x_1 + \frac{1}{12} - \left( a + \frac{1}{12} \right) = x_1 \\
    &f_2(x_2) = \mathbb{E}_{X_1}[g_{2}(X_1, x_2)] - f_0 = \mathbb{E}_{X_1}[a + X_1 + x_{2}^2] - f_0 \\
    &= a + \mathbb{E}[X_1] + x_{2}^2 - f_0 = a + x_{2}^2 - \left( a + \frac{1}{12} \right) = x_{2}^2 - \frac{1}{12} \\
    &\text{Finally, we compute the interaction effect:} \\
    &f_{12}(x_1, x_2) = \mathbb{E}[g_{2}(x_1, x_2)] - f_0 - f_1(x_1) - f_2(x_2) \\
    &= a + x_1 + x_{2}^2 - \left( a + \frac{1}{12} \right) - x_1 - \left( x_{2}^2 - \frac{1}{12} \right) = 0
\end{align*}
This example reveals something interesting about quadratic effects. It shows that they contain a constant effects, which is attributed to the constant terms $f_0$ and subtracted from the main effects. The quadratic effects does not influence the effects of the other terms, as the main effects of $x_1$ behaves as one would expect for a linear term; same goes for the non-existent interaction effect. 
Under the uniform distribution assumption, we can continue the calculations for higher powers of $X_2$ and still get a nice representation of the components, because following Cavalieri's quadrature formula we can find the moment of a uniformly distributed random variable $X$ on the unit interval raised to the power of $k$ as follows:
\begin{equation}
    \mathbb{E}[X^k] = \int_0^1 x^k \, dx = \frac{1}{k+1} \quad \text{for } k \in \mathbb{N}_0
    \label{eq:cavalieri_formula}
\end{equation}
Therefore, if we deal with a cubic term $g_3 = a + x_1 + x_2^2 + x_2^3$, we can compute the fANOVA decomposition as follows:
\begin{align*}
    &f_0 = a + \mathbb{E}[X_1] + \mathbb{E}[X_2^2] + \mathbb{E}[X_2^3] = a + \frac{1}{12} + \frac{1}{4} = a + \frac{1}{3} \\
    &f_1(x_1) = x_1 \\
    &f_2(x_2) = x_{2}^2 - \frac{1}{12} + x_{2}^3 - \frac{1}{4} = x_{2}^2 + x_{2}^3 - \frac{7}{12} \\
    &f_{12}(x_1, x_2) = 0
\end{align*}

\subsubsection*{Orthogonality of the fANOVA terms}
Orthogonality of the fANOVA terms follows using the zero-mean constraint (\autoref{eq:zero_mean_condition}). If two sets of indices are not completely equivalent $(i_1, \dots, i_s) \neq (j_1, \dots, j_l)$ then
\begin{equation}
    \int f_{i_{1}, \dots, i_{s}} f_{j_{1}, \dots, j_{l}} d(x) = 0
    \label{eq:orthogonality}
\end{equation}
This means that fANOVA terms are ``fully orthogonal'' to each other, meaning not only terms of different order are orthogonal to each other but also terms of the same order are.
In our example from before we can test this for $i = 1$ and $j = 2$:
\begin{align*}
    \int f_{1}(x_1) f_{2}(x_2) d(x) &= \int x_1 \cdot 2x_2 \, dx_1 \, dx_2 = \mathbb{E}[x_12x_2] = \mathbb{E}[x_1] \cdot 2\mathbb{E}[x_2] = 0
\end{align*}
% To see this, consider a simple example for $(i_1, i_2) = (1, 2)$ and $(j_1, j_2) = (1, 3)$. We take the inner product between these fANOVA components:
% \[
% \int_0^1 \int_0^1 \int_0^1 f_{1,2}(x_1, x_2) \cdot f_{1,3}(x_1, x_3) \, dx_1 \, dx_2 \, dx_3
% \]

% It is allowed to switch the order of integration because of the independence assumption and \href{https://en.wikipedia.org/wiki/Fubini%27s_theorem}{Fubini theorem}, so we begin by integrating with respect to \( x_2 \). Because $x_2$ integrates to zero w.r.t any of its own terms, we immediately get that the whole integral becomes zero:
% \[
% \int_0^1 \int_0^1 \left( \int_0^1 f_{1,2}(x_1, x_2) \cdot f_{1,3}(x_1, x_3) \, dx_2 \right) dx_1 dx_3 = 0
% \]
To write the expected value of a product as the product of the expected values we needed the independence assumption. To state that the product of the expected values is equal to zero, we used the zero-mean constraint. This shows that the independence assumption and zero-mean constraint are critical to ensure orthogonality in this traditional formulation of the fANOVA decomposition. This is of course also true for terms of different order, e.g. \( f_{1,2}(x_1, x_2) \) and \( f_{1}(x_1) \). Orthogonality ensures that the effects do not overlap and each term represents the isolated contribution.

\subsubsection*{Variance decomposition}
The variance decomposition is Sobol's major use of fANOVA. He built the Sobol indices for sensitivity analysis on it. We sketch the variance decomposition here and note that it is only possible under independence assumption.\par
If $f \in \mathcal{L}^2$, then $f_{i_{1}, \dots, i_{n}} \in \mathcal{L}^2$ {\color{blue}proof? reference?; Sobol 1993 says it is easy to show using Schwarz inequality and the definition of the single fANOVA terms.}
Therefore, we define the variance of $f$ as follows:
\begin{align*}
    \sigma &= \int f^2(x)d\nu (x) - (f_0)^2 \\
    &= \int f^2(x)d\nu (x) - (\int f(x)d\nu (x))^2 \\
    &= \mathbb{E}[f^2(x)] - \mathbb{E}[f(x)]^2
    \label{variance_whole}
\end{align*}
The variance of the fANOVA components is then defined as
\begin{align*}
    \sigma(x_{i_1}, \dots, x_{i_n})
    &= \int \cdots \int f^2_{i_{1}, \dots, i_{n}} \, d\nu(x_1) \cdots d\nu(x_n) - \left( \int \cdots \int f_{i_{1}, \dots, i_{n}} \, d\nu(x_1) \cdots d\nu(x_n) \right)^2\\
    &= \mathbb{E}[f^2_{i_{1}, \dots, i_{n}}] - \mathbb{E}[f_{i_{1}, \dots, i_{n}}]^2
\end{align*}
Because of the zero-mean constraint (\autoref{eq:zero_mean_condition}) the second term vanishes and we get
\begin{align*}
    \sigma(x_{i_1}, \dots, x_{i_n})
    &= \int \cdots \int f^2_{i_{1}, \dots, i_{n}} \, d\nu(x_1) \cdots d\nu(x_n)\\
    &= \mathbb{E}[f^2_{i_{1}, \dots, i_{n}}]
\end{align*}

With the definition of the total variance $D$ and the component-wise variance $D_{i_{1}, \dots, i_{n}}$ we can now see that the total variance can be decomposed into the sum of the component-wise variances.

We come back to our example \( g(x_1, x_2) \) to illustrate the variance decomposition.
\begin{align*}
    \sigma &= \int g^2(x_1, x_2)\, d\nu(x) - f_{0}^2 \\
    &= \mathbb{E}[g^2(x_1, x_2)] - a^2 \\
    &= \mathbb{E}[(x_1 + 2x_2 + x_1x_2 + a)^2] - a^2 \\
    &= \mathbb{E}\left[
        x_1^2 + 4x_2^2 + x_1^2x_2^2 + a^2
        + 4x_1x_2 + 2x_1^2x_2 + 2a x_1
        + 4x_1x_2^2 + 4a x_2 + 2a x_1x_2
    \right] - a^2 \\
    &= \mathbb{E}[x_1^2] + 4\mathbb{E}[x_2^2] + \mathbb{E}[x_1^2x_2^2]
    + 4\mathbb{E}[x_1x_2] + 2\mathbb{E}[x_1^2x_2]
    + 2a\mathbb{E}[x_1] + 4\mathbb{E}[x_1x_2^2]
    + 4a\mathbb{E}[x_2] + 2a\mathbb{E}[x_1x_2] \\
    &= \sigma^2(x_1) + 4\sigma^2(x_2) + \sigma^2(x_1x_2) + 2\mathbb{E}[x_1^2x_2] + 4\mathbb{E}[x_1x_2^2]
\end{align*}

This holds because:
\begin{align*}
    \sigma(X_1) &= \mathbb{E}[X_1^2] - (\mathbb{E}(X_1))^2 = \mathbb{E}[X_1^2] \\
    4\sigma(X_2) &=  \sigma(2X_2) = \mathbb{E}[(2X_2)^2] - (\mathbb{E}(2X_2))^2 = \mathbb{E}[(2X_2)^2] \\
    \sigma(X_1X_2) &= \mathbb{E}[X_1^2X_2^2] - (\mathbb{E}[X_1X_2])^2 = \mathbb{E}[X_1^2X_2^2]
\end{align*}

Notice that we used the independence assumption and the zero-mean constraint again for the variance decomposition.

% We illustrate this for a fANOVA decomposition function \( f(x_1, x_2) \in L^2 \):

% \[
% f(x_1, x_2) = f_0 + f_1(x_1) + f_2(x_2) + f_{1,2}(x_1, x_2)
% \]

% \vspace{1em}

% First, we square the decomposition

% \begin{align*}
% f^2(x_1, x_2) &= \left(f_0 + f_1(x_1) + f_2(x_2) + f_{1,2}(x_1, x_2)\right)^2 \\
% &= f_0^2 + f_1(x_1)^2 + f_2(x_2)^2 + f_{1,2}(x_1, x_2)^2 \\
% &\quad + 2f_0 f_1(x_1) + 2f_0 f_2(x_2) + 2f_0 f_{1,2}(x_1, x_2) \\
% &\quad + 2f_1(x_1) f_2(x_2) + 2f_1(x_1) f_{1,2}(x_1, x_2) + 2f_2(x_2) f_{1,2}(x_1, x_2)
% \end{align*}

% \vspace{1em}

% Next, we integrate over the domain \( [0,1]^2 \)

% \begin{align*}
% \int f^2(x_1, x_2) \, dx_1 dx_2 &= \int f_0^2 \, dx_1 dx_2 + \int f_1(x_1)^2 \, dx_1 dx_2 + \int f_2(x_2)^2 \, dx_1 dx_2 \\
% &\quad + \int f_{1,2}(x_1, x_2)^2 \, dx_1 dx_2 \\
% &\quad + \text{(all cross-terms vanish due to orthogonality)} \\
% &= f_0^2 + \int f_1(x_1)^2 \, dx_1 + \int f_2(x_2)^2 \, dx_2 + \int f_{1,2}(x_1, x_2)^2 \, dx_1 dx_2
% \end{align*}

% \vspace{1em}

% After rearranging terms, we find that

% \[
% \int f(x_1, x_2)^2 \, dx_1 dx_2 - f_0^2 = \int f_1(x_1)^2 \, dx_1 + \int f_2(x_2)^2 \, dx_2 + \int f_{1,2}(x_1, x_2)^2 \, dx_1 dx_2
% \]

% which is equivalent to 

\subsubsection*{fANOVA as projection}
Referring to the general connection between the expected value and orthogonal projections presented in section~\ref{general_definitions}, the fANOVA terms can also be understood from a viewpoint of projections. This will also help to understand the generalization of fANOVA in section~\ref{generalization}.\par

$f_0$ is the projections of the original function $f$ onto the space of all constant functions $\mathcal{G}_0 = \{g(x) = a; a \in \mathbb{R}\}$. It is an unconditional expected value and the best approximation of $f$ given a constant function:
\begin{align*}
    \Pi_{\mathcal{G}_0}f
    &= \arg \min_{g \in \mathcal{G}_0} \|f(x) - g\|^2 \\
    &= \arg \min_{g \in \mathcal{G}_0} \mathbb{E}[\|f(x) - g\|^2] \\
    &= \mathbb{E}[f(X)]
\end{align*}
The main effect $f_i(x_i)$ is the projection of $f$ onto the subspace of all functions that only depend on $x_i$ and have an expected value of zero while accounting for the lower-order effects. The subspace we project onto is $\mathcal{G}_i = \{g(x) = g_i(x_i); \int g(x) d\nu (x_i) = 0\}$.
\begin{align*}
    \Pi_{\mathcal{G}_i}f - f_0
    &= \arg \min_{g \in \mathcal{G}_i} \|f(x) - g(x_i)\|^2 - f_0\\
    &= \arg \min_{g \in \mathcal{G}_i} \mathbb{E}_{-x_i}[\|f(x) - g(x_i)\|^2] - \mathbb{E}[f(x)] \\
    &= \mathbb{E}_{-x_i}[f(X_1, \dots, x_i, \dots, X_n)] - \mathbb{E}[f(X)]
\end{align*}

The two-way interaction effect $f_{ij}(x_i,x_j)$ is the projection of $f$ onto the subspace of all functions that depend on $x_i$ and $x_j$ and have an expected value of zero in each of it's single components, i.e. $\mathcal{G}_{i,j} = \{g(x) = g_{ij}(x_i, x_j); \int g(x) d\nu (x_i) = 0 \land \int g(x) d\nu (x_j) = 0\}$. Again, we account for lower-order effects by subtracting the constant term and all main effects:
\begin{align*}
    \Pi_{\mathcal{G}_{ij}}f - f_0 - f_1(x_i) - \dots
    &= \arg \min_{g \in \mathcal{G}_{ij}} \|f(x) - g(x_i, x_j)\|^2 - f_0 - f_1(x_i) - \dots \\
    &= \arg \min_{g \in \mathcal{G}_{ij}} \mathbb{E}_{-x_i, -x_j}[\|f(x) - g(x_i, x_j)\|^2] - \mathbb{E}[f(x)] - \mathbb{E}_{-x_i}[f(x)]\\
    &= \mathbb{E}_{-x_i, -x_j}[f(X_1, \dots, x_i, x_j, \dots, X_n)] - \mathbb{E}[f(x)] - \mathbb{E}_{-x_i}[f(X)]
\end{align*}
{\color{blue}I think Hilbert space theorem tells us that the orthogonal projection minimizes the squared difference in a Hilbert space?
So the projection is the solution to the minimization problem that wants to minimize the squared differences between two elements of the vector space.
This would be the first equality. The last equality that the solution is equal to the (conditional) expected value also has to be shown, still have to look which theorem this is proven by.}

In general, general we can write:
\begin{equation}
    f_u(x) = \Pi_{\mathcal{G}_u}f - \sum_{v \subsetneq u} f_v(x)
\end{equation}
We project $f$ onto the subspace spanned by the own terms of the fANOVA component to be defined, while accounting for all lower-order terms.

% where

% \[
% D_1 = \int f_1(x_1)^2 \, dx_1, \quad
% D_2 = \int f_2(x_2)^2 \, dx_2, \quad
% D_{1,2} = \int f_{1,2}(x_1, x_2)^2 \, dx_1 dx_2
% \]

% \subsection*{Alternative Formulation of fANOVA}
% Based on \cite{hooker2004} and Owen Lecture notes (find paper to cite).
% We again work with a square integrable function $f(x): [0, 1]^n \rightarrow \mathbb{R}$ with $x = (x_1, \dots, x_n)$ and $x_1, \dots, x_n$ are independent.
% The set of indices $1, \dots, n$ is denoted as $1\colon d$. $u \subset 1\colon d$ and $-u$ is the complement of $u$, i.e. $1\colon d \backslash u$.
% Given a set of indices $u = {i_1, i_2, \dots, i_{|u|}}$ we can write $x_u$ for $(x_{i_1}, x_{i_2}, \dots, x_{i_{|u|}}) = (x_i)_{i \in u}$.\par

% This notation allows us to sum over a set of indices, which avoids lengthy notation. The fANOVA can then be formulated as follows:
% \begin{equation}
%     f_{\emptyset}(\mathbf{x}) = \int_{[0, 1]^n} f(\mathbf{x}) d\nu(\mathbf{x}) = \mu
% \end{equation}

% Generally, the fANOVA decomposition can be written as
% \begin{equation}
%     f_u(\mathbf{x}) = \int_{[0,1]^{d - |u|}} \left( f(\mathbf{x}) - \sum_{v \subsetneq u} f_v(\mathbf{x}) \right) \, d\nu(\mathbf{x}_{-u}).
% \end{equation}
% We can rewrite this as follows, which simplifies the calculation of the integral:
% \begin{equation}
%     f_u(\mathbf{x}) = \int_{[0,1]^{d - |u|}} f(\mathbf{x}) \, d\nu(\mathbf{x}_{-u}) - \sum_{v \subsetneq u} f_v(\mathbf{x}).
% \end{equation}

% In Lemma A.3. Owen states a general formulation of orthogonality of two functions, which is applicable for the fANOVA components and ensures their orthogonality.
% Given to square-integrable, real-valued functions on the unit hypercube $f(x)$ and $g(x)$ with $u,v \subseteq {1, \dots, d}$, it is true that, if $u \neq v$, then
% \[
% \int_{[0,1]^n} f_u(\mathbf{x}) g_v(\mathbf{x}) \, d\nu(\mathbf{x}) = 0
% \]







