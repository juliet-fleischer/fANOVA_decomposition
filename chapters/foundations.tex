% Good historical overview of fANOVA decomposition found in: Owen (2003), Takemura (1983)

\subsection{Early Work on fANOVA}
The main idea of the fANOVA decomposition is to decompose a statistical model into the sum of the main effects and interaction effects of its input variables. The underlying principle of fANOVA decomposition dates back to \cite{hoeffding1948}. In his famous paper he introduced U-statistics, along with the ``Hoeffding decomposition'', which allows to write a symmetric function of the data as a sum of orthogonal components. \cite{sobol1993sensitivity} used the same principle and applied it to deterministic mathematical models.
% Both methods involve the sum of orthogonal components and independent input variables.
He built on the originally called ``decomposition into summands of different dimension'' in \cite{sobol2001}, where he introduces Sobol indices and renames the method to the ``ANOVA-representation''. Sobol indices are now commonly used in sensitivity analysis. \cite{efron1981} use the idea of the decomposition to proof their famous lemma on jackknife variances. \cite{stone1994} mainly uses fANOVA decomposition to base smooth regression models with interactions on it and his paper is the building block for a broader body of work of fANOVA-based models {\color{blue}example citations needed}.


\subsection{Modern Work on fANOVA}
The fANOVA decomposition has a long history with roots in mathematical statistics and non-parametric estimation theory. In more recent years, the method has been rediscovered by the machine-learning community, especially in the context of interpretable machine learning (IML) and explainable AI (XAI). \cite{hooker2004} introduces the fANOVA decomposition with the goal of providing a global explanation method for black-box models. Since the assumptions of independent variables in classical fANOVA is often too restrictive in practice, \cite{hooker2007} generalizes the method to dependent variables. A recent paper by \cite{ilidrissi2025} can be seen as another approach to generalize the principle of fANOVA decomposition to dependent inputs.\par
There are specific domains of statistics, such as geostatistics, that explicitly build models on fANOVA framework (see \cite{muehlenstaedt2012} for fANOVA Kriging models). And recent work discovered interesting mathematical parallels between fANOVA and other IML methods, such as PDP \cite{friedman2001}, or Shapley values (\cite{fumagalli2025}, Herren, Owen preprint).

\cite{liu2006} use of fANOVA and sensitivity analysis for functions arising in computational finance.
\cite{owen2013} formal intro to fANOVA decomposition and generalization of Sobol indices.
Owen has generally a lot of work related to fANOVA decomposition, either lecture notes explaining the decomposition, methods based on it \cite{owen2003}, or deeper into sensitivity analysis and fANOVA \cite{owen2013}.

\textbf{fANOVA and U-statistics, fANOVA and sensitivity analysis, fANOVA and GAMs (with interactions)}

\subsection{Formal Introduction to fANOVA}
This chapter is based on the formal introductions by \cite{rahman2014, sobol1993sensitivity, sobol2001, hooker2004, owen2013, muehlenstaedt2012}. We show both formulations of the fANOVA, via the integral and via the expected value and in general prefer the expected value formulation as it is more intuitive in a probabilistic setting.
Originally, \cite{sobol1993sensitivity} presented the fANOVA decomposition with independent input variables with support bounded to the unit interval, i.e. he considered the measure space $([0, 1]^n, \mathcal{B}([0, 1]^n), \nu)$. Later work shows that this restriction is not necessary, and we can work with the Borel $\sigma$-algebra on the n-dimensional real number line, i.e. $(\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n), \nu)$, and with a general measure $\nu$ defined on it (see e.g. \cite{rahman2014}).
Since we assume independence of the input variables, their joint distribution is given by the product over the marginal distributions, i.e. \(f_{\boldsymbol{X}}(\boldsymbol{x}) \, d\nu(\boldsymbol{x}) = \prod_{i=1}^{N} f_{X_i}(x_i) \, d\nu(x_i)\). \(f_{X_i}: \mathbb{R} \rightarrow \mathbb{R}_{0}^{+}\) is the marginal probability density function of \(X_i\) defined on $(\Omega_i, \mathcal{F}_i, \nu_i)$.


% Let $i_1, \dots , i_s$ denote a set of indices. For now, we assume that $X_i \overset{\text{i.i.d.}}{\sim} \mathcal{U}[0, 1]$ and work in the measure space $(\Omega, \mathcal{F}, \nu) = ([0, 1]^n, \mathcal{B}([0, 1]^n), \lambda_{n})$. $\mathcal{B}([0, 1]^n)$ is the Borel $\sigma$-algebra on the n-dimensional unit interval and $\lambda_{n}$ is the n-dimensional Lebesgue measure. 
% The general inner product and norm we defined earlier simplify under these assumptions.\par
% The inner product under uniform distribution assumption is
% \[
% \langle f, g \rangle = \int f(x) g(x) \, d(x), \quad \forall f, g \in \mathcal{L}^2.
% \]
% The norm under uniform distribution assumption is
% \[
% \|f\| = \sqrt{\langle f, f \rangle} = \sqrt{\int f^2(x) \, d(x)}, \quad \forall f \in \mathcal{L}^2.
% \]

\begin{definition}
Let $y(\boldsymbol{X})$  be a mathematical model with realizations of independent random variables $x_1, \dots, x_N$ as input. We can represent such a model $y$ as the hierarchical sum of specific basis functions with increasing dimensionality:
\begin{equation}
    y(\boldsymbol{X}) = \sum_{u \subseteq \{1, \dots, N\}} y_{u}(\boldsymbol{X}_u),
    \label{eq:fanova_decomposition}
\end{equation}
\end{definition}

If $|u| = 0$ it describes the constant term, if $|u| = 1$ it describes the main effects, if $|u| > 1$ it describes the interaction effects of the variables in $u$. The expansion consists of $2^N$ terms.\par

% To ensure identifiability and interpretation, we set what \cite{rahman2014} calls the strong annihilating conditions and which will results in two crucial properties of the fANOVA terms.
% \begin{proposition}
%     The strong annihilating conditions require that each fANOVA component integrates to zero with respect to the marginal probability density function of each variable it depends on. In other words, the inner product of each fANOVA component and the marginal density of its own variables is zero. The constant term is the only exception to this rule. We write:
%     \begin{equation}
%         \int y_{u}(\boldsymbol{x}_u) f_{X_i}(x_i) \, d\nu(x_i) = 0, \quad \text{for} \ i \in u \neq \emptyset.
%         \label{eq:strong_annihilating_conditions}
%     \end{equation}
% \end{proposition}

The fANOVA terms should be constructed in such a way that they have two specific properties crucial for identifiability and interpretation.

\begin{proposition}
    The zero-mean property states that all effects, except for the constant terms, are centred around zero.
Mathematically this means that the effects integrate to zero w.r.t. their own variables:
\begin{equation}
    \int y_u(\boldsymbol{x_u}) f_{\boldsymbol{X}_u}(\boldsymbol{x}_u) \, d\nu (\boldsymbol{x}_u) = \mathbb{E}[y_u(\boldsymbol{X}_u)] = 0
    \label{eq:zero_mean_c}
\end{equation}
\end{proposition}
\begin{proposition}
    The second property is the orthogonality of the fANOVA terms. If two sets of indices are not completely equivalent, i.e. $\emptyset \neq u \subseteq \{1, \dots, N\}, \emptyset \neq v \subseteq \{1, \dots, N\}, \text{ and } u \neq v$, then it holds that their fANOVA terms are orthogonal to each other:
\begin{equation}
    \int y_u(\boldsymbol{x}_u) y_v(\boldsymbol{x}_v) f_{\boldsymbol{X}}(\boldsymbol{x}) d\nu (\boldsymbol{x}) = \mathbb{E}[y_u(\boldsymbol{X}_u) y_v(\boldsymbol{X}_v)] = 0
    \label{eq:orthogonality_c}
\end{equation}
\end{proposition}
This means that fANOVA terms are ``fully orthogonal'' to each other, meaning not only terms of different order are orthogonal to each other but also terms of the same order are.\par

\cite{rahman2014} derives these two properties (\autoref{eq:zero_mean_c}, \autoref{eq:orthogonality_c})from a more general condition, he calls the ``strong annihilating conditions''.\par
\textbf{The strong annihilating conditions} require that the fANOVA terms integrate to zero w.r.t the individual variables contained in $u$ and weighted by the individual marginal probability density functions:
\begin{equation}
    \int y_u(\boldsymbol{x}_u) f_{X_i}(x_i) \, d\nu(x_i) = 0, \quad \text{for} \ i \in u \neq \emptyset.
    \label{eq:strong_annihilating_conditions}
\end{equation}

We can reassure ourselves that the properties in fact follow from the strong annihilating conditions. For the zero-mean constraint we can write:
\begin{align*}
    \mathbb{E}[y_u(\boldsymbol{X}_u)] &= \int_{\mathbb{R}^{|u|}} y_u(\boldsymbol{x_u}) f_{\boldsymbol{X}_u}(\boldsymbol{x}_u) \, d\nu (\boldsymbol{x}_u) \\
    &= \int_{\mathbb{R}^{|u|}} y_u(\boldsymbol{x_u}) \prod_{i \in u} f_{X_i}(x_i) \, d\nu (\boldsymbol{x}_u) \\
    &= \int_{\mathbb{R}^{|u|-1}} \int_{\mathbb{R}} y_u(\boldsymbol{x_u}) f_{X_i}(x_i) \, dx_u \prod_{j \in u, j \neq i} f_{X_j}(x_j) = 0
\end{align*}
One can follow the same reasoning for the orthogonality condition:
\begin{align*}
    \mathbb{E}[y_u(\boldsymbol{X}_u) y_v(\boldsymbol{X}_v)] &= \int_{\mathbb{R}^{\mathbb{N}}} y_u(\boldsymbol{x_u}) y_v(\boldsymbol{x_v}) f_{\boldsymbol{X}}(\boldsymbol{x}) \, d\nu (\boldsymbol{x}) \\
    &= \int_{\mathbb{R}^{\mathbb{N}}} y_u(\boldsymbol{x_u}) y_v(\boldsymbol{x_v}) \prod_{i=1}^{N} f_{X_i}(x_i) \, d\nu (x_i) \\
    &= \int_{\mathbb{R}^{\mathbb{N}-1}} \int_{\mathbb{R}} y_u(\boldsymbol{x_u}) y_v(\boldsymbol{x_v}) f_{X_i}(x_i) \, dx_u \prod_{j \in \{1, \dots, N\}, j \neq i} f_{X_j}(x_j) = 0
\end{align*}


\subsubsection*{Construction of fANOVA terms}
The individual fANOVA term for the variables with indices in $u$ are constructed from integrating the original function $y(\boldsymbol{X})$ w.r.t all variables expect for the ones in $u$, and subtracting the lower order terms. Intuitively the integral is averaging the original function over all other variables expect the ones of interest, which makes sense as we are then left with a function of the variables of interest only. Subtracting lower order terms corresponds to accounting for effects that are already explained by other variables or interactions so that we obtain the isolated effects.\par
Since $u = \emptyset$ for the constant term, we integrate w.r.t all variables:
\begin{equation}
    y_{\emptyset} = \int y(\boldsymbol{x}) \prod_{i=1}^{N} f_{X_i}(x_i) \, d\nu (x_i) = \mathbb{E}[y(\boldsymbol{X})].
    \label{eq:intercept}
\end{equation}
For all other effects $u \neq \emptyset$ we can write:
\begin{equation}
    y_u(\boldsymbol{X}_u) = \int y(\boldsymbol{X}_u, \boldsymbol{x}_u) \prod_{i=1, i \notin u}^{N} f_{X_i}(x_i) \, d\nu (x_i)- \sum_{v \subsetneq u} y_v(\boldsymbol{X}_v),
    \label{eq:fanova_component}
\end{equation}

The basis components offer a clear interpretation of the model, decomposing it into main effects, two-way interaction effects, and so on. This is why fANOVA decomposition has received increasing attention in the IML and XAI literature, holding the potential for a global explanation method of black box models.\par

% \subsubsection*{Example 2}
% We now look at the function $g_{2} = a + x_1 + x_{2}^2$ which includes a quadratic effect. We again assume $X_1 \perp X_2$. The constant fANOVA term is given by:
% \begin{align*}
%     &f_0 = \mathbb{E}[g_{2}(X_1, X_2)] = \mathbb{E}[a + X_1 + X_{2}^2] = a + \mathbb{E}[X_1] + \mathbb{E}[X_{2}^2] = a + \frac{1}{12} \\
%     &\text{This works because we are still in the setting, in which we assume } X_i \overset{\text{i.i.d.}}{\sim} \mathcal{U}[0, 1], \\
%     &\text{in combination with the zero-mean constraint this allows us to state:} \\
%     &\mathbb{E}[X_2^2] = \mathbb{V}[X_2] = \frac{1}{12}(1 - 0)^2 = \frac{1}{12} \\
%     &\text{Next, we write the main effects:} \\
%     &f_1(x_1) = \mathbb{E}_{X_2}[g_{2}(x_1, X_2)] - f_0 = \mathbb{E}_{X_2}[a + x_1 + X_{2}^2] - f_0 \\
%     &= a + x_1 + \mathbb{E}[X_{2}^2] - f_0 = a + x_1 + \frac{1}{12} - \left( a + \frac{1}{12} \right) = x_1 \\
%     &f_2(x_2) = \mathbb{E}_{X_1}[g_{2}(X_1, x_2)] - f_0 = \mathbb{E}_{X_1}[a + X_1 + x_{2}^2] - f_0 \\
%     &= a + \mathbb{E}[X_1] + x_{2}^2 - f_0 = a + x_{2}^2 - \left( a + \frac{1}{12} \right) = x_{2}^2 - \frac{1}{12} \\
%     &\text{Finally, we compute the interaction effect:} \\
%     &f_{12}(x_1, x_2) = \mathbb{E}[g_{2}(x_1, x_2)] - f_0 - f_1(x_1) - f_2(x_2) \\
%     &= a + x_1 + x_{2}^2 - \left( a + \frac{1}{12} \right) - x_1 - \left( x_{2}^2 - \frac{1}{12} \right) = 0
% \end{align*}
% What we observe in this example is that explicit centering of the quadratic effect by subtracting a constant is necessary.
% To write the expected value of a product as the product of the expected values we needed the independence assumption. To state that the product of the expected values is equal to zero, we used the zero-mean constraint. This shows that the independence assumption and zero-mean constraint are critical to ensure orthogonality in this traditional formulation of the fANOVA decomposition. This is of course also true for terms of different order, e.g. \( f_{1,2}(x_1, x_2) \) and \( f_{1}(x_1) \). Orthogonality ensures that the effects do not overlap and each term represents the isolated contribution.

\subsubsection*{Second-moment statistics}
We already established that $\mathbb{E}[y(\boldsymbol{X})] = y_{\emptyset}$.
For the variance of $y(\boldsymbol{X})$, we find that the total variance can be decomposed into the sum of the fANOVA term variances. The variance decomposition is a major result in \cite{sobol1993sensitivity} and forms the basis for the Sobol indices in sensitivity analysis. We sketch the variance decomposition here and note that it is only possible under independence assumption.\par
If $y \in \mathcal{L}^2$, then $y_{i_{1}, \dots, y_{n}} \in \mathcal{L}^2$ {\color{blue}proof? reference?; Sobol 1993 says it is easy to show using Schwarz inequality and the definition of the single fANOVA terms.}
Therefore, we define the variance of $f$ as follows:
\begin{align*}
    \sigma^2 &:= \int y^2(\boldsymbol{X}) f_{\boldsymbol{X}}(\boldsymbol{x}) d\nu (\boldsymbol{x}) - (y_0)^2 \\
    &= \int y^2(\boldsymbol{X}) f_{\boldsymbol{X}}(\boldsymbol{x}) d\nu (\boldsymbol{x}) - (\int y(\boldsymbol{X}) f_{\boldsymbol{X}}(\boldsymbol{x}) d\nu (\boldsymbol{x}))^2 \\
    &= \mathbb{E}[y^2(\boldsymbol{X})] - \mathbb{E}[y(\boldsymbol{X})]^2
    \label{variance_whole}
\end{align*}
The variance of the fANOVA components is then defined as
\begin{align*}
    \sigma^2_{x_{i_1}, \dots, x_{i_n})}
    &= \int \cdots \int y^2_{i_{1}, \dots, i_{n}} \, f_{\boldsymbol{X}}(\boldsymbol{x}) d\nu(x_1) \cdots d\nu(x_n) - \left( \int \cdots \int f_{i_{1}, \dots, i_{n}} \, f_{\boldsymbol{X}}(\boldsymbol{x}) d\nu(x_1) \cdots d\nu(x_n) \right)^2\\
    &= \mathbb{E}[y^2_{i_{1}, \dots, i_{n}}] - \mathbb{E}[y_{i_{1}, \dots, i_{n}}]^2
\end{align*}
Because of the orthogonality property, the second term vanished and we get:
\begin{align*}
    \sigma^2_{x_{i_1}, \dots, x_{i_n})}
    &= \int \cdots \int y^2_{i_{1}, \dots, i_{n}} \, f_{\boldsymbol{X}}(\boldsymbol{x}) d\nu(x_1) \cdots d\nu(x_n)\\
    &= \mathbb{E}[y^2_{i_{1}, \dots, i_{n}}]
\end{align*}

With the definition of the total variance $\sigma^2$ and the component-wise variance $\sigma^2_{x_{i_1}, \dots, x_{i_n}}$ we can now see that the total variance can be decomposed into the sum of the component-wise variances.

% Second variant of variance decomposition formulated via the expected value
% I could first show this and then show in a Lemma that if the expected value of y^2(X) exists (is smaller than \infinity) then the expected value of y_u^2(X_u) also exists, this would essentially be the translation from Sobols L^2 statement to the expected value wouldn't it?
Alternatively we can formulate this via the expected value. We write the sum over $u$ for the sum over $\emptyset \neq u \subseteq \{1, \dots, N\}$ and the sum over $u \neq v$ for the sum over $\emptyset \neq u \subseteq \{1, \dots, N\}, \emptyset \neq v \subseteq \{1, \dots, N\}, u \neq v$.
\begin{align*}
    \sigma^2 := \mathbb{E}[(y(\boldsymbol{X}) - \mu])^2]
    &= \mathbb{E}[(y_{\emptyset} + \sum_{u} y_u({\boldsymbol{X}_u}) - y_{\emptyset})^2] \\
    &= \mathbb{E}[(\sum_{u} y_u({\boldsymbol{X}_u}))^2] \\
    &= \mathbb{E}[\sum_{u} y_u^2({\boldsymbol{X}_u})] + 2 \mathbb{E}[\sum_{u \neq v} y_u({\boldsymbol{X}_u})  y_v({\boldsymbol{X}_v})] \\
    & = \sum_{u} \mathbb{E}[y_u^2({\boldsymbol{X}_u})]
\end{align*}




\subsubsection*{fANOVA as projection}
In the following we revisit the fANOVA decomposition from the view of orthogonal projections. The section is based on \cite{Vaart_1998}.
This will also help to understand the generalization of fANOVA in section~\ref{generalization}.\par

Situation: $y(\boldsymbol{X}) \in \Omega, \mathcal{G} \subseteq \Omega, g(\boldsymbol{X}) \in \mathcal{G}$.\par

When we define the constant term $y_\emptyset$ our goal is to best approximate the original function $y$ by a constant function. In other words, we want to minimize the squared difference between $y$ and a constant function $g(x) = a$ over all possible constant functions. The solution is the orthogonal projection of $y$ onto the linear subspace of all constant functions $\mathcal{G}_0 = \{g(x) = a; a \in \mathbb{R}\}$. In a probabilistic context, we want to minimize the expected squared different between the random variables $y(\boldsymbol{X})$ and $a$, which turns out to be equivalent to the expected value of the random variable. So intuitively, in the absence of any additional information, the expected value is our best guess. More formally we can write:
\begin{align*}
    \Pi_{\mathcal{G}_0}y
    &= \arg \min_{g \in \mathcal{G}_0} \|y - g\|^2 \\ % here we still focus on the functions (function space view)
    &= \arg \min_{g \in \mathcal{G}_0} \mathbb{E}[(y(\boldsymbol{X}) - a)^2] \\ % here we switch to the probabilistic view, focus on RV
    &= \mathbb{E}[y(\boldsymbol{X})]
\end{align*}
The main effect $f_i(x_i)$ is the projection of $y$ onto the subspace of all functions that only depend on $x_i$ and have an expected value of zero while accounting for the lower-order effects. The subspace we project onto is $\mathcal{G}_i = \{g(x) = g_i(x_i); \int g(x) d\nu (x_i) = 0\}$.
\begin{align*}
    \Pi_{\mathcal{G}_i}y - y_0
    &= \arg \min_{g \in \mathcal{G}_i} \|y(x) - g(x_i)\|^2 - f_0\\
    &= \arg \min_{g \in \mathcal{G}_i} \mathbb{E}_{-x_i}[(f(x) - g(x_i))^2] - \mathbb{E}[y(x)] \\
    &= \mathbb{E}_{-x_i}[y(X_1, \dots, x_i, \dots, X_n)] - \mathbb{E}[y(X)]
\end{align*}

The two-way interaction effect $y_{ij}(x_i,x_j)$ is the projection of $y$ onto the subspace of all functions that depend on $x_i$ and $x_j$ and have an expected value of zero in each of it's single components, i.e. $\mathcal{G}_{i,j} = \{g(x) = g_{ij}(x_i, x_j); \int g(x) d\nu (x_i) = 0 \land \int g(x) d\nu (x_j) = 0\}$. Again, we account for lower-order effects by subtracting the constant term and all main effects:
\begin{align*}
    \Pi_{\mathcal{G}_{ij}}y - y_0 - f_1(x_i) - \dots
    &= \arg \min_{g \in \mathcal{G}_{ij}} \|y(x) - g(x_i, x_j)\|^2 - f_0 - f_1(x_i) - \dots \\
    &= \arg \min_{g \in \mathcal{G}_{ij}} \mathbb{E}_{-x_i, -x_j}[(y(x) - g(x_i, x_j))^2] - \mathbb{E}[y(x)] - \mathbb{E}_{-x_i}[y(x)]\\
    &= \mathbb{E}_{-x_i, -x_j}[y(X_1, \dots, x_i, x_j, \dots, X_n)] - \mathbb{E}[y(x)] - \mathbb{E}_{-x_i}[y(X)]
\end{align*}
{\color{blue}I think Hilbert space theorem tells us that the orthogonal projection minimizes the squared difference in a Hilbert space?
So the projection is the solution to the minimization problem that wants to minimize the squared differences between two elements of the vector space.
This would be the first equality. The last equality that the solution is equal to the (conditional) expected value also has to be shown, still have to look which theorem this is proven by.}

In general, we can write:
\begin{equation}
    y_u(x) = \Pi_{\mathcal{G}_u}y - \sum_{v \subsetneq u} f_v(x)
\end{equation}
We project $y$ onto the subspace spanned by the own terms of the fANOVA component to be defined, while accounting for all lower-order terms.







