\begin{frame}{Formal Setting}
  \begin{itemize}
    \item measure space, probability measure, random vector, subvector, complementary vector, pdf
    \item measure space of square integrable functions
    \item inner product
    \item norm
  \end{itemize}
  
\end{frame}

\begin{frame}{Decompose model into interpretable components} % What is it?
\begin{block}{General Form}
\[
  \begin{aligned}
    y(\boldsymbol{X}) 
    &= \sum_{u \subseteq \{1,\dots,N\}} y_{u}(\boldsymbol{X}_u) \\[3pt]
    &= y_{\emptyset} 
       + \big( y_{\{1\}}(\boldsymbol{X}_1) + \dots + y_{\{N\}}(\boldsymbol{X}_N) \big) \\[2pt]
    &\quad + \big( y_{\{1, 2\}}(\boldsymbol{X}_1,\boldsymbol{X}_2) 
                 + y_{\{1, 3\}}(\boldsymbol{X}_1,\boldsymbol{X}_3) + \dots \big) \\[2pt]
    &\quad + \big( y_{\{1, 2, 3\}}(\boldsymbol{X}_1,\boldsymbol{X}_2,\boldsymbol{X}_3) + \dots \big) 
       + \dots + y_{\{1, \dots, N\}}(\boldsymbol{X}_1, \dots, \boldsymbol{X}_N)
  \end{aligned}
\]
\end{block}

  \begin{itemize}
    \item $y$ : Model
    \item $y_u$ : Component functions for subset $u$
    \item Assumption: $X_1, \dots, X_N$ are independent
  \end{itemize}
\end{frame}

\begin{frame}{Ensure Uniqueness and Interpretability of the Components} % What conditions do we need?
  \begin{block}{Strong Annihilating Conditions}
    \[
      \int_{\mathbb{R}} y_u(\boldsymbol{x}_u) f_{\{i\}}(x_i) \, d\nu(x_i) = 0 \quad \text{for} \ i \in u \neq \emptyset.
    \]
  \end{block}
  \begin{itemize}
    \item Ensures unique component functions
    \item Applies under independent (product-type) input distributions
  \end{itemize}
    \[
    \mathbb{E}[y_u(\boldsymbol{X}_u)] = 0
  \]
  \[
    \mathbb{E}[y_u(\boldsymbol{X}_u) y_v(\boldsymbol{X}_v)] = 0 \quad (u \neq v)
  \]
\end{frame}


\begin{frame}{Recursive Form of the Components} % How do we construct it?
    \[
    y_{\emptyset} = \int_{\mathbb{R}^N} y(\boldsymbol{x}) \prod_{i=1}^{N} f_{\{i\}}(x_i) \, d\nu (x_i) = \mathbb{E}[y(\boldsymbol{X})].
    \]
\begin{equation}
    y_u(\boldsymbol{X}_u) 
    = \int_{\mathbb{R}^{N- |u|}} 
        y(\boldsymbol{X}_u, \boldsymbol{x}_{-u}) 
        \prod_{i=1, i \notin u}^{N} f_{\{i\}}(x_i) 
        \, d\nu (x_i) 
      - \sum_{v \subsetneq u} y_v(\boldsymbol{X}_v).
    \label{eq:fanova_components_classical}
\end{equation}
  \begin{itemize}
    \item $f_{-u}$ : marginal density of variables not in $u$
    \item Components solved sequentially by increasing order
  \end{itemize}
\end{frame}

\begin{frame}{Recursive Form Example}
  \begin{itemize}
    \item $N = 3$
  \end{itemize}
    \[
    y_{\emptyset} = \int_{\mathbb{R}^3} y(\boldsymbol{x}) \prod_{i=1}^{3} f_{\{i\}}(x_i) \, d\nu (x_i) = \mathbb{E}[y(\boldsymbol{X})].
    \]
  \begin{itemize}
    \item $u = \{1\}$ $\rightarrow$ $v = \emptyset$
  \end{itemize}
    \begin{equation*}
    y_{\{1\}}(X_1) 
    = \int_{\mathbb{R}^{2}} 
        y(X_{1}, x_{2}, x_{3}) 
        \prod_{i=2}^{3} f_{\{i\}}(x_i) 
        \, d\nu (x_i) 
      -  y_{\emptyset} = \mathbb{E}[y(X_1, X_2, X_3)|X_1 = x_1] - y_{\emptyset}.
\end{equation*}
  \begin{itemize}
    \item $u = \{1,2\}$ $\rightarrow$ $v = \{1\}, \{2\}, \emptyset$
  \end{itemize}
    \begin{align*}
    y_{\{1,2\}}(X_1, X_2) 
    &= \int_{\mathbb{R}} 
        y(X_1, X_2, x_3) 
        f_{\{3\}}(x_3) 
        \, d\nu (x_3) 
      -  y_{\{1\}}(X_1) -  y_{\{2\}}(X_2) - y_{\emptyset} \\
    &= \mathbb{E}[y(X_1, X_2, X_3)|X_1 = x_1, X_2 = x_2] - y_{\{1\}}(X_1) - y_{\{2\}}(X_2) - y_{\emptyset}.
\end{align*}
  
\end{frame}


\begin{frame}{Example: Independent MVN} % Example of a 2D function
  \begin{align*}
    y(x_1, x_2) = 2x_1 + x_2^{2} + x_1 x_2
  \end{align*}
\[
(X_1, X_2)^\mathsf{T} \sim \mathcal{N}\!\left(
\begin{pmatrix}0 \\ 0\end{pmatrix},
\begin{pmatrix}
1 & \rho \\ 
\rho & 1
\end{pmatrix}
\right),
\]
\[
X_1 \mid X_2=x_2 \sim \mathcal{N}(0, 1), \quad
X_2 \mid X_1=x_1 \sim \mathcal{N}(0, 1).
\]
Components:
\begin{equation*}
    y_{\emptyset} = 1, \qquad
    y_{\{1\}}(x_1) = 2x_1, \qquad
    y_{\{2\}}(x_2) = x_2^{2} - 1, \qquad
    y_{\{1,2\}}(x_1, x_2) = x_1 x_2.
\end{equation*}
\end{frame}


\begin{frame}{Example: 2D Function} % How does it look?
  \begin{columns}
    \column{0.5\textwidth}
      \includegraphics[width=\linewidth]{../images/experiment_section/classical_ex_1_a1p20_a2p00_a11p00_a22p10_a12p10_rhop00_main.png}
    \column{0.5\textwidth}
      \includegraphics[width=\linewidth]{../images/experiment_section/classical_ex_1_a1p20_a2p00_a11p00_a22p10_a12p10_rhop00_interaction.png}
  \end{columns}
\end{frame}

% fANOVA through lens of Orthogonal Projections
\begin{frame}{Reminder: Definition of Orthogonal Projection} % How do we define it?
  \begin{align*}
    \Pi_{\mathcal{G}}y = \arg\min_{g \in \mathcal{G}} \|y - g\|^2
= \arg\min_{g \in \mathcal{G}} \mathbb{E}[(y(\boldsymbol{X}) - g(\boldsymbol{X}))^2].
\end{align*}
  \begin{itemize}
    \item $\mathcal{G}$ : linear subspace of $\mathcal{L}^2$ we project onto
    \item $g$ all functions in the subspace
  \end{itemize}
\end{frame}

\begin{frame}{fANOVA as Orthogonal Projection} % How does it relate to orthogonal projection?
\begin{align*}
    y_{\emptyset}
    &= \mathbb{E}[y(\boldsymbol{X})] \\ 
    &= \arg \min_{a \in \mathbb{R}} \mathbb{E}[(y(\boldsymbol{X}) - a)^2] \\ 
    &= \arg \min_{g_0 \in \mathcal{G}_0} \|y - g_0\|^2
    = \Pi_{\mathcal{G}_0}y,
\end{align*}
\begin{align*}
    y_u(.) 
    &= \mathbb{E}[y(\boldsymbol{X}) \mid X_{u} = .] - \sum_{v \subsetneq u} y_v(.) \\ 
    &= \arg \min_{g_u \in \mathcal{G}_u} \mathbb{E}[(y(\boldsymbol{X}) - g_u(.))^2] - \sum_{v \subsetneq u} y_v(.) \\ 
    &= (\Pi_{\mathcal{G}_u}y)(.) - \sum_{v \subsetneq u} y_v(.)
\end{align*}

  
\end{frame}


\begin{frame}{Equality to Hoeffding Decomposition} % How does it relate to Hoeffding?
  \begin{block}{Hoeffding Decomposition}
    \begin{align}
    y(\boldsymbol{X})
=
\sum_{A \subseteq D} 
y_A(\boldsymbol{X}_A),
\qquad
D := \{1,\dots,N\},
\end{align}
where, for each $A \subseteq D$, the component function $y_A$ is defined by:
\begin{align}\label{eq:hoeffding_components}
    y_A(\boldsymbol{X}_A)
=
\sum_{B \subseteq A}
(-1)^{|A|-|B|}
\,\mathbb{E}\!\left[
  y(\boldsymbol{X}) 
  \,\middle|\, 
  \boldsymbol{X}_B
\right],
\end{align}
  where $y_u$ are orthogonal components.
  \end{block}
  \begin{itemize}
    \item Classical fANOVA and Hoeffding decomposition yield same components under zero-centered inputs
    \item Both assume independence of input variables
  \end{itemize}
  
\end{frame}

\begin{frame}{Hoeffding Decomposition Example} % Example of Hoeffding decomposition
  \begin{align*}
    y(x_1, x_2) = 2x_1 + x_2^{2} + x_1 x_2
  \end{align*}
  \begin{align*}
    % --- constant component
    y_{\emptyset}
    &= \mathbb{E}[y(X_1,X_2)] 
     = 2\,\mathbb{E}[X_1] + \mathbb{E}[X_2^2] + \mathbb{E}[X_1 X_2] 
     = 1,
    \\[1.2em]
    % --- first-order component for X1
    y_{\{1\}}(x_1)
    &= \sum_{B \subseteq \{1\}} (-1)^{1-|B|}
       \,\mathbb{E}[y(\boldsymbol{X})\,|\,X_B] \\[2pt]
    &= -\,\mathbb{E}[y] + \mathbb{E}[y|X_1=x_1] \\[2pt]
    &= -1 + (2x_1 + \mathbb{E}[X_2^2] + x_1\mathbb{E}[X_2]) \\[2pt]
    &= 2x_1,
    \\[1.2em]
    % --- first-order component for X2
    y_{\{2\}}(x_2)
    &= \sum_{B \subseteq \{2\}} (-1)^{1-|B|}
       \,\mathbb{E}[y(\boldsymbol{X})\,|\,X_B] \\[2pt]
    &= -\,\mathbb{E}[y] + \mathbb{E}[y|X_2=x_2] \\[2pt]
    &= -1 + (2\mathbb{E}[X_1] + x_2^2 + x_2\mathbb{E}[X_1]) \\[2pt]
    &= x_2^2 - 1.
\end{align*}
\end{frame}

\begin{frame}
  \begin{align*}
    % --- second-order interaction component
    y_{\{1,2\}}(x_1,x_2)
    &= \sum_{B \subseteq \{1,2\}} (-1)^{2-|B|}
       \,\mathbb{E}[y(\boldsymbol{X})\,|\,X_B] \\[2pt]
    &= (+1)\,\mathbb{E}[y] 
     - \mathbb{E}[y|X_1=x_1]
     - \mathbb{E}[y|X_2=x_2]
     + y(x_1,x_2) \\[2pt]
    &= 1 - (2x_1+1) - (x_2^2) + (2x_1+x_2^2+x_1x_2) \\[2pt]
    &= x_1 x_2.
\end{align*}

\[
y(x_1,x_2)
= y_{\emptyset} + y_{\{1\}}(x_1) + y_{\{2\}}(x_2) + y_{\{1,2\}}(x_1,x_2)
= 1 + 2x_1 + (x_2^2 - 1) + x_1 x_2
\]

\end{frame}


