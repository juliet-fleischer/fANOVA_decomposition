% Show important proofs
% show the foundational s√§tze they use

\begin{frame}{Reminder: Definition of Orthogonal Projection} % How do we define it?
  \begin{align*}
    \Pi_{\mathcal{G}}y = \arg\min_{g \in \mathcal{G}} \|y - g\|^2
= \arg\min_{g \in \mathcal{G}} \mathbb{E}[(y(\boldsymbol{X}) - g(\boldsymbol{X}))^2]
\end{align*}
  \begin{itemize}
    \item $\mathcal{G}$ : linear subspace of $\mathcal{L}^2$ we project onto
    \item $g$ all functions in the subspace
  \end{itemize}
\end{frame}

\begin{frame}{fANOVA as Orthogonal Projection} % How does it relate to orthogonal projection?
\begin{align*}
    y_{\emptyset}
    &= \mathbb{E}[y(\boldsymbol{X})] \\ 
    &= \arg \min_{a \in \mathbb{R}} \mathbb{E}[(y(\boldsymbol{X}) - a)^2] \\ 
    &= \arg \min_{g_0 \in \mathcal{G}_0} \|y - g_0\|^2
    = \Pi_{\mathcal{G}_0}y
\end{align*}
\begin{align*}
    y_u(.) 
    &= \mathbb{E}[y(\boldsymbol{X}) \mid X_{u} = .] - \sum_{v \subsetneq u} y_v(.) \\ 
    &= \arg \min_{g_u \in \mathcal{G}_u} \mathbb{E}[(y(\boldsymbol{X}) - g_u(.))^2] - \sum_{v \subsetneq u} y_v(.) \\ 
    &= (\Pi_{\mathcal{G}_u}y)(.) - \sum_{v \subsetneq u} y_v(.)
\end{align*}
\end{frame}

\begin{frame}{Equality to Hoeffding Decomposition} % How does it relate to Hoeffding?
  \begin{block}{Hoeffding Decomposition}
    \begin{align}
    y(\boldsymbol{X})
=
\sum_{A \subseteq D} 
y_A(\boldsymbol{X}_A),
\qquad
D := \{1,\dots,N\},
\end{align}
where, for each $A \subseteq D$, the component function $y_A$ is defined by:
\begin{align}\label{eq:hoeffding_components}
    y_A(\boldsymbol{X}_A)
=
\sum_{B \subseteq A}
(-1)^{|A|-|B|}
\,\mathbb{E}\!\left[
  y(\boldsymbol{X}) 
  \,\middle|\, 
  \boldsymbol{X}_B
\right],
\end{align}
  where $y_u$ are orthogonal components.
  \end{block}
  \begin{itemize}
    \item Classical fANOVA and Hoeffding decomposition yield same components under zero-centered inputs
    \item Both assume independence of input variables
  \end{itemize}
  
\end{frame}

\begin{frame}{Hoeffding Decomposition Example} % Example of Hoeffding decomposition
  \begin{align*}
    y(x_1, x_2) = 2x_1 + x_2^{2} + x_1 x_2
  \end{align*}
  \begin{align*}
    % --- constant component
    y_{\emptyset}
    &= \mathbb{E}[y(X_1,X_2)] 
     = 2\,\mathbb{E}[X_1] + \mathbb{E}[X_2^2] + \mathbb{E}[X_1 X_2] 
     = 1,
    \\[1.2em]
    % --- first-order component for X1
    y_{\{1\}}(x_1)
    &= \sum_{B \subseteq \{1\}} (-1)^{1-|B|}
       \,\mathbb{E}[y(\boldsymbol{X})\,|\,X_B] = -\,\mathbb{E}[y] + \mathbb{E}[y|X_1=x_1] \\[2pt]
    &= -1 + (2x_1 + \mathbb{E}[X_2^2] + x_1\mathbb{E}[X_2]) = 2x_1,
    \\[1.2em]
    % --- first-order component for X2
    y_{\{2\}}(x_2)
    &= \sum_{B \subseteq \{2\}} (-1)^{1-|B|}
       \,\mathbb{E}[y(\boldsymbol{X})\,|\,X_B] -\,\mathbb{E}[y] + \mathbb{E}[y|X_2=x_2] \\[2pt]
    &= -1 + (2\mathbb{E}[X_1] + x_2^2 + x_2\mathbb{E}[X_1]) = x_2^2 - 1.
\end{align*}
\end{frame}

\begin{frame}
  \begin{align*}
    % --- second-order interaction component
    y_{\{1,2\}}(x_1,x_2)
    &= \sum_{B \subseteq \{1,2\}} (-1)^{2-|B|}
       \,\mathbb{E}[y(\boldsymbol{X})\,|\,X_B] \\[2pt]
    &= (+1)\,\mathbb{E}[y] 
     - \mathbb{E}[y|X_1=x_1]
     - \mathbb{E}[y|X_2=x_2]
     + y(x_1,x_2) \\[2pt]
    &= 1 - (2x_1+1) - (x_2^2) + (2x_1+x_2^2+x_1x_2) \\[2pt]
    &= x_1 x_2.
\end{align*}

\[
y(x_1,x_2)
= y_{\emptyset} + y_{\{1\}}(x_1) + y_{\{2\}}(x_2) + y_{\{1,2\}}(x_1,x_2)
= 1 + 2x_1 + (x_2^2 - 1) + x_1 x_2
\]

\end{frame}



\begin{frame}
  Substituting the basis functions:
  \begin{align*}
   y(x_1,x_2)  = \underbrace{c_0}_{y_0}
   &+ \underbrace{\big(c_{1,1}\,x_1 
                     + c_{1,2}\,(x_1^2 - 1)\big)}_{y_1(x_1)} \\[0.5em]
   &\quad
   + \underbrace{\big(c_{2,1}\,x_2 
                     + c_{2,2}\,(x_2^2 - 1)\big)}_{y_2(x_2)} \\[0.5em]
   &\quad
   + \underbrace{c_{12,11}\,\left(\frac{\rho (x_1^2 + x_2^2)}{1 + \rho^2} 
                         - x_1 x_2 
                         + \frac{\rho(\rho^2 - 1)}{1 + \rho^2}\right)}_{y_{12}(x_1,x_2)}.
\end{align*}
Find weights to recover original polynomial while fulfilling zero-mean and hierarchical orthogonality:
\[
y(x_1,x_2) = a_0 + a_1 x_1 + a_2 x_2 
   + a_{11} x_1^2 + a_{22} x_2^2 + a_{12} x_1 x_2 
\]

\end{frame}

\begin{frame}{Coefficient Matching}
  The corresponding weights can be found via coefficient matching. Start from the interaction term:
      \begin{align*}
-\,c_{12, 11} &= a_{12} &\Rightarrow\quad c_{12, 11} &= -a_{12} \\[0.5em]
c_{1,2} + c_{12, 11}\,\tfrac{\rho}{1+\rho^2} &= a_{11} 
&\Rightarrow\quad c_{1,2} &= a_{11} + \tfrac{\rho}{1+\rho^2}a_{12} \\[0.5em]
c_{2,2} + c_{12, 11}\,\tfrac{\rho}{1+\rho^2} &= a_{22} 
&\Rightarrow\quad c_{2,2} &= a_{22} + \tfrac{\rho}{1+\rho^2}a_{12} \\[0.5em]
c_{1,1} &= a_1 \\[0.5em]
c_{2,1} &= a_2 \\[0.5em]
c_0 - c_{1,2} - c_{2,2} + c_{12, 11}\,\tfrac{\rho(\rho^2 - 1)}{1+\rho^2} &= a_0 
&\Rightarrow\quad 
c_0 &= a_0 + a_{11} + a_{22} + \rho\,a_{12}
\end{align*}
\end{frame}

\begin{frame}{Running Example from Thesis under Independence}
\begin{equation}
  h(x_1, x_2) = x_1 + 2 x_2 + x_1 x_2 \qquad \rho = 0
\end{equation}
    \begin{columns}
    \column{0.5\textwidth}
      \includegraphics[width=\linewidth]{../images/experiment_section/running_example_a1p10_a2p20_a11p00_a22p00_a12p10_rhop00_main.png}
    \column{0.5\textwidth}
      \includegraphics[width=\linewidth]{../images/experiment_section/running_example_a1p10_a2p20_a11p00_a22p00_a12p10_rhop00_interaction.png}
  \end{columns}
\end{frame}

\begin{frame}{Running Example from Thesis under Dependence}
\begin{equation}
  h(x_1, x_2) = x_1 + 2 x_2 + x_1 x_2 \qquad \rho = 0.5
\end{equation}
    \begin{columns}
    \column{0.5\textwidth}
      \includegraphics[width=\linewidth]{../images/experiment_section/running_example_a1p10_a2p20_a11p00_a22p00_a12p10_rhop05_main.png}
    \column{0.5\textwidth}
      \includegraphics[width=\linewidth]{../images/experiment_section/running_example_a1p10_a2p20_a11p00_a22p00_a12p10_rhop05_interaction.png}
  \end{columns}
\end{frame}

\begin{frame}{Example: Only Linear Terms}
  \begin{columns}
    \column{0.5\textwidth}
      \includegraphics[width=\linewidth]{../images/experiment_section/linear_a1p15_a2p35_a11p00_a22p00_a12p00_rhop00_main.png}
      \captionof{figure}{$q(x_1, x_2) = 1.5 x_1 + 3.5 x_2$}
    \column{0.5\textwidth}
      \includegraphics[width=\linewidth]{../images/experiment_section/linear_a1m20_a2p40_a11p00_a22p00_a12p00_rhop00_main.png}
      \captionof{figure}{$q(x_1, x_2) = -2 x_1 + 4 x_2$}
  \end{columns}
\end{frame}


\begin{frame}{Example: Only Main Terms under Independence}
  \[
  y(x_1, x_2) = -2 x_1 - 2x_2 + x_1^2 + x_2^2 \qquad \rho = 0
  \]
    \begin{columns}
    \column{0.5\textwidth}
      \includegraphics[width=\linewidth]{../images/experiment_section/mixed_a1m20_a2p20_a11p10_a22m10_a12p00_rhop00_main.png}
    \column{0.5\textwidth}
      \includegraphics[width=\linewidth]{../images/experiment_section/mixed_a1m20_a2p20_a11p10_a22m10_a12p00_rhop00_interaction.png}
  \end{columns}
\end{frame}


\begin{frame}{Example: Only Interaction Term under Dependence}
  \[
  y(x_1, x_2) = x_1 x_2 \qquad \rho = -0.5
  \]
    \begin{columns}
    \column{0.5\textwidth}
      \includegraphics[width=\linewidth]{../images/experiment_section/interaction_a1p00_a2p00_a11p00_a22p00_a12p20_rhom05_main.png}
    \column{0.5\textwidth}
      \includegraphics[width=\linewidth]{../images/experiment_section/interaction_a1p00_a2p00_a11p00_a22p00_a12p20_rhom05_interaction.png}
  \end{columns}
  
\end{frame}

\begin{frame}{Example: Interaction under Independence}
  \[
  y(x_1, x_2) = x_1 x_2 \qquad \rho = 0
  \]
    \begin{columns}
    \column{0.5\textwidth}
      \includegraphics[width=\linewidth]{../images/experiment_section/interaction_a1p00_a2p00_a11p00_a22p00_a12p20_rhop00_main.png}
    \column{0.5\textwidth}
      \includegraphics[width=\linewidth]{../images/experiment_section/interaction_a1p00_a2p00_a11p00_a22p00_a12p20_rhop00_interaction.png}
  \end{columns}
  
\end{frame}



\begin{frame}{Proof of Zero-Mean Property for Classical Components}
  Strong annihilating conditions hold, so:
  \begin{align*}
    \mathbb{E}[y_u(\boldsymbol{X}_u)] &:= \int_{\mathbb{R}^{N}} y_u(\boldsymbol{x_u}) f_{\boldsymbol{X}}(\boldsymbol{x}) \, d\nu (\boldsymbol{x}) \\[0.5em]
    &= \int_{\mathbb{R}^{|u|}} y_u(\boldsymbol{x_u}) f_{\boldsymbol{u}}(\boldsymbol{x}_u) \, d\nu (\boldsymbol{x}_u) \\[0.5em]
    &= \int_{\mathbb{R}^{|u|}} y_u(\boldsymbol{x_u}) \prod_{j \in u} f_{\{j\}}(x_j) \, d\nu (\boldsymbol{x}_u) \\[0.5em]
    &= \int_{\mathbb{R}^{|u|-1}} \int_{\mathbb{R}} y_u(\boldsymbol{x_u}) f_{\{i\}}(x_i) \, d\nu(x_i) \prod_{j \in u, j \neq i} f_{\{j\}}(x_j) \, d\nu (x_{u \setminus \{i\}}) = 0.
\end{align*}

\end{frame}

\begin{frame}{Proof of Orthogonality for Classical Components}
  \begin{itemize}
    \item $u \neq v$, so pick $i \in u \setminus v$
    \item $y_v(\boldsymbol{x_v})$ is independent of $x_i$
    \item strong annihilating conditions hold by assumption
  \end{itemize}
\[
    \int_{\mathbb{R}} y_u(\boldsymbol{x}_u) f_{\{i\}}(x_i)\,d\nu(x_i) = 0
    \quad \text{for all fixed } \boldsymbol{x}_{u\setminus \{i\}}.
\]
Hence,
\begin{align*}
    \mathbb{E}[y_u(\boldsymbol{X}_u) y_v(\boldsymbol{X}_v)] &= \int_{\mathbb{R}^{N}} y_u(\boldsymbol{x_u}) y_v(\boldsymbol{x_v}) f_{\boldsymbol{X}}(\boldsymbol{x}) \, d\nu (\boldsymbol{x}) \\
    &= \int_{\mathbb{R}^{N}} y_u(\boldsymbol{x}_u) y_v(\boldsymbol{x}_v)
       \prod_{j=1}^N f_{\{j\}}(x_j)\, d\nu(\boldsymbol{x}) \\
    &= \int_{\mathbb{R}^{N-1}}
        \left(\int_{\mathbb{R}} y_u(\boldsymbol{x}_u) f_{\{i\}}(x_i)\,d\nu(x_i)\right)
        y_v(\boldsymbol{x}_v)\prod_{j \neq i} f_{\{j\}}(x_j)\,d\nu(\boldsymbol{x}_{-i}) = 0.
\end{align*}
\end{frame}

\begin{frame}{Proof of Zero-Mean Property for Generalized Components}
  We assume the weak annihilating conditions hold, then:
  \begin{align*}
\mathbb{E}[y_{u,G}(\boldsymbol{X}_u)] 
&:= \int_{\mathbb{R}^N} y_{u,G}(\boldsymbol{x}_u) f_{\boldsymbol{X}}(\boldsymbol{x})\, d \nu (\boldsymbol{x}) \\[0.5em]
&= \int_{\mathbb{R}^{|u|}} y_{u,G}(\boldsymbol{x}_u) \left( \int_{\mathbb{R}^{N - |u|}} f_{\boldsymbol{X}}(\boldsymbol{x}) \, d \nu(\boldsymbol{x}_{-u}) \right) d \nu(\boldsymbol{x}_u) \\[0.5em]
&= \int_{\mathbb{R}^{|u|}} y_{u,G}(\boldsymbol{x}_u) f_u(\boldsymbol{x}_u)\, d \nu(\boldsymbol{x}_u) \\[0.5em]
&= \int_{\mathbb{R}^{|u| - 1}} \left( \int_{\mathbb{R}} y_{u,G}(\boldsymbol{x}_u) f_u(\boldsymbol{x}_u) \, d \nu(x_i) \right) \prod_{j \in u,\, j \ne i} d \nu(\boldsymbol{x}_j) \\[0.5em]
&= 0.
\end{align*}
  
\end{frame}

\begin{frame}{Proof of Hierarchical Orthogonality}
For any two subsets $\emptyset \ne u \subseteq \{1,\dots,N\}$ and $\emptyset \ne v \subseteq \{1,\dots,N\}$, where $v \subsetneq u$, the subset $u = v \cup (u \setminus v)$. Let $i \in (u \setminus v) \subseteq u$. Then we obtain:
\begin{align*}
\mathbb{E}[y_{u,G}(\boldsymbol{X}_u) \, y_{v,G}(\boldsymbol{X}_v)]
&:= \int_{\mathbb{R}^N} y_{u,G}(\boldsymbol{x}_u) y_{v,G}(\boldsymbol{x}_v) f_{\boldsymbol{X}}(\boldsymbol{x}) \, d \nu(\boldsymbol{x}) \\[0.5em]
&= \int_{\mathbb{R}^{|u|}} y_{u,G}(\boldsymbol{x}_u) y_{v,G}(\boldsymbol{x}_v) \left( \int_{\mathbb{R}^{N - |u|}} f_{\boldsymbol{X}}(\boldsymbol{x}) \, d \nu(\boldsymbol{x}_{-u}) \right) d \nu(\boldsymbol{x}_u) \\[0.5em]
&= \int_{\mathbb{R}^{|u|}} y_{u,G}(\boldsymbol{x}_u) y_{v,G}(\boldsymbol{x}_v) f_u(\boldsymbol{x}_u) \, d \nu(\boldsymbol{x}_u) \\[0.5em]
&= \int_{\mathbb{R}^{|v|}} y_{v,G}(\boldsymbol{x}_v)
    \int_{\mathbb{R}^{|u \setminus v|}} y_{u,G}(\boldsymbol{x}_u) f_u(\boldsymbol{x}_u) \, d \nu(\boldsymbol{x}_{u \setminus v}) \, d \nu(\boldsymbol{x}_v) \\[0.5em]
&= \int_{\mathbb{R}^{|v|}} y_{v,G}(\boldsymbol{x}_v)
    \int_{\mathbb{R}^{|u \setminus v| - 1}} \left( \int_{\mathbb{R}} y_{u,G}(\boldsymbol{x}_u) f_u(\boldsymbol{x}_u) \, d \nu(\boldsymbol{x}_i) \right) \\[0.5em]
&\times \prod_{\substack{j \in (u \setminus v) \\ j \ne i}} d \nu(x_j) \, d \nu(\boldsymbol{x}_v) = 0.
\end{align*}
\end{frame}

\begin{frame}{Sobol' Indices}
\[
S_u = \frac{\operatorname{Var}\!\left( \mathbb{E}\left[ y(\boldsymbol{X}) \,\middle|\, \mathbf{X}_u = . \right] \right)}{\operatorname{Var}(y(\boldsymbol{X}))},
\]
where
\begin{itemize}
  \item $y(\boldsymbol{X})$ is the probabilistic model, which is decomposed
  \item $\mathbb{E}[y(\boldsymbol{X}) \mid \mathbf{X}_u = .]$ is the fANOVA component $y_u$
\end{itemize}
\end{frame}


% \begin{frame}{Generalized fANOVA Proofs}
%     \begin{itemize}
%         \item Three integration cases: distinguish between different relationships u and v, depending on the relationship the integral w.r.t. to marginal density simplifies
%         \item Generalized fANOVA components by Rahman: first build constant term; for nonconstant terms use integration cases
%         \item Integration constraint Hooker: show that hierarchical orthogonality is fulfilled if the conditions hold, show that it is not fulfilled if they do not hold; but why exactly these conditions a bit unclear
%         \item Take a look at Sobols proof again
%     \end{itemize}
% \end{frame}

\begin{frame}{External Links}
    \begin{itemize}
        \item \url{https://docs.google.com/spreadsheets/d/1K5ECL6hDPDnHwM_k342xa29H-vHWzdk27PTgDHUwfFE/edit?usp=sharing} - Table with fANOVA-related literature
    \end{itemize}
\end{frame}