
@article{hooker_generalized_2007,
	title = {Generalized {Functional} {ANOVA} {Diagnostics} for {High}-{Dimensional} {Functions} of {Dependent} {Variables}},
	volume = {16},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/abs/10.1198/106186007X237892},
	doi = {10.1198/106186007X237892},
	language = {en},
	number = {3},
	urldate = {2025-03-11},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Hooker, Giles},
	month = sep,
	year = {2007},
	keywords = {\#1 fANOVA ML models},
	pages = {709--732},
	file = {Hooker - 2007 - Generalized Functional ANOVA Diagnostics for High-Dimensional Functions of Dependent Variables.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/hooker2007/Hooker - 2007 - Generalized Functional ANOVA Diagnostics for High-Dimensional Functions of Dependent Variables.pdf:application/pdf},
}

@article{tang_predicting_2024,
	title = {Predicting systemic financial risk with interpretable machine learning},
	volume = {71},
	issn = {10629408},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1062940824000123},
	doi = {10.1016/j.najef.2024.102088},
	abstract = {Predicting systemic financial risk is essential for understanding the financial system’s stability and early warning of financial crises. In this research, we use the financial stress index to measure systemic financial risk. We construct the stress index for five financial submarkets and composite stress index, employ the Markov regime switching model to identify the systemic financial risk stress state. On this basis, we use interpretable machine learning models to forecast systemic financial risk, analyze and compare the results of the intrinsic interpretable machine learning models and the post-hoc explainable methods. The results indicate that systemic financial risk can be effectively predicted using both the submarket stress index and the feature variables, with the submarket stress index as the independent variable providing relatively higher accuracy. There is a linearly positive relationship between the stress index of each submarket and systemic financial risk, with financial stress in the stock and money markets having the greatest impact on systemic financial risk. For each feature variable, stock–bond correlation coefficient, stock valuation risk, the maximum cumulative loss of the SSE Composite Index (SSE CMAX), and loan-deposit ratio have strong predictive power. Our research can provide reference for government to construct prediction model and indicator monitoring platform of systemic financial crisis.},
	language = {en},
	urldate = {2025-04-14},
	journal = {The North American Journal of Economics and Finance},
	author = {Tang, Pan and Tang, Tiantian and Lu, Chennuo},
	month = mar,
	year = {2024},
	pages = {102088},
	file = {Tang et al. - 2024 - Predicting systemic financial risk with interpretable machine learning.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Tang et al. - 2024 - Predicting systemic financial risk with interpretable machine learning.pdf:application/pdf},
}

@article{hu_interpretable_2025,
	title = {Interpretable {Machine} {Learning} {Based} on {Functional} {ANOVA} {Framework}: {Algorithms} and {Comparisons}},
	volume = {41},
	issn = {1524-1904, 1526-4025},
	shorttitle = {Interpretable {Machine} {Learning} {Based} on {Functional} {ANOVA} {Framework}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/asmb.2916},
	doi = {10.1002/asmb.2916},
	abstract = {In the early days of machine learning (ML), the emphasis was on developing complex algorithms to achieve best possible predictive performance. To understand and explain the model results, one had to rely on post hoc explainability techniques, which are known to have limitations. Recently, with the recognition in regulated industries that interpretability is also important, researchers are studying algorithms that compromise on small increases in predictive performance in favor of being more interpretable. While doing so, the ML community has rediscovered the use of low-order functional ANOVA (fANOVA) models that have been known in the statistical literature for some time. This paper starts with a description of challenges with post hoc explainability. This is followed by a brief review of the fANOVA framework with a focus on models with just main effects and second-order interactions (called generalized additive models with interactions or GAMI = GAM + Interactions). It then provides an overview of two recently developed GAMI techniques: Explainable Boosting Machines or EBM and GAMI-Net. The paper proposes a new algorithm that also uses trees, as in EBM, but does linear fits instead of piecewise constants within the partitions. We refer to this as GAMI-linear-tree (GAMI-Lin-T). There are many other differences, including the development of a new interaction filtering algorithm. The paper uses simulated and real datasets to compare the three fANOVA ML algorithms. The results show that GAMI-Lin-T and GAMI-Net have comparable performances, and both are generally better than EBM.},
	language = {en},
	number = {1},
	urldate = {2025-04-14},
	journal = {Applied Stochastic Models in Business and Industry},
	author = {Hu, Linwei and Nair, Vijayan N. and Sudjianto, Agus and Zhang, Aijun and Chen, Jie and Yang, Zebin},
	month = jan,
	year = {2025},
	keywords = {\#1 fANOVA ML models},
	pages = {e2916},
	file = {Hu et al. - 2025 - Interpretable Machine Learning Based on Functional ANOVA Framework Algorithms and Comparisons.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Hu et al. - 2025 - Interpretable Machine Learning Based on Functional ANOVA Framework Algorithms and Comparisons.pdf:application/pdf},
}

@article{hess_interpretable_nodate,
	title = {Interpretable {Machine} {Learning} for {Earnings} {Forecasts}: {Leveraging} {High}-{Dimensional} {Financial} {Statement} {Data}},
	abstract = {We predict earnings for forecast horizons of up to five years by using the entire set of Compustat financial statement data as input and providing it to state-of-the-art machine learning models capable of approximating arbitrary functional forms. Our approach improves prediction one year ahead by an average of 11\% compared to the traditional linear approach that performs best. This superior performance is consistent across a variety of evaluation metrics as well as different firm subsamples and translates into more profitable investment strategies. Extensive model interpretation reveals that income statement variables, especially different definitions of earnings, are by far the most important predictors. Conversely, we find that while income statement variables decline in relevance, balance sheet information becomes more significant as the forecast horizon extends. Lastly, we show that the influence of interactions and nonlinearities on the machine learning forecast is modest, but substantial differences between firm subsamples exist.},
	language = {en},
	author = {Hess, Dieter and Simon, Frederik and Weibels, Sebastian},
	file = {Hess et al. - Interpretable Machine Learning for Earnings Forecasts Leveraging High-Dimensional Financial Stateme.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Hess et al. - Interpretable Machine Learning for Earnings Forecasts Leveraging High-Dimensional Financial Stateme.pdf:application/pdf},
}

@article{tang_forecasting_2025,
	title = {Forecasting {Bank} {Default} {Risk} with {Interpretable} {Machine} {Learning}: {The} {Study} of {Chinese} {Banks}},
	volume = {61},
	issn = {1540-496X, 1558-0938},
	shorttitle = {Forecasting {Bank} {Default} {Risk} with {Interpretable} {Machine} {Learning}},
	url = {https://www.tandfonline.com/doi/full/10.1080/1540496X.2024.2415337},
	doi = {10.1080/1540496X.2024.2415337},
	abstract = {Bank occupies an important position in the financial system. The stable operation of the banking industry is not only one of the important factors in achieving sustainable economic growth but also related to the stability of the entire financial system. This research collects data from 507 banks in China from 2000 to 2021, uses the non-performing loan ratio as the measurement indicator of bank risk, and selects indicators from five levels (macroeconomic environment, industry economic environment, economic policy uncertainty, financial openness and bank financial status) On this basis, we use interpretable machine learning models to predict the bank’s default risk, analyze and compare the interpretable machine learning model and the post-hoc explainable methods. The results indicate that Provision Coverage (PC), Loan Provision Coverage (LPC), Liquidity Ratio (LR), and KOF Financial Globalization Index (KOFFiGI) have strong predictive capability for bank default risk. Our research can provide a reference for banks, government and financial regulatory authorities to construct the prediction model and indicator monitoring platform for bank default risk.},
	language = {en},
	number = {6},
	urldate = {2025-04-14},
	journal = {Emerging Markets Finance and Trade},
	author = {Tang, Pan and Peng, Hongjuan and Luo, Sihang and Liu, Yangguang},
	month = may,
	year = {2025},
	pages = {1661--1683},
	file = {Tang et al. - 2025 - Forecasting Bank Default Risk with Interpretable Machine Learning The Study of Chinese Banks.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Tang et al. - 2025 - Forecasting Bank Default Risk with Interpretable Machine Learning The Study of Chinese Banks.pdf:application/pdf},
}

@article{molnar_model-agnostic_nodate,
	title = {Model-{Agnostic} {Interpretable} {Machine} {Learning}},
	language = {en},
	author = {Molnar, Christoph},
	file = {Molnar - Model-Agnostic Interpretable Machine Learning.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Molnar - Model-Agnostic Interpretable Machine Learning.pdf:application/pdf},
}

@article{buckmann_interpretable_2022,
	title = {An {Interpretable} {Machine} {Learning} {Workflow} with {An} {Application} to {Economic} {Forecasting}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=4130517},
	doi = {10.2139/ssrn.4130517},
	language = {en},
	urldate = {2025-04-14},
	journal = {SSRN Electronic Journal},
	author = {Buckmann, Marcus and Joseph, Andreas},
	year = {2022},
	file = {Buckmann und Joseph - 2022 - An Interpretable Machine Learning Workflow with An Application to Economic Forecasting.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Buckmann und Joseph - 2022 - An Interpretable Machine Learning Workflow with An Application to Economic Forecasting.pdf:application/pdf},
}

@article{walters_how_2023,
	title = {How to {Open} a {Black} {Box} {Classifier} for {Tabular} {Data}},
	volume = {16},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1999-4893},
	url = {https://www.mdpi.com/1999-4893/16/4/181},
	doi = {10.3390/a16040181},
	abstract = {A lack of transparency in machine learning models can limit their application. We show that analysis of variance (ANOVA) methods extract interpretable predictive models from them. This is possible because ANOVA decompositions represent multivariate functions as sums of functions of fewer variables. Retaining the terms in the ANOVA summation involving functions of only one or two variables provides an efﬁcient method to open black box classiﬁers. The proposed method builds generalised additive models (GAMs) by application of L1 regularised logistic regression to the component terms retained from the ANOVA decomposition of the logit function. The resulting GAMs are derived using two alternative measures, Dirac and Lebesgue. Both measures produce functions that are smooth and consistent. The term partial responses in structured models (PRiSM) describes the family of models that are derived from black box classiﬁers by application of ANOVA decompositions. We demonstrate their interpretability and performance for the multilayer perceptron, support vector machines and gradient-boosting machines applied to synthetic data and several real-world data sets, namely Pima Diabetes, German Credit Card, and Statlog Shuttle from the UCI repository. The GAMs are shown to be compliant with the basic principles of a formal framework for interpretability.},
	language = {en},
	number = {4},
	urldate = {2025-04-14},
	journal = {Algorithms},
	author = {Walters, Bradley and Ortega-Martorell, Sandra and Olier, Ivan and Lisboa, Paulo J. G.},
	month = mar,
	year = {2023},
	pages = {181},
	file = {Walters et al. - 2023 - How to Open a Black Box Classifier for Tabular Data.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Walters et al. - 2023 - How to Open a Black Box Classifier for Tabular Data.pdf:application/pdf},
}

@article{belle_principles_2021,
	title = {Principles and {Practice} of {Explainable} {Machine} {Learning}},
	volume = {4},
	issn = {2624-909X},
	url = {https://www.frontiersin.org/articles/10.3389/fdata.2021.688969/full},
	doi = {10.3389/fdata.2021.688969},
	abstract = {Artiﬁcial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and ﬁnance. However, such a highly positive impact is coupled with a signiﬁcant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus speciﬁcally on data-driven methods—machine learning (ML) and pattern recognition models in particular—so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-speciﬁc biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the ﬁeld of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-speciﬁc or model-agnostic post-hoc explainability approaches. We also brieﬂy reﬂect on deep learning models, and conclude with a discussion about future research directions.},
	language = {en},
	urldate = {2025-04-14},
	journal = {Frontiers in Big Data},
	author = {Belle, Vaishak and Papantonis, Ioannis},
	month = jul,
	year = {2021},
	keywords = {XAI},
	pages = {688969},
	file = {Belle und Papantonis - 2021 - Principles and Practice of Explainable Machine Learning.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Belle und Papantonis - 2021 - Principles and Practice of Explainable Machine Learning.pdf:application/pdf},
}

@article{weber_applications_2024,
	title = {Applications of {Explainable} {Artificial} {Intelligence} in {Finance}—a systematic review of {Finance}, {Information} {Systems}, and {Computer} {Science} literature},
	volume = {74},
	issn = {2198-1620, 2198-1639},
	url = {https://link.springer.com/10.1007/s11301-023-00320-0},
	doi = {10.1007/s11301-023-00320-0},
	abstract = {Digitalization and technologization affect numerous domains, promising advantages but also entailing risks. Hence, when decision-makers in highly-regulated domains like Finance implement these technological advances—especially Artificial Intelligence—regulators prescribe high levels of transparency, assuring the traceability of decisions for third parties. Explainable Artificial Intelligence (XAI) is of tremendous importance in this context. We provide an overview of current research on XAI in Finance with a systematic literature review screening 2,022 articles from leading Finance, Information Systems, and Computer Science outlets. We identify a set of 60 relevant articles, classify them according to the used XAI methods and goals that they aim to achieve, and provide an overview of XAI methods used in different Finance areas. Areas like risk management, portfolio optimization, and applications around the stock market are well-researched, while anti-money laundering is understudied. Researchers implement both transparent models and post-hoc explainability, while they recently favored the latter.},
	language = {en},
	number = {2},
	urldate = {2025-04-14},
	journal = {Management Review Quarterly},
	author = {Weber, Patrick and Carl, K. Valerie and Hinz, Oliver},
	month = jun,
	year = {2024},
	pages = {867--907},
	file = {Weber et al. - 2024 - Applications of Explainable Artificial Intelligence in Finance—a systematic review of Finance, Infor.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Weber et al. - 2024 - Applications of Explainable Artificial Intelligence in Finance—a systematic review of Finance, Infor.pdf:application/pdf},
}

@inproceedings{lou_accurate_2013,
	address = {Chicago Illinois USA},
	title = {Accurate intelligible models with pairwise interactions},
	isbn = {978-1-4503-2174-7},
	url = {https://dl.acm.org/doi/10.1145/2487575.2487579},
	doi = {10.1145/2487575.2487579},
	abstract = {Standard generalized additive models (GAMs) usually model the dependent variable as a sum of univariate models. Although previous studies have shown that standard GAMs can be interpreted by users, their accuracy is signiﬁcantly less than more complex models that permit interactions.},
	language = {en},
	urldate = {2025-04-14},
	booktitle = {Proceedings of the 19th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {ACM},
	author = {Lou, Yin and Caruana, Rich and Gehrke, Johannes and Hooker, Giles},
	month = aug,
	year = {2013},
	pages = {623--631},
	file = {Lou et al. - 2013 - Accurate intelligible models with pairwise interactions.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Lou et al. - 2013 - Accurate intelligible models with pairwise interactions.pdf:application/pdf},
}

@inproceedings{lengerich_purifying_2020,
	title = {Purifying {Interaction} {Effects} with the {Functional} {ANOVA}: {An} {Efficient} {Algorithm} for {Recovering} {Identifiable} {Additive} {Models}},
	shorttitle = {Purifying {Interaction} {Effects} with the {Functional} {ANOVA}},
	url = {https://proceedings.mlr.press/v108/lengerich20a.html},
	abstract = {Models which estimate main effects of individual variables alongside interaction effects have an identifiability challenge: effects can be freely moved between main effects and interaction effects without changing the model prediction. This is a critical problem for interpretability because it permits “contradictory" models to represent the same function. To solve this problem, we propose pure interaction effects: variance in the outcome which cannot be represented by any subset of features. This definition has an equivalence with the Functional ANOVA decomposition. To compute this decomposition, we present a fast, exact algorithm that transforms any piecewise-constant function (such as a tree-based model) into a purified, canonical representation. We apply this algorithm to Generalized Additive Models with interactions trained on several datasets and show large disparity, including contradictions, between the apparent and the purified effects. These results underscore the need to specify data distributions and ensure identifiability before interpreting model parameters.},
	language = {en},
	urldate = {2025-04-14},
	booktitle = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Lengerich, Benjamin and Tan, Sarah and Chang, Chun-Hao and Hooker, Giles and Caruana, Rich},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {2402--2412},
	file = {Lengerich et al. - 2020 - Purifying Interaction Effects with the Functional ANOVA An Efficient Algorithm for Recovering Ident.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Lengerich et al. - 2020 - Purifying Interaction Effects with the Functional ANOVA An Efficient Algorithm for Recovering Ident.pdf:application/pdf},
}

@inproceedings{hooker_discovering_2004,
	address = {Seattle WA USA},
	title = {Discovering additive structure in black box functions},
	isbn = {978-1-58113-888-7},
	url = {https://dl.acm.org/doi/10.1145/1014052.1014122},
	doi = {10.1145/1014052.1014122},
	abstract = {Many automated learning procedures lack interpretability, operating eﬀectively as a black box: providing a prediction tool but no explanation of the underlying dynamics that drive it. A common approach to interpretation is to plot the dependence of a learned function on one or two predictors. We present a method that seeks not to display the behavior of a function, but to evaluate the importance of nonadditive interactions within any set of variables. Should the function be close to a sum of low dimensional components, these components can be viewed and even modeled parametrically. Alternatively, the work here provides an indication of where intrinsically high-dimensional behavior takes place. The calculations used in this paper correspond closely with the functional ANOVA decomposition; a well-developed construction in Statistics. In particular, the proposed score of interaction importance measures the loss associated with the projection of the prediction function onto a space of additive models. The algorithm runs in linear time and we present displays of the output as a graphical model of the function for interpretation purposes.},
	language = {en},
	urldate = {2025-04-14},
	booktitle = {Proceedings of the tenth {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {ACM},
	author = {Hooker, Giles},
	month = aug,
	year = {2004},
	pages = {575--580},
	file = {Hooker - 2004 - Discovering additive structure in black box functions.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Hooker - 2004 - Discovering additive structure in black box functions.pdf:application/pdf},
}

@misc{tsang_interpretable_2021,
	title = {Interpretable {Artificial} {Intelligence} through the {Lens} of {Feature} {Interaction}},
	url = {http://arxiv.org/abs/2103.03103},
	doi = {10.48550/arXiv.2103.03103},
	abstract = {Interpretation of deep learning models is a very challenging problem because of their large number of parameters, complex connections between nodes, and unintelligible feature representations. Despite this, many view interpretability as a key solution to trustworthiness, fairness, and safety, especially as deep learning is applied to more critical decision tasks like credit approval, job screening, and recidivism prediction. There is an abundance of good research providing interpretability to deep learning models; however, many of the commonly used methods do not consider a phenomenon called "feature interaction." This work first explains the historical and modern importance of feature interactions and then surveys the modern interpretability methods which do explicitly consider feature interactions. This survey aims to bring to light the importance of feature interactions in the larger context of machine learning interpretability, especially in a modern context where deep learning models heavily rely on feature interactions.},
	urldate = {2025-04-14},
	publisher = {arXiv},
	author = {Tsang, Michael and Enouen, James and Liu, Yan},
	month = mar,
	year = {2021},
	note = {arXiv:2103.03103 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Snapshot:/Users/julietfleischer/Zotero/storage/J9FLF4BN/2103.html:text/html;Tsang et al. - 2021 - Interpretable Artificial Intelligence through the Lens of Feature Interaction.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Tsang et al. - 2021 - Interpretable Artificial Intelligence through the Lens of Feature Interaction.pdf:application/pdf},
}

@inproceedings{sun_puregam_2022,
	address = {Washington DC USA},
	title = {{pureGAM}: {Learning} an {Inherently} {Pure} {Additive} {Model}},
	isbn = {978-1-4503-9385-0},
	shorttitle = {{pureGAM}},
	url = {https://dl.acm.org/doi/10.1145/3534678.3539256},
	doi = {10.1145/3534678.3539256},
	abstract = {Including pairwise or higher-order interactions among predictors of a Generalized Additive Model (GAM) is gaining increasing attention in the literature. However, existing models face an identifiability challenge. In this paper, we propose pureGAM, an inherently pure additive model of both main effects and higher-order interactions. By imposing the pureness condition to constrain each component function, pureGAM is proved to be identifiable without compromising accuracy. Furthermore, the pureness condition introduces additional interpretability in terms of simplicity. Practically, pureGAM is a unified model to support both numerical and categorical features with a novel learning procedure to achieve optimal performance. Evaluations show that pureGAM outperforms other GAMs and has very competitive performance even compared with opaque models, and its interpretability remarkably outperforms competitors in terms of pureness. We also share a successful adoption of pureGAM in one real-world application.},
	language = {en},
	urldate = {2025-04-14},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Sun, Xingzhi and Wang, Ziyu and Ding, Rui and Han, Shi and Zhang, Dongmei},
	month = aug,
	year = {2022},
	pages = {1728--1738},
	file = {Sun et al. - 2022 - pureGAM Learning an Inherently Pure Additive Model.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Sun et al. - 2022 - pureGAM Learning an Inherently Pure Additive Model.pdf:application/pdf},
}

@article{bordt_shapley_nodate,
	title = {From {Shapley} {Values} to {Generalized} {Additive} {Models} and back},
	abstract = {In explainable machine learning, local posthoc explanation algorithms and inherently interpretable models are often seen as competing approaches. This work offers a partial reconciliation between the two by establishing a correspondence between Shapley Values and Generalized Additive Models (GAMs). We introduce n-Shapley Values, a parametric family of local post-hoc explanation algorithms that explain individual predictions with interaction terms up to order n. By varying the parameter n, we obtain a sequence of explanations that covers the entire range from Shapley Values up to a uniquely determined decomposition of the function we want to explain. The relationship between n-Shapley Values and this decomposition offers a functionally-grounded characterization of Shapley Values, which highlights their limitations. We then show that nShapley Values, as well as the Shapley Taylorand Faith-Shap interaction indices, recover GAMs with interaction terms up to order n. This implies that the original Shapely Values recover GAMs without variable interactions. Taken together, our results provide a precise characterization of Shapley Values as they are being used in explainable machine learning. They also offer a principled interpretation of partial dependence plots of Shapley Values in terms of the underlying functional decomposition. A package for the estimation of different interaction indices is available at https: //github.com/tml-tuebingen/nshap.},
	language = {en},
	author = {Bordt, Sebastian},
	file = {Bordt - From Shapley Values to Generalized Additive Models and back.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Bordt - From Shapley Values to Generalized Additive Models and back.pdf:application/pdf},
}

@inproceedings{sorokina_detecting_2008,
	address = {New York, NY, USA},
	series = {{ICML} '08},
	title = {Detecting statistical interactions with additive groves of trees},
	isbn = {978-1-60558-205-4},
	url = {https://dl.acm.org/doi/10.1145/1390156.1390282},
	doi = {10.1145/1390156.1390282},
	abstract = {Discovering additive structure is an important step towards understanding a complex multi-dimensional function because it allows the function to be expressed as the sum of lower-dimensional components. When variables interact, however, their effects are not additive and must be modeled and interpreted simultaneously. We present a new approach for the problem of interaction detection. Our method is based on comparing the performance of unrestricted and restricted prediction models, where restricted models are prevented from modeling an interaction in question. We show that an additive model-based regression ensemble, Additive Groves, can be restricted appropriately for use with this framework, and thus has the right properties for accurately detecting variable interactions.},
	urldate = {2025-04-14},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Sorokina, Daria and Caruana, Rich and Riedewald, Mirek and Fink, Daniel},
	month = jul,
	year = {2008},
	pages = {1000--1007},
	file = {Sorokina et al. - 2008 - Detecting statistical interactions with additive groves of trees.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Sorokina et al. - 2008 - Detecting statistical interactions with additive groves of trees.pdf:application/pdf},
}

@article{liu_estimating_2006,
	title = {Estimating {Mean} {Dimensionality} of {Analysis} of {Variance} {Decompositions}},
	volume = {101},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1198/016214505000001410},
	doi = {10.1198/016214505000001410},
	language = {en},
	number = {474},
	urldate = {2025-04-14},
	journal = {Journal of the American Statistical Association},
	author = {Liu, Ruixue and Owen, Art B},
	month = jun,
	year = {2006},
	pages = {712--721},
	file = {Liu und Owen - 2006 - Estimating Mean Dimensionality of Analysis of Variance Decompositions.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Liu und Owen - 2006 - Estimating Mean Dimensionality of Analysis of Variance Decompositions.pdf:application/pdf},
}

@article{borgonovo_global_2022,
	title = {Global {Sensitivity} {Analysis} with {Mixtures}: {A} {Generalized} {Functional} {ANOVA} {Approach}},
	volume = {42},
	issn = {0272-4332, 1539-6924},
	shorttitle = {Global {Sensitivity} {Analysis} with {Mixtures}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/risa.13763},
	doi = {10.1111/risa.13763},
	abstract = {Abstract
            This work investigates aspects of the global sensitivity analysis of computer codes when alternative plausible distributions for the model inputs are available to the analyst. Analysts may decide to explore results under each distribution or to aggregate the distributions, assigning, for instance, a mixture. In the first case, we lose uniqueness of the sensitivity measures, and in the second case, we lose independence even if the model inputs are independent under each of the assigned distributions. Removing the unique distribution assumption impacts the mathematical properties at the basis of variance‐based sensitivity analysis and has consequences on result interpretation as well. We analyze in detail the technical aspects. From this investigation, we derive corresponding recommendations for the risk analyst. We show that an approach based on the generalized functional ANOVA expansion remains theoretically grounded in the presence of a mixture distribution. Numerically, we base the construction of the generalized function ANOVA effects on the diffeomorphic modulation under observable response preserving homotopy regression. Our application addresses the calculation of variance‐based sensitivity measures for the well‐known Nordhaus' DICE model, when its inputs are assigned a mixture distribution. A discussion of implications for the risk analyst and future research perspectives closes the work.},
	language = {en},
	number = {2},
	urldate = {2025-04-14},
	journal = {Risk Analysis},
	author = {Borgonovo, Emanuele and Li, Genyuan and Barr, John and Plischke, Elmar and Rabitz, Herschel},
	month = feb,
	year = {2022},
	pages = {304--333},
	file = {Borgonovo et al. - 2022 - Global Sensitivity Analysis with Mixtures A Generalized Functional ANOVA Approach.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Borgonovo et al. - 2022 - Global Sensitivity Analysis with Mixtures A Generalized Functional ANOVA Approach.pdf:application/pdf},
}

@article{dette_analysis_2001,
	title = {Analysis of {Variance} in {Nonparametric} {Regression} {Models}},
	volume = {76},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X00919134},
	doi = {10.1006/jmva.2000.1913},
	language = {en},
	number = {1},
	urldate = {2025-04-16},
	journal = {Journal of Multivariate Analysis},
	author = {Dette, Holger and Derbort, Stephan},
	month = jan,
	year = {2001},
	pages = {110--137},
	file = {Dette und Derbort - 2001 - Analysis of Variance in Nonparametric Regression Models.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Dette und Derbort - 2001 - Analysis of Variance in Nonparametric Regression Models.pdf:application/pdf},
}

@book{cellier_machine_2020,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}: {International} {Workshops} of {ECML} {PKDD} 2019, {Würzburg}, {Germany}, {September} 16–20, 2019, {Proceedings}, {Part} {I}},
	volume = {1167},
	copyright = {https://creativecommons.org/licenses/by/4.0},
	isbn = {978-3-030-43822-7 978-3-030-43823-4},
	shorttitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	url = {https://link.springer.com/10.1007/978-3-030-43823-4},
	language = {en},
	urldate = {2025-04-16},
	publisher = {Springer International Publishing},
	editor = {Cellier, Peggy and Driessens, Kurt},
	year = {2020},
	doi = {10.1007/978-3-030-43823-4},
	file = {Cellier und Driessens - 2020 - Machine Learning and Knowledge Discovery in Databases International Workshops of ECML PKDD 2019, Wü.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Cellier und Driessens - 2020 - Machine Learning and Knowledge Discovery in Databases International Workshops of ECML PKDD 2019, Wü.pdf:application/pdf},
}

@article{wahba_1994_nodate,
	title = {{THE} 1994 {NEYMAN} {MEMORIAL} {LECTURE}},
	language = {en},
	author = {Wahba, Grace and Wang, Yuedong and Gu, Chong and Klein, Ronald and Klein, Barbara},
	file = {Wahba et al. - THE 1994 NEYMAN MEMORIAL LECTURE.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Wahba et al. - THE 1994 NEYMAN MEMORIAL LECTURE.pdf:application/pdf},
}

@article{efron_jackknife_1981,
	title = {The {Jackknife} {Estimate} of {Variance}},
	volume = {The Annals of Statistic},
	language = {en},
	number = {Vol. 9, No. 3},
	author = {Efron, B and Stein, C},
	month = may,
	year = {1981},
	pages = {pp. 586--596},
	file = {The Jackknife Estimate of Variance.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/The Jackknife Estimate of Variance.pdf:application/pdf},
}

@article{muehlenstaedt_data-driven_2012,
	title = {Data-driven {Kriging} models based on {FANOVA}-decomposition},
	volume = {22},
	copyright = {http://www.springer.com/tdm},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/s11222-011-9259-7},
	doi = {10.1007/s11222-011-9259-7},
	language = {en},
	number = {3},
	urldate = {2025-04-16},
	journal = {Statistics and Computing},
	author = {Muehlenstaedt, Thomas and Roustant, Olivier and Carraro, Laurent and Kuhnt, Sonja},
	month = may,
	year = {2012},
	pages = {723--738},
	file = {Muehlenstaedt et al. - 2012 - Data-driven Kriging models based on FANOVA-decomposition.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Muehlenstaedt et al. - 2012 - Data-driven Kriging models based on FANOVA-decomposition.pdf:application/pdf},
}

@article{sobol_global_2001,
	title = {Global sensitivity indices for nonlinear mathematical models and their {Monte} {Carlo} estimates},
	volume = {55},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {03784754},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378475400002706},
	doi = {10.1016/S0378-4754(00)00270-6},
	abstract = {Global sensitivity indices for rather complex mathematical models can be efﬁciently computed by Monte Carlo (or quasi-Monte Carlo) methods. These indices are used for estimating the inﬂuence of individual variables or groups of variables on the model output. © 2001 IMACS. Published by Elsevier Science B.V. All rights reserved.},
	language = {en},
	number = {1-3},
	urldate = {2025-04-20},
	journal = {Mathematics and Computers in Simulation},
	author = {Sobol, I.M},
	month = feb,
	year = {2001},
	pages = {271--280},
	file = {Sobol′ - 2001 - Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Sobol′ - 2001 - Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates.pdf:application/pdf},
}


@article{panigrahi_robustness_2025,
	title = {On {Robustness} of the {Explanatory} {Power} of {Machine} {Learning} {Models}: {Insights} {From} a {New} {Explainable} {AI} {Approach} {Using} {Sensitivity} {Analysis}},
	volume = {61},
	issn = {0043-1397, 1944-7973},
	shorttitle = {On {Robustness} of the {Explanatory} {Power} of {Machine} {Learning} {Models}},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2024WR037398},
	doi = {10.1029/2024WR037398},
	abstract = {Machine learning (ML) is increasingly considered the solution to environmental problems where limited or no physico‐chemical process understanding exists. But in supporting high‐stakes decisions, where the ability to explain possible solutions is key to their acceptability and legitimacy, ML can fall short. Here, we develop a method, rooted in formal sensitivity analysis, to uncover the primary drivers behind ML predictions. Unlike many methods for explainable artificial intelligence (XAI), this method (a) accounts for complex multivariate distributional properties of data, common in environmental systems, (b) offers a global assessment of the input‐output response surface formed by ML, rather than focusing solely on local regions around existing data points, and (c) is scalable and data‐size independent, ensuring computational efficiency with large data sets. We apply this method to a suite of ML models predicting various water quality variables in a pilot‐scale experimental pit lake. A critical finding is that subtle alterations in the design of some ML models (such as variations in random seed, functional class, hyperparameters, or data splitting) can lead to different interpretations of how outputs depend on inputs. Further, models from different ML families (decision trees, connectionists, or kernels) may focus on different aspects of the information provided by data, despite displaying similar predictive power. Overall, our results underscore the need to assess the explanatory robustness of ML models and advocate for using model ensembles to gain deeper insights into system drivers and improve prediction reliability.},
	language = {en},
	number = {3},
	urldate = {2025-04-20},
	journal = {Water Resources Research},
	author = {Panigrahi, Banamali and Razavi, Saman and Doig, Lorne E. and Cordell, Blanchard and Gupta, Hoshin V. and Liber, Karsten},
	month = mar,
	year = {2025},
	pages = {e2024WR037398},
	file = {Panigrahi et al. - 2025 - On Robustness of the Explanatory Power of Machine Learning Models Insights From a New Explainable A.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Panigrahi et al. - 2025 - On Robustness of the Explanatory Power of Machine Learning Models Insights From a New Explainable A.pdf:application/pdf},
}

@article{hoepner_significance_2021,
	title = {Significance, relevance and explainability in the machine learning age: an econometrics and financial data science perspective},
	volume = {27},
	issn = {1351-847X, 1466-4364},
	shorttitle = {Significance, relevance and explainability in the machine learning age},
	url = {https://www.tandfonline.com/doi/full/10.1080/1351847X.2020.1847725},
	doi = {10.1080/1351847X.2020.1847725},
	abstract = {Although machine learning is frequently associated with neural networks, it also comprises econometric regression approaches and other statistical techniques whose accuracy enhances with increasing observation. What constitutes high quality machine learning is yet unclear though. Proponents of deep learning (i.e. neural networks) value computational efficiency over human interpretability and tolerate the ‘black box’ appeal of their algorithms, whereas proponents of explainable artificial intelligence (xai) employ traceable ‘white box’ methods (e.g. regressions) to enhance explainability to human decision makers. We extend Brooks et al.’s [2019. ‘Financial Data Science: The Birth of a New Financial Research Paradigm Complementing Econometrics?’ European Journal of Finance 25 (17): 1627–36.] work on significance and relevance as assessment critieria in econometrics and financial data science to contribute to this debate. Specifically, we identify explainability as the Achilles heel of classic machine learning approaches such as neural networks, which are not fully replicable, lack transparency and traceability and therefore do not permit any attempts to establish causal inference. We conclude by suggesting routes for future research to advance the design and efficiency of ‘white box’ algorithms.},
	language = {en},
	number = {1-2},
	urldate = {2025-04-20},
	journal = {The European Journal of Finance},
	author = {Hoepner, Andreas G. F. and McMillan, David and Vivian, Andrew and Wese Simen, Chardin},
	month = jan,
	year = {2021},
	pages = {1--7},
	file = {Hoepner et al. - 2021 - Significance, relevance and explainability in the machine learning age an econometrics and financia.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Hoepner et al. - 2021 - Significance, relevance and explainability in the machine learning age an econometrics and financia.pdf:application/pdf},
}

@article{rane_explainable_2023,
	title = {Explainable {Artificial} {Intelligence} ({XAI}) {Approaches} for {Transparency} and {Accountability} in {Financial} {Decision}-{Making}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=4640316},
	doi = {10.2139/ssrn.4640316},
	abstract = {Recently, there has been a growing trend in incorporating Artificial Intelligence (AI) into financial decisionmaking, prompting concerns about the transparency and accountability of these intricate systems. This study investigates the impact of Explainable Artificial Intelligence (XAI) approaches in alleviating these concerns and improving transparency in financial decision-making processes. The paper commences by outlining the current landscape of AI applications in finance, underscoring the complex and opaque nature of advanced machine learning models. The lack of interpretability in these models presents a significant challenge, as stakeholders, regulators, and end-users often struggle to comprehend the reasoning behind AI-driven financial decisions. This opacity raises questions regarding accountability and trust, particularly in critical financial scenarios. The primary focus of the research centers on the analysis and implementation of XAI techniques to introduce transparency into financial AI systems. Various XAI methods, including rule-based systems, model-agnostic approaches, and interpretable machine learning models, are scrutinized for their effectiveness in producing understandable explanations for AI-driven financial decisions. The paper explores how these approaches can be tailored to meet the distinct requirements of the financial domain, where interpretability is essential for regulatory compliance and stakeholder confidence. Moreover, the research delves into the potential impact of XAI on accountability mechanisms within financial institutions. By offering interpretable explanations for model outputs, XAI not only enhances transparency but also empowers financial professionals to identify and rectify biases, errors, or unethical behaviour in AI algorithms. By promoting transparency and accountability, XAI not only addresses ethical concerns but also facilitates the responsible and trustworthy deployment of AI in the financial sector. This, in turn, contributes to the advancement of fair, reliable, and secure financial systems.},
	language = {en},
	urldate = {2025-04-20},
	journal = {SSRN Electronic Journal},
	author = {Rane, Nitin and Choudhary, Saurabh and Rane, Jayesh},
	year = {2023},
	file = {Rane et al. - 2023 - Explainable Artificial Intelligence (XAI) Approaches for Transparency and Accountability in Financia.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Rane et al. - 2023 - Explainable Artificial Intelligence (XAI) Approaches for Transparency and Accountability in Financia.pdf:application/pdf},
}

@article{cerneviciene_explainable_2024,
	title = {Explainable artificial intelligence ({XAI}) in finance: a systematic literature review},
	volume = {57},
	issn = {1573-7462},
	shorttitle = {Explainable artificial intelligence ({XAI}) in finance},
	url = {https://link.springer.com/10.1007/s10462-024-10854-8},
	doi = {10.1007/s10462-024-10854-8},
	abstract = {As the range of decisions made by Artificial Intelligence (AI) expands, the need for Explainable AI (XAI) becomes increasingly critical. The reasoning behind the specific outcomes of complex and opaque financial models requires a thorough justification to improve risk assessment, minimise the loss of trust, and promote a more resilient and trustworthy financial ecosystem. This Systematic Literature Review (SLR) identifies 138 relevant articles from 2005 to 2022 and highlights empirical examples demonstrating XAI’s potential benefits in the financial industry. We classified the articles according to the financial tasks addressed by AI using XAI, the variation in XAI methods between applications and tasks, and the development and application of new XAI methods. The most popular financial tasks addressed by the AI using XAI were credit management, stock price predictions, and fraud detection. The three most commonly employed AI black-box techniques in finance whose explainability was evaluated were Artificial Neural Networks (ANN), Extreme Gradient Boosting (XGBoost), and Random Forest. Most of the examined publications utilise feature importance, Shapley additive explanations (SHAP), and rule-based methods. In addition, they employ explainability frameworks that integrate multiple XAI techniques. We also concisely define the existing challenges, requirements, and unresolved issues in applying XAI in the financial sector.},
	language = {en},
	number = {8},
	urldate = {2025-04-20},
	journal = {Artificial Intelligence Review},
	author = {Černevičienė, Jurgita and Kabašinskas, Audrius},
	month = jul,
	year = {2024},
	pages = {216},
	file = {Černevičienė und Kabašinskas - 2024 - Explainable artificial intelligence (XAI) in finance a systematic literature review.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Černevičienė und Kabašinskas - 2024 - Explainable artificial intelligence (XAI) in finance a systematic literature review.pdf:application/pdf},
}

@article{andrew_nii_anang_explainable_2024,
	title = {Explainable {AI} in financial technologies: {Balancing} innovation with regulatory compliance},
	volume = {13},
	issn = {25828185},
	shorttitle = {Explainable {AI} in financial technologies},
	url = {https://ijsra.net/node/5858},
	doi = {10.30574/ijsra.2024.13.1.1870},
	abstract = {As artificial intelligence (AI) technologies increasingly permeate the financial sector, their adoption raises significant challenges and opportunities regarding regulatory compliance and innovation. This paper explores the critical role of Explainable AI (XAI) in balancing these two aspects, particularly in applications such as fraud detection, credit scoring, and algorithmic trading. We highlight the necessity of XAI for financial institutions to meet regulatory requirements that demand transparency and accountability in AI-driven decisions. The discussion delves into the complexities faced by these institutions, including the inherent biases in algorithms that can compromise fairness and the ethical implications of opaque decision-making processes. Through case studies of successful XAI implementations, we illustrate how transparency can enhance consumer trust and promote a more robust regulatory environment. This examination underscores the importance of fostering innovation while adhering to compliance mandates, providing a roadmap for financial institutions striving to leverage AI responsibly. Ultimately, we advocate for the integration of XAI as a means to mitigate risks associated with algorithmic bias and enhance the integrity of financial technologies, thereby contributing to a more equitable financial landscape.},
	language = {en},
	number = {1},
	urldate = {2025-04-20},
	journal = {International Journal of Science and Research Archive},
	author = {{Andrew Nii Anang} and {Oluwatosin Esther Ajewumi} and {Tobi Sonubi} and {Kenneth Chukwujekwu Nwafor} and {John Babatope Arogundade} and {Itiade James Akinbi}},
	month = oct,
	year = {2024},
	pages = {1793--1806},
	file = {Andrew Nii Anang et al. - 2024 - Explainable AI in financial technologies Balancing innovation with regulatory compliance.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Andrew Nii Anang et al. - 2024 - Explainable AI in financial technologies Balancing innovation with regulatory compliance.pdf:application/pdf},
}

@article{yang_gami-net_2021,
	title = {{GAMI}-{Net}: {An} explainable neural network based on generalized additive models with structured interactions},
	volume = {120},
	issn = {00313203},
	shorttitle = {{GAMI}-{Net}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320321003484},
	doi = {10.1016/j.patcog.2021.108192},
	abstract = {The lack of interpretability is an inevitable problem when using neural network models in real applications. In this paper, an explainable neural network based on generalized additive models with structured interactions (GAMI-Net) is proposed to pursue a good balance between prediction accuracy and model interpretability. GAMI-Net is a disentangled feedforward network with multiple additive subnetworks; each subnetwork consists of multiple hidden layers and is designed for capturing one main effect or one pairwise interaction. Three interpretability aspects are further considered, including a) sparsity, to select the most signiﬁcant effects for parsimonious representations; b) heredity, a pairwise interaction could only be included when at least one of its parent main effects exists; and c) marginal clarity, to make main effects and pairwise interactions mutually distinguishable. An adaptive training algorithm is developed, where main effects are ﬁrst trained and then pairwise interactions are ﬁtted to the residuals. Numerical experiments on both synthetic functions and real-world datasets show that the proposed model enjoys superior interpretability and it maintains competitive prediction accuracy in comparison to the explainable boosting machine and other classic machine learning models.},
	language = {en},
	urldate = {2025-04-21},
	journal = {Pattern Recognition},
	author = {Yang, Zebin and Zhang, Aijun and Sudjianto, Agus},
	month = dec,
	year = {2021},
	pages = {108192},
	file = {PDF:/Users/julietfleischer/Zotero/storage/QXITTKYC/Yang et al. - 2021 - GAMI-Net An explainable neural network based on generalized additive models with structured interac.pdf:application/pdf;Yang et al. - 2021 - GAMI-Net An explainable neural network based on generalized additive models with structured interac.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Yang et al. - 2021 - GAMI-Net An explainable neural network based on generalized additive models with structured interac.pdf:application/pdf},
}

@book{gu_smoothing_2013,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Smoothing {Spline} {ANOVA} {Models}},
	volume = {297},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-1-4614-5368-0 978-1-4614-5369-7},
	url = {https://link.springer.com/10.1007/978-1-4614-5369-7},
	language = {en},
	urldate = {2025-04-21},
	publisher = {Springer New York},
	author = {Gu, Chong},
	year = {2013},
	doi = {10.1007/978-1-4614-5369-7},
	file = {Gu - 2013 - Smoothing Spline ANOVA Models.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Gu - 2013 - Smoothing Spline ANOVA Models.pdf:application/pdf},
}

@article{choi_meta-anova_2025,
	title = {Meta-anova: screening interactions for interpretable machine learning},
	issn = {1226-3192, 2005-2863},
	shorttitle = {Meta-anova},
	url = {https://link.springer.com/10.1007/s42952-024-00302-2},
	doi = {10.1007/s42952-024-00302-2},
	abstract = {There are two things to be considered when we evaluate predictive models. One is prediction accuracy, and the other is interpretability. Over the recent decades, many prediction models of high performance, such as ensemble-based models and deep neural networks, have been developed. However, these models are often too complex, making it difficult to intuitively interpret their predictions. This complexity in interpretation limits their use in many real-world fields that require accountability, such as medicine, finance, and college admissions. In this study, we develop a novel method called Meta-ANOVA to provide an interpretable model for any given prediction model. The basic idea of Meta-ANOVA is to transform a given blackbox prediction model to the functional ANOVA model. A novel technical contribution of Meta-ANOVA is a procedure of screening out unnecessary interactions before transforming a given black-box model to the functional ANOVA model. This screening procedure allows the inclusion of higher order interactions in the transformed functional ANOVA model without computational difficulties. We prove that the screening procedure is asymptotically consistent. Through various experiments with synthetic and real-world datasets, we empirically demonstrate the superiority of Meta-ANOVA.},
	language = {en},
	urldate = {2025-04-22},
	journal = {Journal of the Korean Statistical Society},
	author = {Choi, Yongchan and Park, Seokhun and Park, Chanmoo and Kim, Dongha and Kim, Yongdai},
	month = jan,
	year = {2025},
	file = {Choi et al. - 2025 - Meta-anova screening interactions for interpretable machine learning.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Choi et al. - 2025 - Meta-anova screening interactions for interpretable machine learning.pdf:application/pdf},
}

@misc{konig_disentangling_2024,
	title = {Disentangling {Interactions} and {Dependencies} in {Feature} {Attribution}},
	url = {http://arxiv.org/abs/2410.23772},
	doi = {10.48550/arXiv.2410.23772},
	abstract = {In explainable machine learning, global feature importance methods try to determine how much each individual feature contributes to predicting the target variable, resulting in one importance score for each feature. But often, predicting the target variable requires interactions between several features (such as in the XOR function), and features might have complex statistical dependencies that allow to partially replace one feature with another one. In commonly used feature importance scores these cooperative effects are conflated with the features' individual contributions, making them prone to misinterpretations. In this work, we derive DIP, a new mathematical decomposition of individual feature importance scores that disentangles three components: the standalone contribution and the contributions stemming from interactions and dependencies. We prove that the DIP decomposition is unique and show how it can be estimated in practice. Based on these results, we propose a new visualization of feature importance scores that clearly illustrates the different contributions.},
	urldate = {2025-04-22},
	publisher = {arXiv},
	author = {König, Gunnar and Günther, Eric and Luxburg, Ulrike von},
	month = oct,
	year = {2024},
	note = {arXiv:2410.23772 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {König et al. - 2024 - Disentangling Interactions and Dependencies in Feature Attribution.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/König et al. - 2024 - Disentangling Interactions and Dependencies in Feature Attribution.pdf:application/pdf;Snapshot:/Users/julietfleischer/Zotero/storage/7HHBWHWR/2410.html:text/html},
}

@article{guidotti_survey_2019,
	title = {A {Survey} of {Methods} for {Explaining} {Black} {Box} {Models}},
	volume = {51},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3236009},
	doi = {10.1145/3236009},
	abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
	language = {en},
	number = {5},
	urldate = {2025-04-22},
	journal = {ACM Computing Surveys},
	author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
	month = sep,
	year = {2019},
	pages = {1--42},
	file = {Guidotti et al. - 2019 - A Survey of Methods for Explaining Black Box Models.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Guidotti et al. - 2019 - A Survey of Methods for Explaining Black Box Models.pdf:application/pdf},
}

@article{razavi_future_2021,
	title = {The {Future} of {Sensitivity} {Analysis}: {An} essential discipline for systems modeling and policy support},
	volume = {137},
	issn = {13648152},
	shorttitle = {The {Future} of {Sensitivity} {Analysis}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364815220310112},
	doi = {10.1016/j.envsoft.2020.104954},
	abstract = {Sensitivity analysis (SA) is en route to becoming an integral part of mathematical modeling. The tremendous potential benefits of SA are, however, yet to be fully realized, both for advancing mechanistic and data-driven modeling of human and natural systems, and in support of decision making. In this perspective paper, a multidisciplinary group of researchers and practitioners revisit the current status of SA, and outline research challenges in regard to both theoretical frameworks and their applications to solve real-world problems. Six areas are discussed that warrant further attention, including (1) structuring and standardizing SA as a discipline, (2) realizing the untapped potential of SA for systems modeling, (3) addressing the computational burden of SA, (4) progressing SA in the context of machine learning, (5) clarifying the relationship and role of SA to uncertainty quantification, and (6) evolving the use of SA in support of decision making. An outlook for the future of SA is provided that underlines how SA must underpin a wide variety of activities to better serve science and society.},
	language = {en},
	urldate = {2025-04-22},
	journal = {Environmental Modelling \& Software},
	author = {Razavi, Saman and Jakeman, Anthony and Saltelli, Andrea and Prieur, Clémentine and Iooss, Bertrand and Borgonovo, Emanuele and Plischke, Elmar and Lo Piano, Samuele and Iwanaga, Takuya and Becker, William and Tarantola, Stefano and Guillaume, Joseph H.A. and Jakeman, John and Gupta, Hoshin and Melillo, Nicola and Rabitti, Giovanni and Chabridon, Vincent and Duan, Qingyun and Sun, Xifu and Smith, Stefán and Sheikholeslami, Razi and Hosseini, Nasim and Asadzadeh, Masoud and Puy, Arnald and Kucherenko, Sergei and Maier, Holger R.},
	month = mar,
	year = {2021},
	pages = {104954},
	file = {Razavi et al. - 2021 - The Future of Sensitivity Analysis An essential discipline for systems modeling and policy support.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Razavi et al. - 2021 - The Future of Sensitivity Analysis An essential discipline for systems modeling and policy support.pdf:application/pdf},
}

@book{steland_artificial_2022,
	address = {Cham},
	title = {Artificial {Intelligence}, {Big} {Data} and {Data} {Science} in {Statistics}: {Challenges} and {Solutions} in {Environmetrics}, the {Natural} {Sciences} and {Technology} (from p. 155)},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-031-07154-6 978-3-031-07155-3},
	shorttitle = {Artificial {Intelligence}, {Big} {Data} and {Data} {Science} in {Statistics}},
	url = {https://link.springer.com/10.1007/978-3-031-07155-3},
	language = {en},
	urldate = {2025-04-22},
	publisher = {Springer International Publishing},
	editor = {Steland, Ansgar and Tsui, Kwok-Leung},
	year = {2022},
	doi = {10.1007/978-3-031-07155-3},
	file = {Steland und Tsui - 2022 - Artificial Intelligence, Big Data and Data Science in Statistics Challenges and Solutions in Enviro.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Steland und Tsui - 2022 - Artificial Intelligence, Big Data and Data Science in Statistics Challenges and Solutions in Enviro.pdf:application/pdf},
}

@article{hoeffding_class_1948,
	title = {A {Class} of {Statistics} with {Asymptotically} {Normal} {Distribution}},
	volume = {19},
	issn = {0003-4851},
	url = {https://www.jstor.org/stable/2235637},
	abstract = {Let X1, ·, Xn be n independent random vectors, Xν = (X(1) ν, ⋯, X(r) ν), and Φ(x1, ⋯, xm) a function of m(≤ n) vectors \$x\_{\textbackslash}nu = (x{\textasciicircum}\{(1)\}\_{\textbackslash}nu, {\textbackslash}cdots, x{\textasciicircum}\{(r)\}\_{\textbackslash}nu)\$. A statistic of the form U = ∑"Φ(Xα 1, ⋯, Xαm )/n(n - 1) ⋯ (n - m + 1), where the sum ∑" is extended over all permutations (α1, ⋯, αm) of m different integers, 1 ≤ αi ≤ n, is called a U-statistic. If X1, ⋯, Xn have the same (cumulative) distribution function (d.f.) F(x), U is an unbiased estimate of the population characteristic θ(F) = ∫ ⋯ ∫Φ(x1, ⋯, xm) dF(x1) ⋯ dF(xm). θ(F) is called a regular functional of the d.f. F(x). Certain optimal properties of U-statistics as unbiased estimates of regular functionals have been established by Halmos [9] (cf. Section 4). The variance of a U-statistic as a function of the sample size n and of certain population characteristics is studied in Section 5. It is shown that if X1, ⋯, Xn have the same distribution and Φ(x1, ⋯, xm) is independent of n, the d.f. of \${\textbackslash}sqrt n(U - {\textbackslash}theta)\$ tends to a normal d.f. as n → ∞ under the sole condition of the existence of EΦ2(X1, ⋯, Xm). Similar results hold for the joint distribution of several U-statistics (Theorems 7.1 and 7.2), for statistics U' which, in a certain sense, are asymptotically equivalent to U (Theorems 7.3 and 7.4), for certain functions of statistics U or U' (Theorem 7.5) and, under certain additional assumptions, for the case of the Xν's having different distributions (Theorems 8.1 and 8.2). Results of a similar character, though under different assumptions, are contained in a recent paper by von Mises [18] (cf. Section 7). Examples of statistics of the form U or U' are the moments, Fisher's k-statistics, Gini's mean difference, and several rank correlation statistics such as Spearman's rank correlation and the difference sign correlation (cf. Section 9). Asymptotic power functions for the non-parametric tests of independence based on these rank statistics are obtained. They show that these tests are not unbiased in the limit (Section 9f). The asymptotic distribution of the coefficient of partial difference sign correlation which has been suggested by Kendall also is obtained (Section 9h).},
	number = {3},
	urldate = {2025-04-22},
	journal = {The Annals of Mathematical Statistics},
	author = {Hoeffding, Wassily},
	year = {1948},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {293--325},
	file = {Hoeffding - 1948 - A Class of Statistics with Asymptotically Normal Distribution.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Hoeffding - 1948 - A Class of Statistics with Asymptotically Normal Distribution.pdf:application/pdf},
}

@article{il_idrissi_hoeffding_2025,
	title = {Hoeffding decomposition of functions of random dependent variables},
	volume = {208},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X25000399},
	doi = {10.1016/j.jmva.2025.105444},
	abstract = {Hoeffding’s functional decomposition is the cornerstone of many post-hoc interpretability methods. It entails decomposing arbitrary functions of mutually independent random variables as a sum of interactions. Many generalizations to dependent covariables have been proposed throughout the years, which rely on finding a set of suitable projectors. This paper characterizes such projectors under hierarchical orthogonality constraints and mild assumptions on the variable’s probabilistic structure. Our approach is deeply rooted in Hilbert space theory, giving intuitive insights on defining, identifying, and separating interactions from the effects due to the variables’ dependence structure. This new decomposition is then leveraged to define a new functional analysis of variance. Toy cases of functions of bivariate Bernoulli and Gaussian random variables are studied.},
	language = {en},
	urldate = {2025-04-22},
	journal = {Journal of Multivariate Analysis},
	author = {Il Idrissi, Marouane and Bousquet, Nicolas and Gamboa, Fabrice and Iooss, Bertrand and Loubes, Jean-Michel},
	month = jul,
	year = {2025},
	pages = {105444},
	file = {Il Idrissi et al. - 2025 - Hoeffding decomposition of functions of random dependent variables.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Il Idrissi et al. - 2025 - Hoeffding decomposition of functions of random dependent variables.pdf:application/pdf},
}

@article{stone_use_1994,
	title = {The {Use} of {Polynomial} {Splines} and {Their} {Tensor} {Products} in {Multivariate} {Function} {Estimation}},
	volume = {22},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/2242446},
	abstract = {Let X$_{\textrm{1}}$, ..., X$_{\textrm{M}}$, Y$_{\textrm{1}}$,..., Y$_{\textrm{N}}$ be random variables, and set X = (X$_{\textrm{1}}$, ..., X$_{\textrm{M}}$) and Y = (Y$_{\textrm{1}}$, ..., Y$_{\textrm{N}}$). Let φ be the regression or logistic or Poisson regression function of Y on X(N = 1) or the logarithm of the density function of Y or the conditional density function of Y on X. Consider the approximation φ$^{\textrm{*}}$ to φ having a suitably defined form involving a specified sum of functions of at most d of the variables x$_{\textrm{1}}$, ..., x$_{\textrm{M}}$, y$_{\textrm{1}}$,..., y$_{\textrm{N}}$ and, subject to this form, selected to minimize the mean squared error of approximation or to maximize the expected log-likelihood or conditional log-likelihood, as appropriate, given the choice of φ. Let p be a suitably defined lower bound to the smoothness of the components of φ$^{\textrm{*}}$. Consider a random sample of size n from the joint distribution of X and Y. Under suitable conditions, the least squares or maximum likelihood method is applied to a model involving nonadaptively selected sums of tensor products of polynomial splines to construct estimates of φ$^{\textrm{*}}$ and its components having the L$_{\textrm{2}}$ rate of convergence n$^{\textrm{-p/(2p + d)}}$.},
	number = {1},
	urldate = {2025-04-23},
	journal = {The Annals of Statistics},
	author = {Stone, Charles J.},
	year = {1994},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {118--171},
	file = {Stone - 1994 - The Use of Polynomial Splines and Their Tensor Products in Multivariate Function Estimation.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Stone - 1994 - The Use of Polynomial Splines and Their Tensor Products in Multivariate Function Estimation.pdf:application/pdf},
}

@incollection{Weber2011,
	address = {Berlin, Heidelberg},
	title = {U-statistics},
	isbn = {978-3-642-04898-2},
	url = {https://doi.org/10.1007/978-3-642-04898-2_607},
	booktitle = {International encyclopedia of statistical science},
	publisher = {Springer Berlin Heidelberg},
	author = {Weber, Neville C.},
	editor = {Lovric, Miodrag},
	year = {2011},
	doi = {10.1007/978-3-642-04898-2_607},
	pages = {1634--1635},
}

@article{huang_projection_1998,
	title = {Projection estimation in multiple regression with application to functional {ANOVA} models},
	volume = {26},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-26/issue-1/Projection-estimation-in-multiple-regression-with-application-to-functional-ANOVA/10.1214/aos/1030563984.full},
	doi = {10.1214/aos/1030563984},
	language = {en},
	number = {1},
	urldate = {2025-04-23},
	journal = {The Annals of Statistics},
	author = {Huang, Jianhua Z.},
	month = feb,
	year = {1998},
	file = {Huang - 1998 - Projection estimation in multiple regression with application to functional ANOVA models 1.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Huang - 1998 - Projection estimation in multiple regression with application to functional ANOVA models 1.pdf:application/pdf},
}

@article{stone_polynomial_1997,
	title = {Polynomial {Splines} and their {Tensor} {Products} in {Extended} {Linear} {Modeling}},
	volume = {25},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/2959054},
	abstract = {Analysis of variance type models are considered for a regression function or for the logarithm of a probability function, conditional probability function, density function, conditional density function, hazard function, conditional hazard function or spectral density function. Polynomial splines are used to model the main effects, and their tensor products are used to model any interaction components that are included. In the special context of survival analysis, the baseline hazard function is modeled and nonproportionality is allowed. In general, the theory involves the \$L\_2\$ rate of convergence for the fitted model and its components. The methodology involves least squares and maximum likelihood estimation, stepwise addition of basis functions using Rao statistics, stepwise deletion using Wald statistics and model selection using the Bayesian information criterion, cross-validation or an independent test set. Publicly available software, written in C and interfaced to S/S-PLUS, is used to apply this methodology to real data.},
	number = {4},
	urldate = {2025-04-23},
	journal = {The Annals of Statistics},
	author = {Stone, Charles J. and Hansen, Mark H. and Kooperberg, Charles and Truong, Young K.},
	year = {1997},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {1371--1425},
	file = {Stone et al. - 1997 - Polynomial Splines and their Tensor Products in Extended Linear Modeling.pdf:/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Stone et al. - 1997 - Polynomial Splines and their Tensor Products in Extended Linear Modeling.pdf:application/pdf},
}

@article{doi:10.1137/120876782,
	title = {Variance components and generalized sobol' indices},
	volume = {1},
	url = {https://doi.org/10.1137/120876782},
	doi = {10.1137/120876782},
	abstract = {This paper introduces generalized Sobol' indices, compares strategies for their estimation, and makes a systematic search for efficient estimators. Of particular interest are contrasts, sums of squares, and indices of bilinear form which allow a reduced number of function evaluations compared to alternatives. The bilinear framework includes some efficient estimators from Saltelli [Comput. Phys. Comm., 145 (2002), pp. 280–297] and Mauntz [Global Sensitivity Analysis of General Nonlinear Systems, Master's thesis, Imperial College, London, 2002] as well as some new estimators for specific variance components and mean dimensions. This paper also provides a bias corrected version of the estimator of Janon et al. [Asymptotic Normality and Efficiency of Two Sobol' Index Estimators, technical report, INRIA, Rocquencourt, France] and extends the bias correction to generalized Sobol' indices. Some numerical comparisons are given.},
	number = {1},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Owen, Art B.},
	year = {2013},
	note = {tex.eprint: https://doi.org/10.1137/120876782},
	pages = {19--41},
}
